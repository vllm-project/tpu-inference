apiVersion: v1
kind: Pod
metadata:
  name: sglang-benchmark
spec:
  containers:
  - name: sglang-benchmark-container
    image: python:3.9-slim
    command: ["/bin/bash", "-c"]
    args:
    - |
      set -ex
      apt-get update && apt-get install -y git
      git clone -b v0.5.2 https://github.com/sgl-project/sglang.git
      cd sglang
      pip install --upgrade pip
      pip install protobuf aiohttp numpy requests tqdm transformers
      python3 python/sglang/bench_serving.py \
        --host=$(IP) \
        --port=$(PORT) \
        --dataset-name='generated-shared-prefix' \
        --model=$(MODEL) \
        --tokenizer=$(MODEL) \
        --backend=vllm \
        --gsp-num-groups=$(GSP_NUM_GROUPS) \
        --gsp-prompts-per-group=$(GSP_PROMPTS_PER_GROUP) \
        --gsp-system-prompt-len=$(GSP_SYSTEM_PROMPT_LEN) \
        --gsp-question-len=$(GSP_QUESTION_LEN) \
        --gsp-output-len=$(GSP_OUTPUT_LEN) \
        --request-rate=800 \
        --max-concurrency=300 \
        --seed 42
    env:
    - name: IP
      value: "34.162.66.198" # Replace with the external IP of your deployed service
    - name: PORT
      value: "80"
    - name: MODEL
      value: "meta-llama/Llama-3.3-70B-Instruct"
    - name: HF_TOKEN
      valueFrom:
        secretKeyRef:
          name: hf-token-secret
          key: token
    - name: GSP_NUM_GROUPS
      value: "2"
    - name: GSP_PROMPTS_PER_GROUP
      value: "16"
    - name: GSP_SYSTEM_PROMPT_LEN
      value: "2048"
    - name: GSP_QUESTION_LEN
      value: "256"
    - name: GSP_OUTPUT_LEN
      value: "512"
  restartPolicy: Never
