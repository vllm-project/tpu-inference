apiVersion: v1
kind: Pod
metadata:
  name: tpu-job-offline-inference
spec:
  restartPolicy: Never
  nodeSelector:
    cloud.google.com/gke-tpu-accelerator: tpu-v6e-slice
    cloud.google.com/gke-tpu-topology: 2x4 # Specify the physical topology for the TPU slice.
  containers:
  - name: tpu-job
    image: <your-tpu-inference-container-image>
    imagePullPolicy: Always # Uncomment to always pull the latest image for any dev work
    command:
    - python
    - /workspace/tpu_inference/examples/offline_inference_kv_cache.py
    - --model=meta-llama/Llama-3.1-8B
    - --tensor_parallel_size=8
    - --max_model_len=1024
    - --kv-transfer-config
    - '{"kv_connector":"TPUConnector","kv_connector_module_path":"tpu_inference.distributed.tpu_connector_local","kv_role":"kv_both"}'
    env:
    - name: HUGGING_FACE_HUB_TOKEN
      valueFrom:
        secretKeyRef:
          name: hf-token-secret
          key: token
    resources:
      requests:
        google.com/tpu: 8
      limits:
        google.com/tpu: 8
---
apiVersion: v1
kind: Secret
metadata:
  name: hf-token-secret
  namespace: default
type: Opaque
stringData:
  token: <HF token to access the model>
