<p align="center">
   <!-- This image will ONLY show up in GitHub's dark mode -->
  <img src="assets/tpu_inference_dark_mode_short.png#gh-dark-mode-only" alt="vLLM TPU" style="width: 86%;">
    <!-- This image will ONLY show up in GitHub's light mode (and on other platforms) -->
  <img src="assets/tpu_inference_light_mode_short.png#gh-light-mode-only" alt="vLLM TPU" style="width: 86%;">
</p>

<h3 align="center">
A New High Performance TPU Backend Unifying PyTorch and JAX in vLLM
</h3>

<p align="center">
| <a href="https://tpu.vllm.ai"><b>Documentation</b></a> | <a href="https://blog.vllm.ai/"><b>Blog</b></a> | <a href="https://discuss.vllm.ai/c/hardware-support/google-tpu-support/27"><b>User Forum</b></a> | <a href="https://join.slack.com/share/enQtOTY2OTUxMDIyNjY1OS00M2MxYWQwZjAyMGZjM2MyZjRjNTA0ZjRkNjkzOTRhMzg0NDM2OTlkZDAxOTAzYmJmNzdkNDc4OGZjYTUwMmRh"><b>Developer Slack</b></a> |
</p>

---

# Welcome to vLLM on TPU

This is the documentation for vLLM on TPU, a high-performance, unified backend for running vLLM with PyTorch and JAX on Google TPUs.

## Getting Started

If you are new to vLLM on TPU, we recommend starting with the **[Quickstart](getting_started/quickstart.md)** guide. It will walk you through the process of setting up your environment and running your first model.

## Key Features

* **High Performance:** vLLM on TPU is designed to deliver the best possible performance for large language models on Google TPUs.
* **Unified Backend:** Run your PyTorch models on TPUs without any code changes, and get native support for JAX.
* **vLLM Standardization:** Enjoy the same user experience, telemetry, and interface as the original vLLM project.

## Developer Guides

If you are interested in contributing to the project or want to learn more about the internals, check out our developer guides:

* **[JAX Model Development](developer_guides/jax_model_development.md)**
* **[Torch Model Development](developer_guides/torchax_model_development.md)**

## Community and Support

Have questions or want to get involved?

* **[User Forum](https://discuss.vllm.ai/c/hardware-support/google-tpu-support/27)**
* **[Developer Slack](https://join.slack.com/share/enQtOTY2OTUxMDIyNjY1OS00M2MxYWQwZjAyMGZjM2MyZjRjNTA0ZjRkNjkzOTRhMzg0NDM2OTlkZDAxOTAzYmJmNzdkNDc4OGZjYTUwMmRh)**
* **[GitHub Issues](https://github.com/vllm-project/tpu-inference/issues)**
* **[Good First Issues](https://github.com/vllm-project/tpu-inference/issues?q=is%3Aissue+state%3Aopen+label%3A%22good+first+issue%22)**
