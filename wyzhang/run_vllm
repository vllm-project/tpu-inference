vllm serve "meta-llama/Llama-3.1-8B" \
    --download_dir /tmp \
    --disable-log-requests \
    --tensor_parallel_size=1 \
    --max-model-len=2048

