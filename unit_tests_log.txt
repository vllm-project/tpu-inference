============================= test session starts ==============================
platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0 -- /home/wenxindong_google_com/miniconda3/envs/wenxin_test/bin/python
cachedir: .pytest_cache
rootdir: /home/wenxindong_google_com/tpu-inference
configfile: pyproject.toml
plugins: anyio-4.10.0, mock-3.15.0, jaxtyping-0.3.2
collecting ... INFO 11-05 06:34:52 [__init__.py:22] TPU info: node_name=wenxindong-v6e-2 | tpu_type=v6e-8 | worker_id=0 | num_chips=8 | num_cores_per_chip=1
INFO 11-05 06:34:52 [importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.
INFO 11-05 06:34:52 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
WARNING 11-05 06:34:52 [interface.py:171] Failed to import from vllm._C: ModuleNotFoundError("No module named 'vllm._C'")
INFO 11-05 06:34:54 [model.py:630] Resolved architecture: LlamaForCausalLM
INFO 11-05 06:34:54 [model.py:1728] Using max model len 131072
INFO 11-05 06:34:54 [model.py:630] Resolved architecture: LlamaForCausalLM
INFO 11-05 06:34:54 [model.py:1728] Using max model len 131072
INFO 11-05 06:34:55 [model.py:630] Resolved architecture: LlamaForCausalLM
INFO 11-05 06:34:55 [model.py:1728] Using max model len 8192
INFO 11-05 06:34:55 [model.py:630] Resolved architecture: LlamaForCausalLM
WARNING 11-05 06:34:55 [model.py:1954] Casting torch.float16 to torch.bfloat16.
INFO 11-05 06:34:55 [model.py:1728] Using max model len 2048
INFO 11-05 06:34:56 [model.py:630] Resolved architecture: LlamaForCausalLM
INFO 11-05 06:34:56 [model.py:1728] Using max model len 8192
INFO 11-05 06:34:56 [model.py:630] Resolved architecture: LlamaForCausalLM
WARNING 11-05 06:34:56 [model.py:1954] Casting torch.float16 to torch.bfloat16.
INFO 11-05 06:34:56 [model.py:1728] Using max model len 2048
INFO 11-05 06:34:56 [config.py:414] Replacing legacy 'type' key with 'rope_type'
INFO 11-05 06:34:56 [model.py:630] Resolved architecture: Phi3ForCausalLM
INFO 11-05 06:34:56 [model.py:1728] Using max model len 4096
INFO 11-05 06:34:57 [config.py:414] Replacing legacy 'type' key with 'rope_type'
INFO 11-05 06:34:57 [model.py:630] Resolved architecture: Phi3ForCausalLM
INFO 11-05 06:34:57 [model.py:1728] Using max model len 4096
INFO 11-05 06:34:57 [model.py:630] Resolved architecture: Qwen2ForCausalLM
INFO 11-05 06:34:57 [model.py:1728] Using max model len 131072
INFO 11-05 06:34:57 [model.py:630] Resolved architecture: Qwen2ForCausalLM
INFO 11-05 06:34:57 [model.py:1728] Using max model len 131072
INFO 11-05 06:34:57 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:34:57 [model.py:1728] Using max model len 40960
INFO 11-05 06:34:58 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:34:58 [model.py:1728] Using max model len 40960
WARNING 11-05 06:34:58 [tpu_jax.py:228] Pin memory is not supported on TPU.
collected 190 items

tests/models/common/test_model_loader.py::test_get_model_architecture_supported INFO 11-05 06:34:58 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:34:58 [model.py:1728] Using max model len 40960
PASSED
tests/models/common/test_model_loader.py::test_get_model_architecture_unsupported PASSED
tests/models/common/test_model_loader.py::test_register_model_validation WARNING 11-05 06:34:58 [interfaces_base.py:72] The model (<class 'tpu_inference.models.common.model_loader.VllmCompatibleValidModel'>) is missing the `get_input_embeddings` method.
WARNING 11-05 06:34:58 [interfaces_base.py:72] The model (<class 'tpu_inference.models.common.model_loader.VllmCompatibleValidModel'>) is missing the `get_input_embeddings` method.
WARNING 11-05 06:34:58 [interfaces_base.py:72] The model (<class 'tpu_inference.models.common.model_loader.VllmCompatibleValidModel'>) is missing the `get_input_embeddings` method.
INFO 11-05 06:34:58 [model_loader.py:443] Registered JAX model ValidModel with tpu_inference and vLLM registries.
PASSED
tests/models/common/test_model_loader.py::test_register_model_new_arch WARNING 11-05 06:34:58 [interfaces_base.py:72] The model (<class 'tpu_inference.models.common.model_loader.VllmCompatibleMockModelA'>) is missing the `get_input_embeddings` method.
WARNING 11-05 06:34:58 [interfaces_base.py:72] The model (<class 'tpu_inference.models.common.model_loader.VllmCompatibleMockModelA'>) is missing the `get_input_embeddings` method.
WARNING 11-05 06:34:58 [interfaces_base.py:72] The model (<class 'tpu_inference.models.common.model_loader.VllmCompatibleMockModelA'>) is missing the `get_input_embeddings` method.
INFO 11-05 06:34:58 [model_loader.py:443] Registered JAX model NewArch with tpu_inference and vLLM registries.
PASSED
tests/models/common/test_model_loader.py::test_register_model_update_arch WARNING 11-05 06:34:58 [interfaces_base.py:72] The model (<class 'tpu_inference.models.common.model_loader.VllmCompatibleMockModelA'>) is missing the `get_input_embeddings` method.
WARNING 11-05 06:34:58 [interfaces_base.py:72] The model (<class 'tpu_inference.models.common.model_loader.VllmCompatibleMockModelA'>) is missing the `get_input_embeddings` method.
WARNING 11-05 06:34:58 [interfaces_base.py:72] The model (<class 'tpu_inference.models.common.model_loader.VllmCompatibleMockModelA'>) is missing the `get_input_embeddings` method.
INFO 11-05 06:34:58 [model_loader.py:443] Registered JAX model UpdatableArch with tpu_inference and vLLM registries.
WARNING 11-05 06:34:58 [registry.py:739] Model architecture UpdatableArch is already registered, and will be overwritten by the new model class <class 'tpu_inference.models.common.model_loader.VllmCompatibleMockModelB'>.
WARNING 11-05 06:34:58 [interfaces_base.py:72] The model (<class 'tpu_inference.models.common.model_loader.VllmCompatibleMockModelB'>) is missing the `get_input_embeddings` method.
WARNING 11-05 06:34:58 [interfaces_base.py:72] The model (<class 'tpu_inference.models.common.model_loader.VllmCompatibleMockModelB'>) is missing the `get_input_embeddings` method.
WARNING 11-05 06:34:58 [interfaces_base.py:72] The model (<class 'tpu_inference.models.common.model_loader.VllmCompatibleMockModelB'>) is missing the `get_input_embeddings` method.
INFO 11-05 06:34:58 [model_loader.py:443] Registered JAX model UpdatableArch with tpu_inference and vLLM registries.
PASSED
tests/models/common/test_model_loader.py::test_register_model_vllm_wrapper_methods WARNING 11-05 06:34:58 [interfaces_base.py:72] The model (<class 'tpu_inference.models.common.model_loader.VllmCompatibleMockModelA'>) is missing the `get_input_embeddings` method.
WARNING 11-05 06:34:58 [interfaces_base.py:72] The model (<class 'tpu_inference.models.common.model_loader.VllmCompatibleMockModelA'>) is missing the `get_input_embeddings` method.
WARNING 11-05 06:34:58 [interfaces_base.py:72] The model (<class 'tpu_inference.models.common.model_loader.VllmCompatibleMockModelA'>) is missing the `get_input_embeddings` method.
INFO 11-05 06:34:58 [model_loader.py:443] Registered JAX model WrapperMethodTestArch with tpu_inference and vLLM registries.
PASSED
tests/models/common/test_model_loader.py::test_get_flax_model INFO 11-05 06:35:08 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:35:08 [model.py:1728] Using max model len 40960
FAILED
tests/models/common/test_model_loader.py::test_get_vllm_model INFO 11-05 06:35:08 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:35:08 [model.py:1728] Using max model len 40960
INFO 11-05 06:35:08 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 11-05 06:35:08 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:35:08 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
WARNING 11-05 06:35:08 [tpu_jax.py:177] Increase the page size from 16 to 64 to make sure there'sno SMEM OOM
INFO 11-05 06:35:08 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:35:08 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 11-05 06:35:08 [parallel_state.py:1325] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 11-05 06:35:10 [tpu_jax.py:63] Cannot use None backend on TPU.
INFO 11-05 06:35:10 [tpu_jax.py:66] Using Pallas V1 backend.
INFO 11-05 06:35:10 [weight_utils.py:480] No model.safetensors.index.json found in remote.

Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.76it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.76it/s]

INFO 11-05 06:35:11 [default_loader.py:314] Loading weights took 0.61 seconds
FAILED
tests/models/common/test_model_loader.py::test_get_vllm_model_random_weights[True] INFO 11-05 06:35:12 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:35:12 [model.py:1728] Using max model len 40960
INFO 11-05 06:35:12 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 11-05 06:35:12 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:35:12 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
WARNING 11-05 06:35:12 [tpu_jax.py:177] Increase the page size from 16 to 64 to make sure there'sno SMEM OOM
INFO 11-05 06:35:12 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:35:12 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:35:12 [vllm_model_wrapper.py:97] Initializing vLLM model with random weights, weight loading skipped.
FAILED
tests/models/common/test_model_loader.py::test_get_vllm_model_random_weights[False] INFO 11-05 06:35:12 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:35:12 [model.py:1728] Using max model len 40960
INFO 11-05 06:35:12 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 11-05 06:35:12 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:35:12 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
WARNING 11-05 06:35:12 [tpu_jax.py:177] Increase the page size from 16 to 64 to make sure there'sno SMEM OOM
INFO 11-05 06:35:12 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:35:12 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:35:12 [vllm_model_wrapper.py:97] Initializing vLLM model with random weights, weight loading skipped.
FAILED
tests/models/common/test_model_loader.py::TestGetModel::test_get_model_flax_happy_path INFO 11-05 06:35:12 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:35:12 [model.py:1728] Using max model len 40960
INFO 11-05 06:35:12 [model_loader.py:318] Loading model with MODEL_IMPL_TYPE=flax_nnx
PASSED
tests/models/common/test_model_loader.py::TestGetModel::test_get_model_vllm_happy_path INFO 11-05 06:35:13 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:35:13 [model.py:1728] Using max model len 40960
INFO 11-05 06:35:13 [model_loader.py:318] Loading model with MODEL_IMPL_TYPE=vllm
PASSED
tests/models/common/test_model_loader.py::TestGetModel::test_get_model_flax_fallback_on_unsupported_arch INFO 11-05 06:35:13 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:35:13 [model.py:1728] Using max model len 40960
INFO 11-05 06:35:13 [model_loader.py:318] Loading model with MODEL_IMPL_TYPE=flax_nnx
WARNING 11-05 06:35:13 [model_loader.py:328] Flax model failed with: 'Model not supported'. Falling back to vLLM implementation.
PASSED
tests/models/common/test_model_loader.py::TestGetModel::test_get_model_flax_reraises_other_errors INFO 11-05 06:35:13 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:35:13 [model.py:1728] Using max model len 40960
INFO 11-05 06:35:13 [model_loader.py:318] Loading model with MODEL_IMPL_TYPE=flax_nnx
PASSED
tests/models/common/test_model_loader.py::TestGetModel::test_get_model_not_implemented INFO 11-05 06:35:13 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:35:13 [model.py:1728] Using max model len 40960
INFO 11-05 06:35:13 [model_loader.py:318] Loading model with MODEL_IMPL_TYPE=jetpack
PASSED
tests/models/jax/test_attention_interface.py::test_attention FAILED
tests/models/jax/test_llama3.py::TestLlamaForCausalLM::test_llama32_1b[mock_vllm_config0] FAILED
tests/models/jax/test_llama3.py::TestLlamaForCausalLM::test_llama32_1b[mock_vllm_config1] FAILED
tests/models/jax/test_llama4.py::TestLlama4ForCausalLM::test_init_llama4 PASSED
tests/models/jax/test_llama4.py::TestLlama4ForCausalLM::test_create_model_with_random_weights PASSED
tests/models/jax/test_llama4.py::TestLlama4ForCausalLM::test_load_weights_called_correctly PASSED
tests/models/jax/test_llama4.py::TestLlama4WeightLoader::test_get_layer_num[language_model.model.layers.15.self_attn.q_proj.weight-15] PASSED
tests/models/jax/test_llama4.py::TestLlama4WeightLoader::test_get_layer_num[layers.0.feed_forward.router.weight-0] PASSED
tests/models/jax/test_llama4.py::TestLlama4WeightLoader::test_get_layer_num[language_model.model.layers.99.norm.weight-99] PASSED
tests/models/jax/test_llama4.py::TestLlama4WeightLoader::test_get_layer_num[language_model.model.norm.weight-None] PASSED
tests/models/jax/test_llama4.py::TestLlama4WeightLoader::test_get_layer_num[language_model.model.embed_tokens.weight-None] PASSED
tests/models/jax/test_llama4.py::TestLlama4WeightLoader::test_map_loaded_to_standardized_name[language_model.model.layers.15.self_attn.q_proj.weight-layers.15.attn.kernel_q_proj_DNH] PASSED
tests/models/jax/test_llama4.py::TestLlama4WeightLoader::test_map_loaded_to_standardized_name[language_model.model.layers.0.feed_forward.shared_expert.down_proj.weight-layers.0.shared_experts.kernel_down_proj_FD] PASSED
tests/models/jax/test_llama4.py::TestLlama4WeightLoader::test_map_loaded_to_standardized_name[language_model.model.embed_tokens.weight-embedder.input_embedding_table_VD] PASSED
tests/models/jax/test_llama4.py::TestLlama4WeightLoader::test_map_loaded_to_standardized_name[language_model.model.norm.weight-final_norm.scale] PASSED
tests/models/jax/test_llama4.py::TestLlama4WeightLoader::test_map_loaded_to_standardized_name[language_model.lm_head.weight-lm_head.input_embedding_table_DV] PASSED
tests/models/jax/test_llama4.py::TestLlama4WeightLoader::test_map_loaded_to_standardized_name[unmapped.key.name-unmapped.key.name] PASSED
tests/models/jax/test_llama4.py::TestLlama4WeightLoader::test_load_weights_transformation PASSED
tests/models/jax/test_llama4.py::TestLlama4WeightLoader::test_map_llama4_gate_up_proj PASSED
tests/models/jax/test_llama_eagle3.py::TestEagleLlama3ForCausalLM::test_eagle3_decoder_layer_init INFO 11-05 06:35:27 [model.py:630] Resolved architecture: LlamaForCausalLM
INFO 11-05 06:35:27 [model.py:1728] Using max model len 8192
INFO 11-05 06:35:27 [model.py:630] Resolved architecture: LlamaForCausalLM
WARNING 11-05 06:35:27 [model.py:1954] Casting torch.float16 to torch.bfloat16.
INFO 11-05 06:35:27 [model.py:1728] Using max model len 2048
PASSED
tests/models/jax/test_llama_eagle3.py::TestEagleLlama3ForCausalLM::test_forward_pass[mock_vllm_config0] WARNING 11-05 06:35:44 [tuned_block_sizes.py:4070] Couldn`t find tuned sizes for the RPA v3 kernel with ('TPU v6e', 16, 'q_bfloat16_kv_bfloat16', 'q_head-32_kv_head-8_head-128', 16)
PASSED
tests/models/jax/test_llama_eagle3.py::TestEagleLlama3ForCausalLM::test_forward_pass[mock_vllm_config1] WARNING 11-05 06:35:50 [tuned_block_sizes.py:4070] Couldn`t find tuned sizes for the RPA v3 kernel with ('TPU v6e', 16, 'q_bfloat16_kv_float8_e4m3fn', 'q_head-32_kv_head-8_head-128', 16)
PASSED
tests/models/jax/test_llama_eagle3.py::TestEagleLlama3ForCausalLM::test_load_weights INFO 11-05 06:35:52 [model.py:630] Resolved architecture: LlamaForCausalLM
INFO 11-05 06:35:52 [model.py:1728] Using max model len 8192
INFO 11-05 06:35:52 [model.py:630] Resolved architecture: LlamaForCausalLM
WARNING 11-05 06:35:52 [model.py:1954] Casting torch.float16 to torch.bfloat16.
INFO 11-05 06:35:52 [model.py:1728] Using max model len 2048
PASSED
tests/models/jax/test_phi3.py::TestPhi3ForCausalLM::test_phi3_mini[mock_vllm_config0] FAILED
tests/models/jax/test_phi3.py::TestPhi3ForCausalLM::test_phi3_mini[mock_vllm_config1] FAILED
tests/models/jax/test_qwen2.py::TestQwen2ForCausalLM::test_qwen25_1_5b[mock_vllm_config0] ERROR
tests/models/jax/test_qwen2.py::TestQwen2ForCausalLM::test_qwen25_1_5b[mock_vllm_config1] ERROR
tests/models/jax/test_qwen2_5_vl.py::TestUtils::test_apply_rotary_pos_emb_vision PASSED
tests/models/jax/test_qwen2_5_vl.py::TestUtils::test_generate_window_segment_ids PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VisionMLP::test_forward PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VisionAttention::test_forward_fullattn PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VisionAttention::test_forward_windowed PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VisionAttention::test_batch_fail PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VisionBlock::test_forward PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VisionPatchEmbed::test_forward PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VisionPatchMerger::test_forward PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VisionRotaryEmbedding::test_forward PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VisionTransformer::test_rotary_pos_emb_thw PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VisionTransformer::test_get_window_index_thw PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VisionTransformer::test_get_rope_by_thw PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VisionTransformer::test_call PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VLForConditionalGeneration::test_validate_and_reshape_mm_tensor PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VLForConditionalGeneration::test_parse_and_validate_image_input PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VLForConditionalGeneration::test_parse_and_validate_multimodal_inputs PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VLForConditionalGeneration::test_process_image_input_pixels PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VLForConditionalGeneration::test_get_multimodal_embeddings PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VLForConditionalGeneration::test_get_input_embeddings PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VLForConditionalGeneration::test_call PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VLForConditionalGeneration::test_compute_logits PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VLForConditionalGeneration::test_load_weights PASSED
tests/models/jax/test_qwen2_5_vl.py::TestQwen2_5_VLForConditionalGeneration::test_load_weights_tied PASSED
tests/models/jax/test_qwen3.py::TestQwen3ForCausalLM::test_qwen3_600M[mock_vllm_config0] FAILED
tests/models/jax/test_qwen3.py::TestQwen3ForCausalLM::test_qwen3_600M[mock_vllm_config1] FAILED
tests/models/jax/test_weight_loading.py::WeightTransfer::test_transfer_state INFO 11-05 06:36:19 [weight_utils.py:475] mappings={'layers.*.kernel': ('model.layers.*.mlp.up_proj.kernel', (None,)), 'layers.*.bias': ('model.layers.*.mlp.up_proj.bias', (None,)), 'src_lm_head': ('tgt_lm_head', (None, None))}
INFO 11-05 06:36:19 [weight_utils.py:476] transpose_keys=None
INFO 11-05 06:36:19 [weight_utils.py:480] Processing source key: layers.0.bias and value: (4,) float32
INFO 11-05 06:36:19 [weight_utils.py:480] Processing source key: layers.0.kernel and value: (4, 4) float32
INFO 11-05 06:36:19 [weight_utils.py:480] Processing source key: src_lm_head and value: (2, 4) float32
PASSED
tests/models/jax/test_weight_loading.py::WeightLoadingDtypeTest::test_keep_original_dtype INFO 11-05 06:36:19 [weight_utils.py:96] Found weights from local: /tmp/tmpmxy6nrla
INFO 11-05 06:36:19 [weight_utils.py:122] Loading weights from /tmp/tmpmxy6nrla/model.safetensors
WARNING 11-05 06:36:19 [weight_utils.py:292] Converting dtype for weight_to_cast.weight from float32 to <class 'jax.numpy.bfloat16'>
PASSED
tests/models/jax/utils/test_multi_modal_utils.py::test_sanity_check_valid_list PASSED
tests/models/jax/utils/test_multi_modal_utils.py::test_sanity_check_valid_tuple PASSED
tests/models/jax/utils/test_multi_modal_utils.py::test_sanity_check_valid_3d_jax_array PASSED
tests/models/jax/utils/test_multi_modal_utils.py::test_sanity_check_invalid_type PASSED
tests/models/jax/utils/test_multi_modal_utils.py::test_sanity_check_wrong_num_items PASSED
tests/models/jax/utils/test_multi_modal_utils.py::test_sanity_check_wrong_dimensions_in_list PASSED
tests/models/jax/utils/test_multi_modal_utils.py::test_flatten_single_array PASSED
tests/models/jax/utils/test_multi_modal_utils.py::test_flatten_single_3d_array PASSED
tests/models/jax/utils/test_multi_modal_utils.py::test_flatten_list_of_arrays PASSED
tests/models/jax/utils/test_multi_modal_utils.py::test_flatten_nested_list PASSED
tests/models/jax/utils/test_multi_modal_utils.py::test_merge_single_placeholder PASSED
tests/models/jax/utils/test_multi_modal_utils.py::test_merge_no_placeholders PASSED
tests/models/jax/utils/test_multi_modal_utils.py::test_merge_mm_embeds_count_too_few[-1] PASSED
tests/models/jax/utils/test_multi_modal_utils.py::test_merge_mm_embeds_count_too_few[placeholder_id1] PASSED
tests/models/jax/utils/test_multi_modal_utils.py::test_merge_mm_embeds_count_too_many_no_raise[-1] PASSED
tests/models/jax/utils/test_multi_modal_utils.py::test_merge_mm_embeds_count_too_many_no_raise[placeholder_id1] PASSED
tests/runner/test_block_table_jax.py::TestBlockTable::test_init PASSED
tests/runner/test_block_table_jax.py::TestBlockTable::test_add_and_append_row PASSED
tests/runner/test_block_table_jax.py::TestBlockTable::test_move_row PASSED
tests/runner/test_block_table_jax.py::TestBlockTable::test_swap_row PASSED
tests/runner/test_block_table_jax.py::TestBlockTable::test_commit PASSED
tests/runner/test_block_table_jax.py::TestBlockTable::test_clear PASSED
tests/runner/test_block_table_jax.py::TestMultiGroupBlockTable::test_init PASSED
tests/runner/test_block_table_jax.py::TestMultiGroupBlockTable::test_add_row PASSED
tests/runner/test_block_table_jax.py::TestMultiGroupBlockTable::test_swap_row PASSED
tests/runner/test_block_table_jax.py::TestMultiGroupBlockTable::test_commit_and_clear PASSED
tests/runner/test_input_batch_jax.py::test_initialization PASSED
tests/runner/test_input_batch_jax.py::test_add_request PASSED
tests/runner/test_input_batch_jax.py::test_add_multiple_requests PASSED
tests/runner/test_input_batch_jax.py::test_remove_request PASSED
tests/runner/test_input_batch_jax.py::test_condense PASSED
tests/runner/test_input_batch_jax.py::test_swap_states PASSED
tests/runner/test_input_batch_jax.py::test_all_greedy_property PASSED
tests/runner/test_kv_cache.py::test_create_kv_caches FAILED
tests/runner/test_kv_cache_manager.py::TestKVCacheManager::test_insert_request_with_kv_cache INFO 11-05 06:36:21 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:21 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:21 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:21 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:21 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:21 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:21 [tpu_jax_runner.py:275] Init mesh | mesh=Mesh('data': 1, 'model': 1, axis_types=(Auto, Auto))
INFO 11-05 06:36:21 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:21 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:21 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_kv_cache_manager.py::TestKVCacheManager::test_get_kv_cache_spec_with_compilation_cfg INFO 11-05 06:36:21 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:21 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:21 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:21 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:21 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:21 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:21 [tpu_jax_runner.py:275] Init mesh | mesh=Mesh('data': 1, 'model': 1, axis_types=(Auto, Auto))
INFO 11-05 06:36:21 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:21 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:21 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_kv_cache_manager.py::TestKVCacheManager::test_get_kv_cache_spec_with_compilation_cfg_mla INFO 11-05 06:36:21 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:21 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:21 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:21 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:21 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:21 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:21 [tpu_jax_runner.py:275] Init mesh | mesh=Mesh('data': 1, 'model': 1, axis_types=(Auto, Auto))
INFO 11-05 06:36:21 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:21 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:21 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_kv_cache_manager.py::TestKVCacheManager::test_get_kv_cache_spec_without_compilation_cfg INFO 11-05 06:36:22 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:22 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:22 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:22 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:22 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:22 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:22 [tpu_jax_runner.py:275] Init mesh | mesh=Mesh('data': 1, 'model': 1, axis_types=(Auto, Auto))
INFO 11-05 06:36:22 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:22 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:22 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_kv_cache_manager.py::TestKVCacheManager::test_get_kv_cache_spec_without_compilation_cfg_mla INFO 11-05 06:36:22 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:22 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:22 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:22 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:22 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:22 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:22 [tpu_jax_runner.py:275] Init mesh | mesh=Mesh('data': 1, 'model': 1, axis_types=(Auto, Auto))
INFO 11-05 06:36:22 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:22 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:22 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_kv_cache_manager.py::TestKVCacheManager::test_initialize_kv_cache INFO 11-05 06:36:22 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:22 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:22 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:22 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:22 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:22 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:22 [tpu_jax_runner.py:275] Init mesh | mesh=Mesh('data': 1, 'model': 1, axis_types=(Auto, Auto))
INFO 11-05 06:36:22 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:22 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:22 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_kv_cache_manager.py::TestKVCacheManager::test_get_kv_cache_spec_with_eagle3 INFO 11-05 06:36:22 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:22 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:22 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:22 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:22 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:22 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:22 [tpu_jax_runner.py:275] Init mesh | mesh=Mesh('data': 1, 'model': 1, axis_types=(Auto, Auto))
INFO 11-05 06:36:22 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:22 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:22 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_kv_cache_manager.py::TestKVCacheManager::test_get_kv_cache_spec_with_eagle3_mla INFO 11-05 06:36:23 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:23 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:23 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:23 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:23 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:23 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:23 [tpu_jax_runner.py:275] Init mesh | mesh=Mesh('data': 1, 'model': 1, axis_types=(Auto, Auto))
INFO 11-05 06:36:23 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:23 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:23 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_multimodal_manager.py::TestMultiModalManager::test_execute_mm_encoder_single_image INFO 11-05 06:36:23 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:23 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:23 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:23 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:23 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:23 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:23 [tpu_jax_runner.py:275] Init mesh | mesh=<MagicMock id='132103272212880'>
INFO 11-05 06:36:23 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:23 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:23 [utils.py:93] Prepared token paddings: [8, 16, 32, 64, 128]
INFO 11-05 06:36:23 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_multimodal_manager.py::TestMultiModalManager::test_execute_mm_encoder_multiple_images INFO 11-05 06:36:23 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:23 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:23 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:23 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:23 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:23 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:23 [tpu_jax_runner.py:275] Init mesh | mesh=<MagicMock id='132103271528256'>
INFO 11-05 06:36:23 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:23 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:23 [utils.py:93] Prepared token paddings: [8, 16, 32, 64, 128]
INFO 11-05 06:36:23 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_multimodal_manager.py::TestMultiModalManager::test_gather_mm_embeddings_chunked_prefill INFO 11-05 06:36:23 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:23 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:23 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:23 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:23 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:23 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:23 [tpu_jax_runner.py:275] Init mesh | mesh=<MagicMock id='132103271456480'>
INFO 11-05 06:36:23 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:23 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:23 [utils.py:93] Prepared token paddings: [8, 16, 32, 64, 128]
INFO 11-05 06:36:23 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_multimodal_manager.py::TestMultiModalManager::test_calc_mrope_positions INFO 11-05 06:36:24 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:24 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:24 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:24 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:24 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:24 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:24 [tpu_jax_runner.py:275] Init mesh | mesh=<MagicMock id='132103259466896'>
INFO 11-05 06:36:24 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:24 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:24 [utils.py:93] Prepared token paddings: [8, 16, 32, 64, 128]
INFO 11-05 06:36:24 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_speculative_decoding_manager.py::TestSpeculativeDecodingManager::test_propose_draft_token_ids_dispatches_to_eagle INFO 11-05 06:36:24 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:24 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:24 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:24 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:24 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:24 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:24 [tpu_jax_runner.py:275] Init mesh | mesh=<MagicMock id='132103271901056'>
INFO 11-05 06:36:24 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:24 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:24 [utils.py:93] Prepared token paddings: [8, 16, 32, 64, 128]
INFO 11-05 06:36:24 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_speculative_decoding_manager.py::TestSpeculativeDecodingManager::test_propose_draft_token_ids_wrong_drafter_type INFO 11-05 06:36:24 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:24 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:24 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:24 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:24 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:24 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:24 [tpu_jax_runner.py:275] Init mesh | mesh=<MagicMock id='132103284046160'>
INFO 11-05 06:36:24 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:24 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:24 [utils.py:93] Prepared token paddings: [8, 16, 32, 64, 128]
INFO 11-05 06:36:24 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_speculative_decoding_manager.py::TestSpeculativeDecodingManager::test_take_draft_token_ids INFO 11-05 06:36:24 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:24 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:24 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:24 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:24 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:24 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:24 [tpu_jax_runner.py:275] Init mesh | mesh=<MagicMock id='132103255402320'>
INFO 11-05 06:36:24 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:24 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:24 [utils.py:93] Prepared token paddings: [8, 16, 32, 64, 128]
INFO 11-05 06:36:24 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_speculative_decoding_manager.py::TestSpeculativeDecodingManager::test_get_spec_decode_metadata_parametrized[num_draft_tokens0-cu_num_scheduled_tokens0-8-expected_logits_indices0-expected_bonus_logits_indices0-expected_target_logits_indices0-expected_draft_token_ids0] INFO 11-05 06:36:24 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:24 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:24 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:24 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:24 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:24 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:24 [tpu_jax_runner.py:275] Init mesh | mesh=<MagicMock id='132103255188416'>
INFO 11-05 06:36:24 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:24 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:24 [utils.py:93] Prepared token paddings: [8, 16, 32, 64, 128]
INFO 11-05 06:36:24 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_speculative_decoding_manager.py::TestSpeculativeDecodingManager::test_get_spec_decode_metadata_parametrized[num_draft_tokens1-cu_num_scheduled_tokens1-8-expected_logits_indices1-expected_bonus_logits_indices1-expected_target_logits_indices1-expected_draft_token_ids1] INFO 11-05 06:36:25 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:25 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:25 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:25 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:25 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:25 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:25 [tpu_jax_runner.py:275] Init mesh | mesh=<MagicMock id='132103255429904'>
INFO 11-05 06:36:25 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:25 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:25 [utils.py:93] Prepared token paddings: [8, 16, 32, 64, 128]
INFO 11-05 06:36:25 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_speculative_decoding_manager.py::TestSpeculativeDecodingManager::test_propose_eagle3_draft_token_ids[True] INFO 11-05 06:36:25 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:25 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:25 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:25 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:25 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:25 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:25 [tpu_jax_runner.py:275] Init mesh | mesh=<MagicMock id='132103255578800'>
INFO 11-05 06:36:25 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:25 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:25 [utils.py:93] Prepared token paddings: [8, 16, 32, 64, 128]
INFO 11-05 06:36:25 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_speculative_decoding_manager.py::TestSpeculativeDecodingManager::test_propose_eagle3_draft_token_ids[False] INFO 11-05 06:36:25 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:25 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:25 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:25 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:25 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:25 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:25 [tpu_jax_runner.py:275] Init mesh | mesh=<MagicMock id='132103255520560'>
INFO 11-05 06:36:25 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:25 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:25 [utils.py:93] Prepared token paddings: [8, 16, 32, 64, 128]
INFO 11-05 06:36:25 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_structured_decoding_manager.py::TestStructuredDecodingManager::test_structured_decoding INFO 11-05 06:36:25 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:25 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:25 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:25 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:25 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:25 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:25 [tpu_jax_runner.py:275] Init mesh | mesh=<MagicMock id='132103250914304'>
INFO 11-05 06:36:25 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:25 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:25 [utils.py:93] Prepared token paddings: [8, 16, 32, 64, 128]
INFO 11-05 06:36:25 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_tpu_jax_runner.py::TestTPUJaxRunner::test_get_supported_tasks_runner INFO 11-05 06:36:25 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:25 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:25 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:25 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:25 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:25 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:25 [tpu_jax_runner.py:275] Init mesh | mesh=<MagicMock id='132103250734128'>
INFO 11-05 06:36:25 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:25 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:25 [utils.py:93] Prepared token paddings: [8, 16, 32, 64, 128]
INFO 11-05 06:36:25 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_tpu_jax_runner.py::TestTPUJaxRunner::test_get_input_ids_embeds INFO 11-05 06:36:26 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:26 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:26 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:26 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:26 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:26 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:26 [tpu_jax_runner.py:275] Init mesh | mesh=<MagicMock id='132103250942800'>
INFO 11-05 06:36:26 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:26 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:26 [utils.py:93] Prepared token paddings: [8, 16, 32, 64, 128]
INFO 11-05 06:36:26 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_tpu_jax_runner.py::TestTPUJaxRunnerMultimodalModelLoadedForTextOnly::test_is_multimodal_model INFO 11-05 06:36:26 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:26 [model.py:1728] Using max model len 40960
INFO 11-05 06:36:26 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:26 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:26 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:26 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:26 [tpu_jax_runner.py:275] Init mesh | mesh=<MagicMock id='132103251003392'>
INFO 11-05 06:36:26 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
INFO 11-05 06:36:26 [utils.py:59] Prepared request paddings: [8, 16]
INFO 11-05 06:36:26 [compilation_manager.py:34] Enabling JAX compile cache.
ERROR
tests/runner/test_tpu_jax_runner_dp.py::TestTPUJaxRunnerDPInputsLightweight::test_prepare_inputs_dp_basic_functionality PASSED
tests/runner/test_tpu_jax_runner_dp.py::TestTPUJaxRunnerDPInputsLightweight::test_prepare_inputs_dp_error_conditions PASSED
tests/runner/test_tpu_jax_runner_dp.py::TestTPUJaxRunnerDPInputsLightweight::test_prepare_dp_input_metadata PASSED
tests/runner/test_tpu_jax_runner_dp.py::TestTPUJaxRunnerDPInputsLightweight::test_prepare_dp_input_metadata_empty_rank PASSED
tests/runner/test_tpu_jax_runner_dp.py::TestTPUJaxRunnerDPInputsLightweight::test_prepare_dp_input_metadata_logits_indices_selector_ordering PASSED
tests/runner/test_tpu_jax_runner_dp.py::TestTPUJaxRunnerDPInputsLightweight::test_prepare_inputs_dp_verify_content_balanced PASSED
tests/runner/test_tpu_jax_runner_dp.py::TestTPUJaxRunnerDPInputsLightweight::test_prepare_inputs_dp_verify_content_empty_rank PASSED
tests/runner/test_utils.py::test_get_padded_num_reqs_with_upper_limit PASSED
tests/runner/test_utils.py::test_get_paddings INFO 11-05 06:36:27 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 192, 256, 320, 384, 448, 512]
INFO 11-05 06:36:27 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 192, 256, 320]
INFO 11-05 06:36:27 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024]
INFO 11-05 06:36:27 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512]
PASSED
tests/runner/test_utils.py::test_get_padded_token_len INFO 11-05 06:36:27 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 192, 256, 320, 384, 448, 512]
PASSED
tests/runner/test_utils.py::test_get_req_paddings INFO 11-05 06:36:27 [utils.py:59] Prepared request paddings: [8, 16, 32]
INFO 11-05 06:36:27 [utils.py:59] Prepared request paddings: [8, 16, 32]
INFO 11-05 06:36:27 [utils.py:59] Prepared request paddings: [8, 16, 32, 36]
PASSED
tests/runner/test_utils.py::test_latency_tracker PASSED
tests/runner/test_utils.py::test_forbid_compile_raises_error_on_first_call PASSED
tests/runner/test_utils.py::test_forbid_compile_succeeds_on_cached_call PASSED
tests/runner/test_utils.py::test_forbid_compile_restores_original_function PASSED
tests/runner/test_utils.py::test_forbid_compile_with_exception PASSED
tests/runner/test_utils.py::test_forbid_compile_raises_on_new_shape PASSED
tests/runner/test_utils.py::test_get_batch_composition_stats[prefill_only-2-req_ids0-computed0-scheduled0-150-0] PASSED
tests/runner/test_utils.py::test_get_batch_composition_stats[decode_only-3-req_ids1-computed1-scheduled1-0-3] PASSED
tests/runner/test_utils.py::test_get_batch_composition_stats[mixed_batch-4-req_ids2-computed2-scheduled2-150-2] PASSED
tests/runner/test_utils.py::test_get_batch_composition_stats[chunked_prefill-2-req_ids3-computed3-scheduled3-50-1] PASSED
tests/runner/test_utils.py::test_determine_phase_from_batch_composition_stats[90-100-InferencePhase.PREFILL_HEAVY] PASSED
tests/runner/test_utils.py::test_determine_phase_from_batch_composition_stats[89-100-InferencePhase.AMBIGUOUS] PASSED
tests/runner/test_utils.py::test_determine_phase_from_batch_composition_stats[15-100-InferencePhase.DECODE_HEAVY] PASSED
tests/runner/test_utils.py::test_determine_phase_from_batch_composition_stats[50-100-InferencePhase.BALANCED0] PASSED
tests/runner/test_utils.py::test_determine_phase_from_batch_composition_stats[70-100-InferencePhase.AMBIGUOUS] PASSED
tests/runner/test_utils.py::test_determine_phase_from_batch_composition_stats[30-100-InferencePhase.AMBIGUOUS] PASSED
tests/runner/test_utils.py::test_determine_phase_from_batch_composition_stats[40-100-InferencePhase.BALANCED] PASSED
tests/runner/test_utils.py::test_determine_phase_from_batch_composition_stats[50-100-InferencePhase.BALANCED1] PASSED
tests/runner/test_utils.py::test_determine_phase_from_batch_composition_stats[60-100-InferencePhase.BALANCED] PASSED
tests/runner/test_utils.py::test_determine_phase_from_batch_composition_stats[100-100-InferencePhase.PREFILL_HEAVY] PASSED
tests/runner/test_utils.py::test_determine_phase_from_batch_composition_stats[20-100-InferencePhase.DECODE_HEAVY] PASSED
tests/runner/test_utils.py::test_determine_phase_from_batch_composition_stats[21-100-InferencePhase.AMBIGUOUS] PASSED
tests/runner/test_utils.py::test_determine_phase_from_batch_composition_stats[0-100-InferencePhase.DECODE_HEAVY] PASSED
tests/runner/test_utils.py::test_phased_profiler_full_cycle INFO 11-05 06:36:27 [utils.py:314] Phased-based profiler enabled. Traces will be saved to: /tmp/pytest-of-wenxindong_google_com/pytest-0/test_phased_profiler_full_cycl0
INFO 11-05 06:36:27 [utils.py:358] Starting profiling for prefill_heavy phase
INFO 11-05 06:36:27 [utils.py:359] Batch composition stats: {'num_reqs': 2, 'total_num_scheduled_tokens': 100}
INFO 11-05 06:36:27 [utils.py:397] Profiling for prefill_heavy phase finished
PASSED
tests/runner/test_utils.py::test_phased_profiler_ignores_initial_request INFO 11-05 06:36:27 [utils.py:314] Phased-based profiler enabled. Traces will be saved to: /tmp/pytest-of-wenxindong_google_com/pytest-0/test_phased_profiler_ignores_i0
INFO 11-05 06:36:27 [utils.py:358] Starting profiling for prefill_heavy phase
INFO 11-05 06:36:27 [utils.py:359] Batch composition stats: {'num_reqs': 2, 'total_num_scheduled_tokens': 2}
PASSED
tests/runner/test_utils.py::test_phased_profiler_handles_all_phases INFO 11-05 06:36:27 [utils.py:314] Phased-based profiler enabled. Traces will be saved to: /tmp/pytest-of-wenxindong_google_com/pytest-0/test_phased_profiler_handles_a0
INFO 11-05 06:36:27 [utils.py:358] Starting profiling for prefill_heavy phase
INFO 11-05 06:36:27 [utils.py:359] Batch composition stats: {'num_reqs': 2, 'total_num_scheduled_tokens': 100}
INFO 11-05 06:36:27 [utils.py:397] Profiling for prefill_heavy phase finished
INFO 11-05 06:36:27 [utils.py:358] Starting profiling for decode_heavy phase
INFO 11-05 06:36:27 [utils.py:359] Batch composition stats: {'num_reqs': 2, 'total_num_scheduled_tokens': 100}
INFO 11-05 06:36:27 [utils.py:397] Profiling for decode_heavy phase finished
INFO 11-05 06:36:27 [utils.py:358] Starting profiling for balanced phase
INFO 11-05 06:36:27 [utils.py:359] Batch composition stats: {'num_reqs': 2, 'total_num_scheduled_tokens': 100}
INFO 11-05 06:36:27 [utils.py:397] Profiling for balanced phase finished
PASSED
tests/spec_decode/test_eagle3.py::test_prepare_inputs INFO 11-05 06:36:27 [model.py:630] Resolved architecture: LlamaForCausalLM
INFO 11-05 06:36:27 [model.py:1728] Using max model len 8192
INFO 11-05 06:36:27 [model.py:630] Resolved architecture: LlamaForCausalLM
WARNING 11-05 06:36:27 [model.py:1954] Casting torch.float16 to torch.bfloat16.
INFO 11-05 06:36:27 [model.py:1728] Using max model len 2048
INFO 11-05 06:36:27 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:27 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:27 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:27 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
INFO 11-05 06:36:28 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024]
PASSED
tests/spec_decode/test_eagle3.py::test_propose[1-eagle3] INFO 11-05 06:36:28 [model.py:630] Resolved architecture: LlamaForCausalLM
INFO 11-05 06:36:28 [model.py:1728] Using max model len 8192
INFO 11-05 06:36:28 [model.py:630] Resolved architecture: LlamaForCausalLM
WARNING 11-05 06:36:28 [model.py:1954] Casting torch.float16 to torch.bfloat16.
INFO 11-05 06:36:28 [model.py:1728] Using max model len 2048
INFO 11-05 06:36:28 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:28 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:28 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:28 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
PASSED
tests/spec_decode/test_eagle3.py::test_propose[3-eagle3] INFO 11-05 06:36:29 [model.py:630] Resolved architecture: LlamaForCausalLM
INFO 11-05 06:36:29 [model.py:1728] Using max model len 8192
INFO 11-05 06:36:30 [model.py:630] Resolved architecture: LlamaForCausalLM
WARNING 11-05 06:36:30 [model.py:1954] Casting torch.float16 to torch.bfloat16.
INFO 11-05 06:36:30 [model.py:1728] Using max model len 2048
INFO 11-05 06:36:30 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:30 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:30 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:30 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
PASSED
tests/spec_decode/test_eagle3.py::test_propose[8-eagle3] INFO 11-05 06:36:30 [model.py:630] Resolved architecture: LlamaForCausalLM
INFO 11-05 06:36:30 [model.py:1728] Using max model len 8192
INFO 11-05 06:36:30 [model.py:630] Resolved architecture: LlamaForCausalLM
WARNING 11-05 06:36:30 [model.py:1954] Casting torch.float16 to torch.bfloat16.
INFO 11-05 06:36:30 [model.py:1728] Using max model len 2048
INFO 11-05 06:36:30 [tpu_jax.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)
WARNING 11-05 06:36:30 [tpu_jax.py:156] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:30 [tpu_jax.py:192] Force using UniProcExecutor for JAX on single host.
INFO 11-05 06:36:30 [dp_scheduler.py:525] DP size (1) < 2, using default Scheduler
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_init_success INFO 11-05 06:36:31 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:31 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:31 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_init_with_profiler_on_rank_zero INFO 11-05 06:36:31 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:31 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:31 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:31 [tpu_worker_jax.py:94] Profiling enabled. Traces will be saved to: /tmp/profiles
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_init_with_profiler_on_other_ranks INFO 11-05 06:36:31 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:31 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:31 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_initialize_cache INFO 11-05 06:36:31 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:31 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:31 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_init_device_with_provided_devices INFO 11-05 06:36:32 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:32 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:32 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:32 [tpu_worker_jax.py:152] Init worker | rank=0 | node_id=0 | is_driver_worker=False | hbm=<MagicMock name='utils.hbm_usage_gb()' id='132103267794816'>GiB
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_init_device_autodetects_devices INFO 11-05 06:36:32 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:32 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:32 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
FAILED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_determine_available_memory INFO 11-05 06:36:32 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:32 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:32 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
INFO 11-05 06:36:32 [tpu_worker_jax.py:175] Memory statistics | total_hbm_limit_gb=<MagicMock name='utils.GBYTES.__rtruediv__().__round__()' id='132103258931568'>GiB | total_hbm_limit_cap_gb=<MagicMock name='utils.GBYTES.__rtruediv__().__round__()' id='132103258931568'>GiB | total_hbm_used_gb=<MagicMock name='utils.GBYTES.__rtruediv__().__round__()' id='132103258931568'>GiB | total_hbm_avail_gb=<MagicMock name='utils.GBYTES.__rtruediv__().__round__()' id='132103258931568'>GiB
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_execute_model INFO 11-05 06:36:32 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:32 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:32 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_execute_model_non_driver_returns_none INFO 11-05 06:36:32 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:32 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:32 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_take_draft_token_ids INFO 11-05 06:36:33 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:33 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:33 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_add_lora_not_implemented INFO 11-05 06:36:33 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:33 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:33 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_add_lora_not_implemented_lora_request INFO 11-05 06:36:33 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:33 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:33 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_profile_start INFO 11-05 06:36:33 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:33 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:33 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_profile_stop INFO 11-05 06:36:33 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:33 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:33 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_check_health INFO 11-05 06:36:34 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:34 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:34 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_runner_passthrough_methods[load_model-load_model-method_args0] INFO 11-05 06:36:34 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:34 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:34 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_runner_passthrough_methods[get_model-get_model-method_args1] INFO 11-05 06:36:34 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:34 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:34 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_runner_passthrough_methods[get_kv_cache_spec-get_kv_cache_spec-method_args2] INFO 11-05 06:36:34 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:34 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:34 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_initialize_from_config INFO 11-05 06:36:35 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:35 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:35 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_initialize_from_config_kv_cache_config INFO 11-05 06:36:35 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:35 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:35 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_compile_or_warm_up_model INFO 11-05 06:36:35 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:35 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:35 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
PASSED
tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_get_supported_tasks INFO 11-05 06:36:35 [model.py:630] Resolved architecture: Qwen3ForCausalLM
INFO 11-05 06:36:35 [model.py:1728] Using max model len 40960
WARNING 11-05 06:36:35 [tpu_worker_jax.py:58] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16
PASSED

==================================== ERRORS ====================================
__ ERROR at setup of TestQwen2ForCausalLM.test_qwen25_1_5b[mock_vllm_config0] __

    @pytest.fixture(scope="module")
    def mesh():
        """
        Creates a mesh with 1 device.
        """
        if not jax.devices():
            pytest.skip("No JAX devices available for mesh creation.")

        devices = np.array(jax.local_devices()[:1])
        num_devices = len(devices)
        assert num_devices == 1
        device_mesh = devices.reshape((num_devices, 1))

>       with Mesh(device_mesh, axis_names=('data', 'attn_dp', 'model')) as m:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/models/jax/test_qwen2.py:41:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'jax._src.mesh.Mesh'>
devices = array([[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]],
      dtype=object)
axis_names = ('data', 'attn_dp', 'model'), axis_types = None

    def __new__(cls, devices: np.ndarray | Sequence[xc.Device],
                axis_names: str | Sequence[MeshAxisName],
                axis_types: tuple[AxisType, ...] | None = None):
      if not isinstance(devices, np.ndarray):
        devices = np.array(devices)
      if isinstance(axis_names, str):
        axis_names = (axis_names,)
      axis_names = tuple(axis_names)
      if any(i is None for i in axis_names):
        raise ValueError(f"Mesh axis names cannot be None. Got: {axis_names}")

      if devices.ndim != len(axis_names):
>       raise ValueError(
            "Mesh requires the ndim of its first argument (`devices`) to equal "
            "the length of its second argument (`axis_names`), but got "
            f"devices.ndim == {devices.ndim} and "
            f"len(axis_names) == {len(axis_names)}.")
E       ValueError: Mesh requires the ndim of its first argument (`devices`) to equal the length of its second argument (`axis_names`), but got devices.ndim == 2 and len(axis_names) == 3.

../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/mesh.py:260: ValueError
__ ERROR at setup of TestQwen2ForCausalLM.test_qwen25_1_5b[mock_vllm_config1] __

    @pytest.fixture(scope="module")
    def mesh():
        """
        Creates a mesh with 1 device.
        """
        if not jax.devices():
            pytest.skip("No JAX devices available for mesh creation.")

        devices = np.array(jax.local_devices()[:1])
        num_devices = len(devices)
        assert num_devices == 1
        device_mesh = devices.reshape((num_devices, 1))

>       with Mesh(device_mesh, axis_names=('data', 'attn_dp', 'model')) as m:
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/models/jax/test_qwen2.py:41:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'jax._src.mesh.Mesh'>
devices = array([[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]],
      dtype=object)
axis_names = ('data', 'attn_dp', 'model'), axis_types = None

    def __new__(cls, devices: np.ndarray | Sequence[xc.Device],
                axis_names: str | Sequence[MeshAxisName],
                axis_types: tuple[AxisType, ...] | None = None):
      if not isinstance(devices, np.ndarray):
        devices = np.array(devices)
      if isinstance(axis_names, str):
        axis_names = (axis_names,)
      axis_names = tuple(axis_names)
      if any(i is None for i in axis_names):
        raise ValueError(f"Mesh axis names cannot be None. Got: {axis_names}")

      if devices.ndim != len(axis_names):
>       raise ValueError(
            "Mesh requires the ndim of its first argument (`devices`) to equal "
            "the length of its second argument (`axis_names`), but got "
            f"devices.ndim == {devices.ndim} and "
            f"len(axis_names) == {len(axis_names)}.")
E       ValueError: Mesh requires the ndim of its first argument (`devices`) to equal the length of its second argument (`axis_names`), but got devices.ndim == 2 and len(axis_names) == 3.

../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/mesh.py:260: ValueError
____ ERROR at setup of TestKVCacheManager.test_insert_request_with_kv_cache ____

self = <test_kv_cache_manager.TestKVCacheManager object at 0x7825fb0868d0>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_rng_key = MagicMock()

        # create 1x1 mesh
        devices = np.asarray(jax.devices()[:1])
        axis_names = ('data', 'model')
        mesh_shape = (1, 1)
        self.mock_mesh = jax.sharding.Mesh(devices.reshape(mesh_shape),
                                           axis_names)

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_kv_cache_manager.py:66:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tpu_inference/runner/tpu_jax_runner.py:249: in __init__
    self.data_parallel_attn_sharding = NamedSharding(
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:462: in check_pspec
    _check_mesh_resource_axis(mesh, spec)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

mesh = Mesh(axis_sizes=(1, 1), axis_names=('data', 'model'), axis_types=(Auto, Auto))
pspec = PartitionSpec(('data', 'attn_dp'),)

    def _check_mesh_resource_axis(mesh, pspec):
      for p in pspec:
        if p is PartitionSpec.UNCONSTRAINED or p is None:
          continue
        p = p if isinstance(p, tuple) else (p,)
        for r in p:
          if r not in mesh.axis_names:
>           raise ValueError(
                f"Resource axis: {r} of {pspec} "
                f"is not found in mesh: {tuple(mesh.shape.keys())}.")
E           ValueError: Resource axis: attn_dp of PartitionSpec(('data', 'attn_dp'),) is not found in mesh: ('data', 'model').

../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:517: ValueError
_ ERROR at setup of TestKVCacheManager.test_get_kv_cache_spec_with_compilation_cfg _

self = <test_kv_cache_manager.TestKVCacheManager object at 0x7825fb087c20>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_rng_key = MagicMock()

        # create 1x1 mesh
        devices = np.asarray(jax.devices()[:1])
        axis_names = ('data', 'model')
        mesh_shape = (1, 1)
        self.mock_mesh = jax.sharding.Mesh(devices.reshape(mesh_shape),
                                           axis_names)

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_kv_cache_manager.py:66:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tpu_inference/runner/tpu_jax_runner.py:249: in __init__
    self.data_parallel_attn_sharding = NamedSharding(
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:462: in check_pspec
    _check_mesh_resource_axis(mesh, spec)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

mesh = Mesh(axis_sizes=(1, 1), axis_names=('data', 'model'), axis_types=(Auto, Auto))
pspec = PartitionSpec(('data', 'attn_dp'),)

    def _check_mesh_resource_axis(mesh, pspec):
      for p in pspec:
        if p is PartitionSpec.UNCONSTRAINED or p is None:
          continue
        p = p if isinstance(p, tuple) else (p,)
        for r in p:
          if r not in mesh.axis_names:
>           raise ValueError(
                f"Resource axis: {r} of {pspec} "
                f"is not found in mesh: {tuple(mesh.shape.keys())}.")
E           ValueError: Resource axis: attn_dp of PartitionSpec(('data', 'attn_dp'),) is not found in mesh: ('data', 'model').

../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:517: ValueError
_ ERROR at setup of TestKVCacheManager.test_get_kv_cache_spec_with_compilation_cfg_mla _

self = <test_kv_cache_manager.TestKVCacheManager object at 0x7825fb087dd0>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_rng_key = MagicMock()

        # create 1x1 mesh
        devices = np.asarray(jax.devices()[:1])
        axis_names = ('data', 'model')
        mesh_shape = (1, 1)
        self.mock_mesh = jax.sharding.Mesh(devices.reshape(mesh_shape),
                                           axis_names)

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_kv_cache_manager.py:66:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tpu_inference/runner/tpu_jax_runner.py:249: in __init__
    self.data_parallel_attn_sharding = NamedSharding(
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:462: in check_pspec
    _check_mesh_resource_axis(mesh, spec)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

mesh = Mesh(axis_sizes=(1, 1), axis_names=('data', 'model'), axis_types=(Auto, Auto))
pspec = PartitionSpec(('data', 'attn_dp'),)

    def _check_mesh_resource_axis(mesh, pspec):
      for p in pspec:
        if p is PartitionSpec.UNCONSTRAINED or p is None:
          continue
        p = p if isinstance(p, tuple) else (p,)
        for r in p:
          if r not in mesh.axis_names:
>           raise ValueError(
                f"Resource axis: {r} of {pspec} "
                f"is not found in mesh: {tuple(mesh.shape.keys())}.")
E           ValueError: Resource axis: attn_dp of PartitionSpec(('data', 'attn_dp'),) is not found in mesh: ('data', 'model').

../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:517: ValueError
_ ERROR at setup of TestKVCacheManager.test_get_kv_cache_spec_without_compilation_cfg _

self = <test_kv_cache_manager.TestKVCacheManager object at 0x7825fb087f80>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_rng_key = MagicMock()

        # create 1x1 mesh
        devices = np.asarray(jax.devices()[:1])
        axis_names = ('data', 'model')
        mesh_shape = (1, 1)
        self.mock_mesh = jax.sharding.Mesh(devices.reshape(mesh_shape),
                                           axis_names)

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_kv_cache_manager.py:66:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tpu_inference/runner/tpu_jax_runner.py:249: in __init__
    self.data_parallel_attn_sharding = NamedSharding(
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:462: in check_pspec
    _check_mesh_resource_axis(mesh, spec)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

mesh = Mesh(axis_sizes=(1, 1), axis_names=('data', 'model'), axis_types=(Auto, Auto))
pspec = PartitionSpec(('data', 'attn_dp'),)

    def _check_mesh_resource_axis(mesh, pspec):
      for p in pspec:
        if p is PartitionSpec.UNCONSTRAINED or p is None:
          continue
        p = p if isinstance(p, tuple) else (p,)
        for r in p:
          if r not in mesh.axis_names:
>           raise ValueError(
                f"Resource axis: {r} of {pspec} "
                f"is not found in mesh: {tuple(mesh.shape.keys())}.")
E           ValueError: Resource axis: attn_dp of PartitionSpec(('data', 'attn_dp'),) is not found in mesh: ('data', 'model').

../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:517: ValueError
_ ERROR at setup of TestKVCacheManager.test_get_kv_cache_spec_without_compilation_cfg_mla _

self = <test_kv_cache_manager.TestKVCacheManager object at 0x7825fb0b8170>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_rng_key = MagicMock()

        # create 1x1 mesh
        devices = np.asarray(jax.devices()[:1])
        axis_names = ('data', 'model')
        mesh_shape = (1, 1)
        self.mock_mesh = jax.sharding.Mesh(devices.reshape(mesh_shape),
                                           axis_names)

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_kv_cache_manager.py:66:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tpu_inference/runner/tpu_jax_runner.py:249: in __init__
    self.data_parallel_attn_sharding = NamedSharding(
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:462: in check_pspec
    _check_mesh_resource_axis(mesh, spec)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

mesh = Mesh(axis_sizes=(1, 1), axis_names=('data', 'model'), axis_types=(Auto, Auto))
pspec = PartitionSpec(('data', 'attn_dp'),)

    def _check_mesh_resource_axis(mesh, pspec):
      for p in pspec:
        if p is PartitionSpec.UNCONSTRAINED or p is None:
          continue
        p = p if isinstance(p, tuple) else (p,)
        for r in p:
          if r not in mesh.axis_names:
>           raise ValueError(
                f"Resource axis: {r} of {pspec} "
                f"is not found in mesh: {tuple(mesh.shape.keys())}.")
E           ValueError: Resource axis: attn_dp of PartitionSpec(('data', 'attn_dp'),) is not found in mesh: ('data', 'model').

../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:517: ValueError
________ ERROR at setup of TestKVCacheManager.test_initialize_kv_cache _________

self = <test_kv_cache_manager.TestKVCacheManager object at 0x7825fb0b8320>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_rng_key = MagicMock()

        # create 1x1 mesh
        devices = np.asarray(jax.devices()[:1])
        axis_names = ('data', 'model')
        mesh_shape = (1, 1)
        self.mock_mesh = jax.sharding.Mesh(devices.reshape(mesh_shape),
                                           axis_names)

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_kv_cache_manager.py:66:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tpu_inference/runner/tpu_jax_runner.py:249: in __init__
    self.data_parallel_attn_sharding = NamedSharding(
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:462: in check_pspec
    _check_mesh_resource_axis(mesh, spec)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

mesh = Mesh(axis_sizes=(1, 1), axis_names=('data', 'model'), axis_types=(Auto, Auto))
pspec = PartitionSpec(('data', 'attn_dp'),)

    def _check_mesh_resource_axis(mesh, pspec):
      for p in pspec:
        if p is PartitionSpec.UNCONSTRAINED or p is None:
          continue
        p = p if isinstance(p, tuple) else (p,)
        for r in p:
          if r not in mesh.axis_names:
>           raise ValueError(
                f"Resource axis: {r} of {pspec} "
                f"is not found in mesh: {tuple(mesh.shape.keys())}.")
E           ValueError: Resource axis: attn_dp of PartitionSpec(('data', 'attn_dp'),) is not found in mesh: ('data', 'model').

../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:517: ValueError
___ ERROR at setup of TestKVCacheManager.test_get_kv_cache_spec_with_eagle3 ____

self = <test_kv_cache_manager.TestKVCacheManager object at 0x7825fb0b84d0>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_rng_key = MagicMock()

        # create 1x1 mesh
        devices = np.asarray(jax.devices()[:1])
        axis_names = ('data', 'model')
        mesh_shape = (1, 1)
        self.mock_mesh = jax.sharding.Mesh(devices.reshape(mesh_shape),
                                           axis_names)

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_kv_cache_manager.py:66:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tpu_inference/runner/tpu_jax_runner.py:249: in __init__
    self.data_parallel_attn_sharding = NamedSharding(
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:462: in check_pspec
    _check_mesh_resource_axis(mesh, spec)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

mesh = Mesh(axis_sizes=(1, 1), axis_names=('data', 'model'), axis_types=(Auto, Auto))
pspec = PartitionSpec(('data', 'attn_dp'),)

    def _check_mesh_resource_axis(mesh, pspec):
      for p in pspec:
        if p is PartitionSpec.UNCONSTRAINED or p is None:
          continue
        p = p if isinstance(p, tuple) else (p,)
        for r in p:
          if r not in mesh.axis_names:
>           raise ValueError(
                f"Resource axis: {r} of {pspec} "
                f"is not found in mesh: {tuple(mesh.shape.keys())}.")
E           ValueError: Resource axis: attn_dp of PartitionSpec(('data', 'attn_dp'),) is not found in mesh: ('data', 'model').

../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:517: ValueError
_ ERROR at setup of TestKVCacheManager.test_get_kv_cache_spec_with_eagle3_mla __

self = <test_kv_cache_manager.TestKVCacheManager object at 0x7825fb0b8680>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_rng_key = MagicMock()

        # create 1x1 mesh
        devices = np.asarray(jax.devices()[:1])
        axis_names = ('data', 'model')
        mesh_shape = (1, 1)
        self.mock_mesh = jax.sharding.Mesh(devices.reshape(mesh_shape),
                                           axis_names)

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_kv_cache_manager.py:66:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tpu_inference/runner/tpu_jax_runner.py:249: in __init__
    self.data_parallel_attn_sharding = NamedSharding(
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:462: in check_pspec
    _check_mesh_resource_axis(mesh, spec)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

mesh = Mesh(axis_sizes=(1, 1), axis_names=('data', 'model'), axis_types=(Auto, Auto))
pspec = PartitionSpec(('data', 'attn_dp'),)

    def _check_mesh_resource_axis(mesh, pspec):
      for p in pspec:
        if p is PartitionSpec.UNCONSTRAINED or p is None:
          continue
        p = p if isinstance(p, tuple) else (p,)
        for r in p:
          if r not in mesh.axis_names:
>           raise ValueError(
                f"Resource axis: {r} of {pspec} "
                f"is not found in mesh: {tuple(mesh.shape.keys())}.")
E           ValueError: Resource axis: attn_dp of PartitionSpec(('data', 'attn_dp'),) is not found in mesh: ('data', 'model').

../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:517: ValueError
_ ERROR at setup of TestMultiModalManager.test_execute_mm_encoder_single_image _

self = <test_multimodal_manager.TestMultiModalManager object at 0x7825fb0b8b00>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_mesh = MagicMock()
        self.mock_rng_key = MagicMock()

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            speculative_config = SpeculativeConfig(
                model='ngram',
                num_speculative_tokens=5,
                prompt_lookup_max=4,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                speculative_config=speculative_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_multimodal_manager.py:62:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tpu_inference.runner.tpu_jax_runner.TPUModelRunner object at 0x7825b0922d80>
vllm_config = VllmConfig(model_config=ModelConfig(model='Qwen/Qwen3-0.6B', runner='auto', convert='auto', task=None, tokenizer='Qwen...e_dir': None}, kv_transfer_config=None, kv_events_config=None, additional_config={}, instance_id='1762324583352786457')
devices = [<MagicMock id='132103272212928'>, <MagicMock id='132103272208848'>, <MagicMock id='132103272216432'>, <MagicMock id='132103272243776'>]

    def __init__(
        self,
        vllm_config: VllmConfig,
        devices: List[Any],
    ):
        self.vllm_config = vllm_config
        self.model_config = vllm_config.model_config
        # TODO(jevinjiang): override block size based on RPA v3.
        self.cache_config = vllm_config.cache_config
        self.lora_config = vllm_config.lora_config
        self.load_config = vllm_config.load_config
        self.parallel_config = vllm_config.parallel_config
        self.scheduler_config = vllm_config.scheduler_config
        self.speculative_config = vllm_config.speculative_config
        self.observability_config = vllm_config.observability_config
        self.device_config = vllm_config.device_config

        self.devices = devices
        self.dtype = self.model_config.dtype
        self.maybe_forbid_compile = runner_utils.ForbidCompile(
        ) if envs.VLLM_XLA_CHECK_RECOMPILATION else nullcontext()
        self.dp_size = self.vllm_config.sharding_config.total_dp_size

        self._init_random()
        self._init_mesh()
        self._init_phased_profiling()
        self._init_mm()
        self._init_inputs()
        self._init_speculative_decoding()

        # Delegate functions to specific manager classes.
        self.compilation_manager = CompilationManager(self)
        self.speculative_decoding_manager = SpeculativeDecodingManager(self)
        self.structured_decoding_manager = StructuredDecodingManager(self)
        self.kv_cache_manager = KVCacheManager(self)
        self.mm_manager = MultiModalManager(self)
        self.persistent_batch_manager = PersistentBatchManager(
            self.requests, self.input_batch, self.encoder_cache,
            self.uses_mrope, self.model_config)
        self.lora_utils = LoraUtils(self)

        cache_config = self.cache_config
        if cache_config.cache_dtype == "auto":
            model_dtype = self.dtype
            if isinstance(model_dtype, str):
                self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[model_dtype]
            elif isinstance(getattr(model_dtype, 'dtype', None), jnp.dtype):
                self.kv_cache_dtype = j2t_dtype(model_dtype.dtype)
            elif isinstance(model_dtype, torch.dtype):
                self.kv_cache_dtype = model_dtype
            else:
                raise ValueError(
                    "KV cache is unsupported for model_dtype of %s",
                    model_dtype)
        else:
            self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[
                cache_config.cache_dtype]

        self._pre_async_results: AsyncPreResults | None = None
        self._substitute_placeholder_token_fn = _substitute_placeholder_token
        self.execute_model_state: ExecuteModelState | None = None

>       self.data_parallel_mlp_sharding = NamedSharding(
            self.mesh, PartitionSpec(ShardingAxisName.MLP_DATA))
E       RuntimeError: std::bad_cast

tpu_inference/runner/tpu_jax_runner.py:247: RuntimeError
_ ERROR at setup of TestMultiModalManager.test_execute_mm_encoder_multiple_images _

self = <test_multimodal_manager.TestMultiModalManager object at 0x7825fb0b8cb0>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_mesh = MagicMock()
        self.mock_rng_key = MagicMock()

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            speculative_config = SpeculativeConfig(
                model='ngram',
                num_speculative_tokens=5,
                prompt_lookup_max=4,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                speculative_config=speculative_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_multimodal_manager.py:62:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tpu_inference.runner.tpu_jax_runner.TPUModelRunner object at 0x7825b097bbc0>
vllm_config = VllmConfig(model_config=ModelConfig(model='Qwen/Qwen3-0.6B', runner='auto', convert='auto', task=None, tokenizer='Qwen...e_dir': None}, kv_transfer_config=None, kv_events_config=None, additional_config={}, instance_id='1762324583606387508')
devices = [<MagicMock id='132103271528112'>, <MagicMock id='132103279814048'>, <MagicMock id='132103284174784'>, <MagicMock id='132103284174832'>]

    def __init__(
        self,
        vllm_config: VllmConfig,
        devices: List[Any],
    ):
        self.vllm_config = vllm_config
        self.model_config = vllm_config.model_config
        # TODO(jevinjiang): override block size based on RPA v3.
        self.cache_config = vllm_config.cache_config
        self.lora_config = vllm_config.lora_config
        self.load_config = vllm_config.load_config
        self.parallel_config = vllm_config.parallel_config
        self.scheduler_config = vllm_config.scheduler_config
        self.speculative_config = vllm_config.speculative_config
        self.observability_config = vllm_config.observability_config
        self.device_config = vllm_config.device_config

        self.devices = devices
        self.dtype = self.model_config.dtype
        self.maybe_forbid_compile = runner_utils.ForbidCompile(
        ) if envs.VLLM_XLA_CHECK_RECOMPILATION else nullcontext()
        self.dp_size = self.vllm_config.sharding_config.total_dp_size

        self._init_random()
        self._init_mesh()
        self._init_phased_profiling()
        self._init_mm()
        self._init_inputs()
        self._init_speculative_decoding()

        # Delegate functions to specific manager classes.
        self.compilation_manager = CompilationManager(self)
        self.speculative_decoding_manager = SpeculativeDecodingManager(self)
        self.structured_decoding_manager = StructuredDecodingManager(self)
        self.kv_cache_manager = KVCacheManager(self)
        self.mm_manager = MultiModalManager(self)
        self.persistent_batch_manager = PersistentBatchManager(
            self.requests, self.input_batch, self.encoder_cache,
            self.uses_mrope, self.model_config)
        self.lora_utils = LoraUtils(self)

        cache_config = self.cache_config
        if cache_config.cache_dtype == "auto":
            model_dtype = self.dtype
            if isinstance(model_dtype, str):
                self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[model_dtype]
            elif isinstance(getattr(model_dtype, 'dtype', None), jnp.dtype):
                self.kv_cache_dtype = j2t_dtype(model_dtype.dtype)
            elif isinstance(model_dtype, torch.dtype):
                self.kv_cache_dtype = model_dtype
            else:
                raise ValueError(
                    "KV cache is unsupported for model_dtype of %s",
                    model_dtype)
        else:
            self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[
                cache_config.cache_dtype]

        self._pre_async_results: AsyncPreResults | None = None
        self._substitute_placeholder_token_fn = _substitute_placeholder_token
        self.execute_model_state: ExecuteModelState | None = None

>       self.data_parallel_mlp_sharding = NamedSharding(
            self.mesh, PartitionSpec(ShardingAxisName.MLP_DATA))
E       RuntimeError: std::bad_cast

tpu_inference/runner/tpu_jax_runner.py:247: RuntimeError
_ ERROR at setup of TestMultiModalManager.test_gather_mm_embeddings_chunked_prefill _

self = <test_multimodal_manager.TestMultiModalManager object at 0x7825fb0b8e60>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_mesh = MagicMock()
        self.mock_rng_key = MagicMock()

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            speculative_config = SpeculativeConfig(
                model='ngram',
                num_speculative_tokens=5,
                prompt_lookup_max=4,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                speculative_config=speculative_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_multimodal_manager.py:62:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tpu_inference.runner.tpu_jax_runner.TPUModelRunner object at 0x7825b0979550>
vllm_config = VllmConfig(model_config=ModelConfig(model='Qwen/Qwen3-0.6B', runner='auto', convert='auto', task=None, tokenizer='Qwen...e_dir': None}, kv_transfer_config=None, kv_events_config=None, additional_config={}, instance_id='1762324583823559511')
devices = [<MagicMock id='132103271456336'>, <MagicMock id='132103279972896'>, <MagicMock id='132103267359472'>, <MagicMock id='132103279978416'>]

    def __init__(
        self,
        vllm_config: VllmConfig,
        devices: List[Any],
    ):
        self.vllm_config = vllm_config
        self.model_config = vllm_config.model_config
        # TODO(jevinjiang): override block size based on RPA v3.
        self.cache_config = vllm_config.cache_config
        self.lora_config = vllm_config.lora_config
        self.load_config = vllm_config.load_config
        self.parallel_config = vllm_config.parallel_config
        self.scheduler_config = vllm_config.scheduler_config
        self.speculative_config = vllm_config.speculative_config
        self.observability_config = vllm_config.observability_config
        self.device_config = vllm_config.device_config

        self.devices = devices
        self.dtype = self.model_config.dtype
        self.maybe_forbid_compile = runner_utils.ForbidCompile(
        ) if envs.VLLM_XLA_CHECK_RECOMPILATION else nullcontext()
        self.dp_size = self.vllm_config.sharding_config.total_dp_size

        self._init_random()
        self._init_mesh()
        self._init_phased_profiling()
        self._init_mm()
        self._init_inputs()
        self._init_speculative_decoding()

        # Delegate functions to specific manager classes.
        self.compilation_manager = CompilationManager(self)
        self.speculative_decoding_manager = SpeculativeDecodingManager(self)
        self.structured_decoding_manager = StructuredDecodingManager(self)
        self.kv_cache_manager = KVCacheManager(self)
        self.mm_manager = MultiModalManager(self)
        self.persistent_batch_manager = PersistentBatchManager(
            self.requests, self.input_batch, self.encoder_cache,
            self.uses_mrope, self.model_config)
        self.lora_utils = LoraUtils(self)

        cache_config = self.cache_config
        if cache_config.cache_dtype == "auto":
            model_dtype = self.dtype
            if isinstance(model_dtype, str):
                self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[model_dtype]
            elif isinstance(getattr(model_dtype, 'dtype', None), jnp.dtype):
                self.kv_cache_dtype = j2t_dtype(model_dtype.dtype)
            elif isinstance(model_dtype, torch.dtype):
                self.kv_cache_dtype = model_dtype
            else:
                raise ValueError(
                    "KV cache is unsupported for model_dtype of %s",
                    model_dtype)
        else:
            self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[
                cache_config.cache_dtype]

        self._pre_async_results: AsyncPreResults | None = None
        self._substitute_placeholder_token_fn = _substitute_placeholder_token
        self.execute_model_state: ExecuteModelState | None = None

>       self.data_parallel_mlp_sharding = NamedSharding(
            self.mesh, PartitionSpec(ShardingAxisName.MLP_DATA))
E       RuntimeError: std::bad_cast

tpu_inference/runner/tpu_jax_runner.py:247: RuntimeError
______ ERROR at setup of TestMultiModalManager.test_calc_mrope_positions _______

self = <test_multimodal_manager.TestMultiModalManager object at 0x7825fb0b9010>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_mesh = MagicMock()
        self.mock_rng_key = MagicMock()

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            speculative_config = SpeculativeConfig(
                model='ngram',
                num_speculative_tokens=5,
                prompt_lookup_max=4,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                speculative_config=speculative_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_multimodal_manager.py:62:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tpu_inference.runner.tpu_jax_runner.TPUModelRunner object at 0x7825b0931760>
vllm_config = VllmConfig(model_config=ModelConfig(model='Qwen/Qwen3-0.6B', runner='auto', convert='auto', task=None, tokenizer='Qwen...e_dir': None}, kv_transfer_config=None, kv_events_config=None, additional_config={}, instance_id='1762324584035613674')
devices = [<MagicMock id='132103259468432'>, <MagicMock id='132103279860080'>, <MagicMock id='132103342806160'>, <MagicMock id='132103342759024'>]

    def __init__(
        self,
        vllm_config: VllmConfig,
        devices: List[Any],
    ):
        self.vllm_config = vllm_config
        self.model_config = vllm_config.model_config
        # TODO(jevinjiang): override block size based on RPA v3.
        self.cache_config = vllm_config.cache_config
        self.lora_config = vllm_config.lora_config
        self.load_config = vllm_config.load_config
        self.parallel_config = vllm_config.parallel_config
        self.scheduler_config = vllm_config.scheduler_config
        self.speculative_config = vllm_config.speculative_config
        self.observability_config = vllm_config.observability_config
        self.device_config = vllm_config.device_config

        self.devices = devices
        self.dtype = self.model_config.dtype
        self.maybe_forbid_compile = runner_utils.ForbidCompile(
        ) if envs.VLLM_XLA_CHECK_RECOMPILATION else nullcontext()
        self.dp_size = self.vllm_config.sharding_config.total_dp_size

        self._init_random()
        self._init_mesh()
        self._init_phased_profiling()
        self._init_mm()
        self._init_inputs()
        self._init_speculative_decoding()

        # Delegate functions to specific manager classes.
        self.compilation_manager = CompilationManager(self)
        self.speculative_decoding_manager = SpeculativeDecodingManager(self)
        self.structured_decoding_manager = StructuredDecodingManager(self)
        self.kv_cache_manager = KVCacheManager(self)
        self.mm_manager = MultiModalManager(self)
        self.persistent_batch_manager = PersistentBatchManager(
            self.requests, self.input_batch, self.encoder_cache,
            self.uses_mrope, self.model_config)
        self.lora_utils = LoraUtils(self)

        cache_config = self.cache_config
        if cache_config.cache_dtype == "auto":
            model_dtype = self.dtype
            if isinstance(model_dtype, str):
                self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[model_dtype]
            elif isinstance(getattr(model_dtype, 'dtype', None), jnp.dtype):
                self.kv_cache_dtype = j2t_dtype(model_dtype.dtype)
            elif isinstance(model_dtype, torch.dtype):
                self.kv_cache_dtype = model_dtype
            else:
                raise ValueError(
                    "KV cache is unsupported for model_dtype of %s",
                    model_dtype)
        else:
            self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[
                cache_config.cache_dtype]

        self._pre_async_results: AsyncPreResults | None = None
        self._substitute_placeholder_token_fn = _substitute_placeholder_token
        self.execute_model_state: ExecuteModelState | None = None

>       self.data_parallel_mlp_sharding = NamedSharding(
            self.mesh, PartitionSpec(ShardingAxisName.MLP_DATA))
E       RuntimeError: std::bad_cast

tpu_inference/runner/tpu_jax_runner.py:247: RuntimeError
_ ERROR at setup of TestSpeculativeDecodingManager.test_propose_draft_token_ids_dispatches_to_eagle _

self = <test_speculative_decoding_manager.TestSpeculativeDecodingManager object at 0x7825fb0b8ce0>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_mesh = MagicMock()
        self.mock_rng_key = MagicMock()

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            speculative_config = SpeculativeConfig(
                model='ngram',
                num_speculative_tokens=5,
                prompt_lookup_max=4,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                speculative_config=speculative_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_speculative_decoding_manager.py:61:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tpu_inference.runner.tpu_jax_runner.TPUModelRunner object at 0x7825b09a8350>
vllm_config = VllmConfig(model_config=ModelConfig(model='Qwen/Qwen3-0.6B', runner='auto', convert='auto', task=None, tokenizer='Qwen...e_dir': None}, kv_transfer_config=None, kv_events_config=None, additional_config={}, instance_id='1762324584252425346')
devices = [<MagicMock id='132103271900960'>, <MagicMock id='132103267276160'>, <MagicMock id='132103347695792'>, <MagicMock id='132103271633680'>]

    def __init__(
        self,
        vllm_config: VllmConfig,
        devices: List[Any],
    ):
        self.vllm_config = vllm_config
        self.model_config = vllm_config.model_config
        # TODO(jevinjiang): override block size based on RPA v3.
        self.cache_config = vllm_config.cache_config
        self.lora_config = vllm_config.lora_config
        self.load_config = vllm_config.load_config
        self.parallel_config = vllm_config.parallel_config
        self.scheduler_config = vllm_config.scheduler_config
        self.speculative_config = vllm_config.speculative_config
        self.observability_config = vllm_config.observability_config
        self.device_config = vllm_config.device_config

        self.devices = devices
        self.dtype = self.model_config.dtype
        self.maybe_forbid_compile = runner_utils.ForbidCompile(
        ) if envs.VLLM_XLA_CHECK_RECOMPILATION else nullcontext()
        self.dp_size = self.vllm_config.sharding_config.total_dp_size

        self._init_random()
        self._init_mesh()
        self._init_phased_profiling()
        self._init_mm()
        self._init_inputs()
        self._init_speculative_decoding()

        # Delegate functions to specific manager classes.
        self.compilation_manager = CompilationManager(self)
        self.speculative_decoding_manager = SpeculativeDecodingManager(self)
        self.structured_decoding_manager = StructuredDecodingManager(self)
        self.kv_cache_manager = KVCacheManager(self)
        self.mm_manager = MultiModalManager(self)
        self.persistent_batch_manager = PersistentBatchManager(
            self.requests, self.input_batch, self.encoder_cache,
            self.uses_mrope, self.model_config)
        self.lora_utils = LoraUtils(self)

        cache_config = self.cache_config
        if cache_config.cache_dtype == "auto":
            model_dtype = self.dtype
            if isinstance(model_dtype, str):
                self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[model_dtype]
            elif isinstance(getattr(model_dtype, 'dtype', None), jnp.dtype):
                self.kv_cache_dtype = j2t_dtype(model_dtype.dtype)
            elif isinstance(model_dtype, torch.dtype):
                self.kv_cache_dtype = model_dtype
            else:
                raise ValueError(
                    "KV cache is unsupported for model_dtype of %s",
                    model_dtype)
        else:
            self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[
                cache_config.cache_dtype]

        self._pre_async_results: AsyncPreResults | None = None
        self._substitute_placeholder_token_fn = _substitute_placeholder_token
        self.execute_model_state: ExecuteModelState | None = None

>       self.data_parallel_mlp_sharding = NamedSharding(
            self.mesh, PartitionSpec(ShardingAxisName.MLP_DATA))
E       RuntimeError: std::bad_cast

tpu_inference/runner/tpu_jax_runner.py:247: RuntimeError
_ ERROR at setup of TestSpeculativeDecodingManager.test_propose_draft_token_ids_wrong_drafter_type _

self = <test_speculative_decoding_manager.TestSpeculativeDecodingManager object at 0x7825fb0b8500>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_mesh = MagicMock()
        self.mock_rng_key = MagicMock()

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            speculative_config = SpeculativeConfig(
                model='ngram',
                num_speculative_tokens=5,
                prompt_lookup_max=4,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                speculative_config=speculative_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_speculative_decoding_manager.py:61:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tpu_inference.runner.tpu_jax_runner.TPUModelRunner object at 0x7825af9f9a90>
vllm_config = VllmConfig(model_config=ModelConfig(model='Qwen/Qwen3-0.6B', runner='auto', convert='auto', task=None, tokenizer='Qwen...e_dir': None}, kv_transfer_config=None, kv_events_config=None, additional_config={}, instance_id='1762324584466710049')
devices = [<MagicMock id='132103284046064'>, <MagicMock id='132103342722848'>, <MagicMock id='132103259241072'>, <MagicMock id='132103342766272'>]

    def __init__(
        self,
        vllm_config: VllmConfig,
        devices: List[Any],
    ):
        self.vllm_config = vllm_config
        self.model_config = vllm_config.model_config
        # TODO(jevinjiang): override block size based on RPA v3.
        self.cache_config = vllm_config.cache_config
        self.lora_config = vllm_config.lora_config
        self.load_config = vllm_config.load_config
        self.parallel_config = vllm_config.parallel_config
        self.scheduler_config = vllm_config.scheduler_config
        self.speculative_config = vllm_config.speculative_config
        self.observability_config = vllm_config.observability_config
        self.device_config = vllm_config.device_config

        self.devices = devices
        self.dtype = self.model_config.dtype
        self.maybe_forbid_compile = runner_utils.ForbidCompile(
        ) if envs.VLLM_XLA_CHECK_RECOMPILATION else nullcontext()
        self.dp_size = self.vllm_config.sharding_config.total_dp_size

        self._init_random()
        self._init_mesh()
        self._init_phased_profiling()
        self._init_mm()
        self._init_inputs()
        self._init_speculative_decoding()

        # Delegate functions to specific manager classes.
        self.compilation_manager = CompilationManager(self)
        self.speculative_decoding_manager = SpeculativeDecodingManager(self)
        self.structured_decoding_manager = StructuredDecodingManager(self)
        self.kv_cache_manager = KVCacheManager(self)
        self.mm_manager = MultiModalManager(self)
        self.persistent_batch_manager = PersistentBatchManager(
            self.requests, self.input_batch, self.encoder_cache,
            self.uses_mrope, self.model_config)
        self.lora_utils = LoraUtils(self)

        cache_config = self.cache_config
        if cache_config.cache_dtype == "auto":
            model_dtype = self.dtype
            if isinstance(model_dtype, str):
                self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[model_dtype]
            elif isinstance(getattr(model_dtype, 'dtype', None), jnp.dtype):
                self.kv_cache_dtype = j2t_dtype(model_dtype.dtype)
            elif isinstance(model_dtype, torch.dtype):
                self.kv_cache_dtype = model_dtype
            else:
                raise ValueError(
                    "KV cache is unsupported for model_dtype of %s",
                    model_dtype)
        else:
            self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[
                cache_config.cache_dtype]

        self._pre_async_results: AsyncPreResults | None = None
        self._substitute_placeholder_token_fn = _substitute_placeholder_token
        self.execute_model_state: ExecuteModelState | None = None

>       self.data_parallel_mlp_sharding = NamedSharding(
            self.mesh, PartitionSpec(ShardingAxisName.MLP_DATA))
E       RuntimeError: std::bad_cast

tpu_inference/runner/tpu_jax_runner.py:247: RuntimeError
__ ERROR at setup of TestSpeculativeDecodingManager.test_take_draft_token_ids __

self = <test_speculative_decoding_manager.TestSpeculativeDecodingManager object at 0x7825fb0b80e0>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_mesh = MagicMock()
        self.mock_rng_key = MagicMock()

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            speculative_config = SpeculativeConfig(
                model='ngram',
                num_speculative_tokens=5,
                prompt_lookup_max=4,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                speculative_config=speculative_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_speculative_decoding_manager.py:61:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tpu_inference.runner.tpu_jax_runner.TPUModelRunner object at 0x7825af98c830>
vllm_config = VllmConfig(model_config=ModelConfig(model='Qwen/Qwen3-0.6B', runner='auto', convert='auto', task=None, tokenizer='Qwen...e_dir': None}, kv_transfer_config=None, kv_events_config=None, additional_config={}, instance_id='1762324584676988492')
devices = [<MagicMock id='132103255402272'>, <MagicMock id='132103255406256'>, <MagicMock id='132103259238480'>, <MagicMock id='132103263040240'>]

    def __init__(
        self,
        vllm_config: VllmConfig,
        devices: List[Any],
    ):
        self.vllm_config = vllm_config
        self.model_config = vllm_config.model_config
        # TODO(jevinjiang): override block size based on RPA v3.
        self.cache_config = vllm_config.cache_config
        self.lora_config = vllm_config.lora_config
        self.load_config = vllm_config.load_config
        self.parallel_config = vllm_config.parallel_config
        self.scheduler_config = vllm_config.scheduler_config
        self.speculative_config = vllm_config.speculative_config
        self.observability_config = vllm_config.observability_config
        self.device_config = vllm_config.device_config

        self.devices = devices
        self.dtype = self.model_config.dtype
        self.maybe_forbid_compile = runner_utils.ForbidCompile(
        ) if envs.VLLM_XLA_CHECK_RECOMPILATION else nullcontext()
        self.dp_size = self.vllm_config.sharding_config.total_dp_size

        self._init_random()
        self._init_mesh()
        self._init_phased_profiling()
        self._init_mm()
        self._init_inputs()
        self._init_speculative_decoding()

        # Delegate functions to specific manager classes.
        self.compilation_manager = CompilationManager(self)
        self.speculative_decoding_manager = SpeculativeDecodingManager(self)
        self.structured_decoding_manager = StructuredDecodingManager(self)
        self.kv_cache_manager = KVCacheManager(self)
        self.mm_manager = MultiModalManager(self)
        self.persistent_batch_manager = PersistentBatchManager(
            self.requests, self.input_batch, self.encoder_cache,
            self.uses_mrope, self.model_config)
        self.lora_utils = LoraUtils(self)

        cache_config = self.cache_config
        if cache_config.cache_dtype == "auto":
            model_dtype = self.dtype
            if isinstance(model_dtype, str):
                self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[model_dtype]
            elif isinstance(getattr(model_dtype, 'dtype', None), jnp.dtype):
                self.kv_cache_dtype = j2t_dtype(model_dtype.dtype)
            elif isinstance(model_dtype, torch.dtype):
                self.kv_cache_dtype = model_dtype
            else:
                raise ValueError(
                    "KV cache is unsupported for model_dtype of %s",
                    model_dtype)
        else:
            self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[
                cache_config.cache_dtype]

        self._pre_async_results: AsyncPreResults | None = None
        self._substitute_placeholder_token_fn = _substitute_placeholder_token
        self.execute_model_state: ExecuteModelState | None = None

>       self.data_parallel_mlp_sharding = NamedSharding(
            self.mesh, PartitionSpec(ShardingAxisName.MLP_DATA))
E       RuntimeError: std::bad_cast

tpu_inference/runner/tpu_jax_runner.py:247: RuntimeError
_ ERROR at setup of TestSpeculativeDecodingManager.test_get_spec_decode_metadata_parametrized[num_draft_tokens0-cu_num_scheduled_tokens0-8-expected_logits_indices0-expected_bonus_logits_indices0-expected_target_logits_indices0-expected_draft_token_ids0] _

self = <test_speculative_decoding_manager.TestSpeculativeDecodingManager object at 0x7825fb0b9670>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_mesh = MagicMock()
        self.mock_rng_key = MagicMock()

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            speculative_config = SpeculativeConfig(
                model='ngram',
                num_speculative_tokens=5,
                prompt_lookup_max=4,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                speculative_config=speculative_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_speculative_decoding_manager.py:61:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tpu_inference.runner.tpu_jax_runner.TPUModelRunner object at 0x7825af9a0f20>
vllm_config = VllmConfig(model_config=ModelConfig(model='Qwen/Qwen3-0.6B', runner='auto', convert='auto', task=None, tokenizer='Qwen...e_dir': None}, kv_transfer_config=None, kv_events_config=None, additional_config={}, instance_id='1762324584882688805')
devices = [<MagicMock id='132103255188464'>, <MagicMock id='132103255188032'>, <MagicMock id='132103255181504'>, <MagicMock id='132103255184000'>]

    def __init__(
        self,
        vllm_config: VllmConfig,
        devices: List[Any],
    ):
        self.vllm_config = vllm_config
        self.model_config = vllm_config.model_config
        # TODO(jevinjiang): override block size based on RPA v3.
        self.cache_config = vllm_config.cache_config
        self.lora_config = vllm_config.lora_config
        self.load_config = vllm_config.load_config
        self.parallel_config = vllm_config.parallel_config
        self.scheduler_config = vllm_config.scheduler_config
        self.speculative_config = vllm_config.speculative_config
        self.observability_config = vllm_config.observability_config
        self.device_config = vllm_config.device_config

        self.devices = devices
        self.dtype = self.model_config.dtype
        self.maybe_forbid_compile = runner_utils.ForbidCompile(
        ) if envs.VLLM_XLA_CHECK_RECOMPILATION else nullcontext()
        self.dp_size = self.vllm_config.sharding_config.total_dp_size

        self._init_random()
        self._init_mesh()
        self._init_phased_profiling()
        self._init_mm()
        self._init_inputs()
        self._init_speculative_decoding()

        # Delegate functions to specific manager classes.
        self.compilation_manager = CompilationManager(self)
        self.speculative_decoding_manager = SpeculativeDecodingManager(self)
        self.structured_decoding_manager = StructuredDecodingManager(self)
        self.kv_cache_manager = KVCacheManager(self)
        self.mm_manager = MultiModalManager(self)
        self.persistent_batch_manager = PersistentBatchManager(
            self.requests, self.input_batch, self.encoder_cache,
            self.uses_mrope, self.model_config)
        self.lora_utils = LoraUtils(self)

        cache_config = self.cache_config
        if cache_config.cache_dtype == "auto":
            model_dtype = self.dtype
            if isinstance(model_dtype, str):
                self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[model_dtype]
            elif isinstance(getattr(model_dtype, 'dtype', None), jnp.dtype):
                self.kv_cache_dtype = j2t_dtype(model_dtype.dtype)
            elif isinstance(model_dtype, torch.dtype):
                self.kv_cache_dtype = model_dtype
            else:
                raise ValueError(
                    "KV cache is unsupported for model_dtype of %s",
                    model_dtype)
        else:
            self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[
                cache_config.cache_dtype]

        self._pre_async_results: AsyncPreResults | None = None
        self._substitute_placeholder_token_fn = _substitute_placeholder_token
        self.execute_model_state: ExecuteModelState | None = None

>       self.data_parallel_mlp_sharding = NamedSharding(
            self.mesh, PartitionSpec(ShardingAxisName.MLP_DATA))
E       RuntimeError: std::bad_cast

tpu_inference/runner/tpu_jax_runner.py:247: RuntimeError
_ ERROR at setup of TestSpeculativeDecodingManager.test_get_spec_decode_metadata_parametrized[num_draft_tokens1-cu_num_scheduled_tokens1-8-expected_logits_indices1-expected_bonus_logits_indices1-expected_target_logits_indices1-expected_draft_token_ids1] _

self = <test_speculative_decoding_manager.TestSpeculativeDecodingManager object at 0x7825fb0b9a00>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_mesh = MagicMock()
        self.mock_rng_key = MagicMock()

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            speculative_config = SpeculativeConfig(
                model='ngram',
                num_speculative_tokens=5,
                prompt_lookup_max=4,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                speculative_config=speculative_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_speculative_decoding_manager.py:61:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tpu_inference.runner.tpu_jax_runner.TPUModelRunner object at 0x7825af9536e0>
vllm_config = VllmConfig(model_config=ModelConfig(model='Qwen/Qwen3-0.6B', runner='auto', convert='auto', task=None, tokenizer='Qwen...e_dir': None}, kv_transfer_config=None, kv_events_config=None, additional_config={}, instance_id='1762324585119574588')
devices = [<MagicMock id='132103255429856'>, <MagicMock id='132103267271216'>, <MagicMock id='132103279751536'>, <MagicMock id='132103255507168'>]

    def __init__(
        self,
        vllm_config: VllmConfig,
        devices: List[Any],
    ):
        self.vllm_config = vllm_config
        self.model_config = vllm_config.model_config
        # TODO(jevinjiang): override block size based on RPA v3.
        self.cache_config = vllm_config.cache_config
        self.lora_config = vllm_config.lora_config
        self.load_config = vllm_config.load_config
        self.parallel_config = vllm_config.parallel_config
        self.scheduler_config = vllm_config.scheduler_config
        self.speculative_config = vllm_config.speculative_config
        self.observability_config = vllm_config.observability_config
        self.device_config = vllm_config.device_config

        self.devices = devices
        self.dtype = self.model_config.dtype
        self.maybe_forbid_compile = runner_utils.ForbidCompile(
        ) if envs.VLLM_XLA_CHECK_RECOMPILATION else nullcontext()
        self.dp_size = self.vllm_config.sharding_config.total_dp_size

        self._init_random()
        self._init_mesh()
        self._init_phased_profiling()
        self._init_mm()
        self._init_inputs()
        self._init_speculative_decoding()

        # Delegate functions to specific manager classes.
        self.compilation_manager = CompilationManager(self)
        self.speculative_decoding_manager = SpeculativeDecodingManager(self)
        self.structured_decoding_manager = StructuredDecodingManager(self)
        self.kv_cache_manager = KVCacheManager(self)
        self.mm_manager = MultiModalManager(self)
        self.persistent_batch_manager = PersistentBatchManager(
            self.requests, self.input_batch, self.encoder_cache,
            self.uses_mrope, self.model_config)
        self.lora_utils = LoraUtils(self)

        cache_config = self.cache_config
        if cache_config.cache_dtype == "auto":
            model_dtype = self.dtype
            if isinstance(model_dtype, str):
                self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[model_dtype]
            elif isinstance(getattr(model_dtype, 'dtype', None), jnp.dtype):
                self.kv_cache_dtype = j2t_dtype(model_dtype.dtype)
            elif isinstance(model_dtype, torch.dtype):
                self.kv_cache_dtype = model_dtype
            else:
                raise ValueError(
                    "KV cache is unsupported for model_dtype of %s",
                    model_dtype)
        else:
            self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[
                cache_config.cache_dtype]

        self._pre_async_results: AsyncPreResults | None = None
        self._substitute_placeholder_token_fn = _substitute_placeholder_token
        self.execute_model_state: ExecuteModelState | None = None

>       self.data_parallel_mlp_sharding = NamedSharding(
            self.mesh, PartitionSpec(ShardingAxisName.MLP_DATA))
E       RuntimeError: std::bad_cast

tpu_inference/runner/tpu_jax_runner.py:247: RuntimeError
_ ERROR at setup of TestSpeculativeDecodingManager.test_propose_eagle3_draft_token_ids[True] _

self = <test_speculative_decoding_manager.TestSpeculativeDecodingManager object at 0x7825fb0b9be0>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_mesh = MagicMock()
        self.mock_rng_key = MagicMock()

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            speculative_config = SpeculativeConfig(
                model='ngram',
                num_speculative_tokens=5,
                prompt_lookup_max=4,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                speculative_config=speculative_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_speculative_decoding_manager.py:61:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tpu_inference.runner.tpu_jax_runner.TPUModelRunner object at 0x7825af946330>
vllm_config = VllmConfig(model_config=ModelConfig(model='Qwen/Qwen3-0.6B', runner='auto', convert='auto', task=None, tokenizer='Qwen...e_dir': None}, kv_transfer_config=None, kv_events_config=None, additional_config={}, instance_id='1762324585351981840')
devices = [<MagicMock id='132103255580816'>, <MagicMock id='132103255576784'>, <MagicMock id='132103255580864'>, <MagicMock id='132103258885584'>]

    def __init__(
        self,
        vllm_config: VllmConfig,
        devices: List[Any],
    ):
        self.vllm_config = vllm_config
        self.model_config = vllm_config.model_config
        # TODO(jevinjiang): override block size based on RPA v3.
        self.cache_config = vllm_config.cache_config
        self.lora_config = vllm_config.lora_config
        self.load_config = vllm_config.load_config
        self.parallel_config = vllm_config.parallel_config
        self.scheduler_config = vllm_config.scheduler_config
        self.speculative_config = vllm_config.speculative_config
        self.observability_config = vllm_config.observability_config
        self.device_config = vllm_config.device_config

        self.devices = devices
        self.dtype = self.model_config.dtype
        self.maybe_forbid_compile = runner_utils.ForbidCompile(
        ) if envs.VLLM_XLA_CHECK_RECOMPILATION else nullcontext()
        self.dp_size = self.vllm_config.sharding_config.total_dp_size

        self._init_random()
        self._init_mesh()
        self._init_phased_profiling()
        self._init_mm()
        self._init_inputs()
        self._init_speculative_decoding()

        # Delegate functions to specific manager classes.
        self.compilation_manager = CompilationManager(self)
        self.speculative_decoding_manager = SpeculativeDecodingManager(self)
        self.structured_decoding_manager = StructuredDecodingManager(self)
        self.kv_cache_manager = KVCacheManager(self)
        self.mm_manager = MultiModalManager(self)
        self.persistent_batch_manager = PersistentBatchManager(
            self.requests, self.input_batch, self.encoder_cache,
            self.uses_mrope, self.model_config)
        self.lora_utils = LoraUtils(self)

        cache_config = self.cache_config
        if cache_config.cache_dtype == "auto":
            model_dtype = self.dtype
            if isinstance(model_dtype, str):
                self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[model_dtype]
            elif isinstance(getattr(model_dtype, 'dtype', None), jnp.dtype):
                self.kv_cache_dtype = j2t_dtype(model_dtype.dtype)
            elif isinstance(model_dtype, torch.dtype):
                self.kv_cache_dtype = model_dtype
            else:
                raise ValueError(
                    "KV cache is unsupported for model_dtype of %s",
                    model_dtype)
        else:
            self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[
                cache_config.cache_dtype]

        self._pre_async_results: AsyncPreResults | None = None
        self._substitute_placeholder_token_fn = _substitute_placeholder_token
        self.execute_model_state: ExecuteModelState | None = None

>       self.data_parallel_mlp_sharding = NamedSharding(
            self.mesh, PartitionSpec(ShardingAxisName.MLP_DATA))
E       RuntimeError: std::bad_cast

tpu_inference/runner/tpu_jax_runner.py:247: RuntimeError
_ ERROR at setup of TestSpeculativeDecodingManager.test_propose_eagle3_draft_token_ids[False] _

self = <test_speculative_decoding_manager.TestSpeculativeDecodingManager object at 0x7825fb0b9d00>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_mesh = MagicMock()
        self.mock_rng_key = MagicMock()

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            speculative_config = SpeculativeConfig(
                model='ngram',
                num_speculative_tokens=5,
                prompt_lookup_max=4,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                speculative_config=speculative_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_speculative_decoding_manager.py:61:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tpu_inference.runner.tpu_jax_runner.TPUModelRunner object at 0x7825af571b50>
vllm_config = VllmConfig(model_config=ModelConfig(model='Qwen/Qwen3-0.6B', runner='auto', convert='auto', task=None, tokenizer='Qwen...e_dir': None}, kv_transfer_config=None, kv_events_config=None, additional_config={}, instance_id='1762324585558479533')
devices = [<MagicMock id='132103255520608'>, <MagicMock id='132103255427648'>, <MagicMock id='132103343112944'>, <MagicMock id='132103255230656'>]

    def __init__(
        self,
        vllm_config: VllmConfig,
        devices: List[Any],
    ):
        self.vllm_config = vllm_config
        self.model_config = vllm_config.model_config
        # TODO(jevinjiang): override block size based on RPA v3.
        self.cache_config = vllm_config.cache_config
        self.lora_config = vllm_config.lora_config
        self.load_config = vllm_config.load_config
        self.parallel_config = vllm_config.parallel_config
        self.scheduler_config = vllm_config.scheduler_config
        self.speculative_config = vllm_config.speculative_config
        self.observability_config = vllm_config.observability_config
        self.device_config = vllm_config.device_config

        self.devices = devices
        self.dtype = self.model_config.dtype
        self.maybe_forbid_compile = runner_utils.ForbidCompile(
        ) if envs.VLLM_XLA_CHECK_RECOMPILATION else nullcontext()
        self.dp_size = self.vllm_config.sharding_config.total_dp_size

        self._init_random()
        self._init_mesh()
        self._init_phased_profiling()
        self._init_mm()
        self._init_inputs()
        self._init_speculative_decoding()

        # Delegate functions to specific manager classes.
        self.compilation_manager = CompilationManager(self)
        self.speculative_decoding_manager = SpeculativeDecodingManager(self)
        self.structured_decoding_manager = StructuredDecodingManager(self)
        self.kv_cache_manager = KVCacheManager(self)
        self.mm_manager = MultiModalManager(self)
        self.persistent_batch_manager = PersistentBatchManager(
            self.requests, self.input_batch, self.encoder_cache,
            self.uses_mrope, self.model_config)
        self.lora_utils = LoraUtils(self)

        cache_config = self.cache_config
        if cache_config.cache_dtype == "auto":
            model_dtype = self.dtype
            if isinstance(model_dtype, str):
                self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[model_dtype]
            elif isinstance(getattr(model_dtype, 'dtype', None), jnp.dtype):
                self.kv_cache_dtype = j2t_dtype(model_dtype.dtype)
            elif isinstance(model_dtype, torch.dtype):
                self.kv_cache_dtype = model_dtype
            else:
                raise ValueError(
                    "KV cache is unsupported for model_dtype of %s",
                    model_dtype)
        else:
            self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[
                cache_config.cache_dtype]

        self._pre_async_results: AsyncPreResults | None = None
        self._substitute_placeholder_token_fn = _substitute_placeholder_token
        self.execute_model_state: ExecuteModelState | None = None

>       self.data_parallel_mlp_sharding = NamedSharding(
            self.mesh, PartitionSpec(ShardingAxisName.MLP_DATA))
E       RuntimeError: std::bad_cast

tpu_inference/runner/tpu_jax_runner.py:247: RuntimeError
___ ERROR at setup of TestStructuredDecodingManager.test_structured_decoding ___

self = <test_structured_decoding_manager.TestStructuredDecodingManager object at 0x7825fb0ba420>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_mesh = MagicMock()
        self.mock_rng_key = MagicMock()

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            speculative_config = SpeculativeConfig(
                model='ngram',
                num_speculative_tokens=5,
                prompt_lookup_max=4,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                speculative_config=speculative_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_structured_decoding_manager.py:57:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tpu_inference.runner.tpu_jax_runner.TPUModelRunner object at 0x7825af5842f0>
vllm_config = VllmConfig(model_config=ModelConfig(model='Qwen/Qwen3-0.6B', runner='auto', convert='auto', task=None, tokenizer='Qwen...e_dir': None}, kv_transfer_config=None, kv_events_config=None, additional_config={}, instance_id='1762324585778912105')
devices = [<MagicMock id='132103250914256'>, <MagicMock id='132103250918240'>, <MagicMock id='132103351967792'>, <MagicMock id='132103250565344'>]

    def __init__(
        self,
        vllm_config: VllmConfig,
        devices: List[Any],
    ):
        self.vllm_config = vllm_config
        self.model_config = vllm_config.model_config
        # TODO(jevinjiang): override block size based on RPA v3.
        self.cache_config = vllm_config.cache_config
        self.lora_config = vllm_config.lora_config
        self.load_config = vllm_config.load_config
        self.parallel_config = vllm_config.parallel_config
        self.scheduler_config = vllm_config.scheduler_config
        self.speculative_config = vllm_config.speculative_config
        self.observability_config = vllm_config.observability_config
        self.device_config = vllm_config.device_config

        self.devices = devices
        self.dtype = self.model_config.dtype
        self.maybe_forbid_compile = runner_utils.ForbidCompile(
        ) if envs.VLLM_XLA_CHECK_RECOMPILATION else nullcontext()
        self.dp_size = self.vllm_config.sharding_config.total_dp_size

        self._init_random()
        self._init_mesh()
        self._init_phased_profiling()
        self._init_mm()
        self._init_inputs()
        self._init_speculative_decoding()

        # Delegate functions to specific manager classes.
        self.compilation_manager = CompilationManager(self)
        self.speculative_decoding_manager = SpeculativeDecodingManager(self)
        self.structured_decoding_manager = StructuredDecodingManager(self)
        self.kv_cache_manager = KVCacheManager(self)
        self.mm_manager = MultiModalManager(self)
        self.persistent_batch_manager = PersistentBatchManager(
            self.requests, self.input_batch, self.encoder_cache,
            self.uses_mrope, self.model_config)
        self.lora_utils = LoraUtils(self)

        cache_config = self.cache_config
        if cache_config.cache_dtype == "auto":
            model_dtype = self.dtype
            if isinstance(model_dtype, str):
                self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[model_dtype]
            elif isinstance(getattr(model_dtype, 'dtype', None), jnp.dtype):
                self.kv_cache_dtype = j2t_dtype(model_dtype.dtype)
            elif isinstance(model_dtype, torch.dtype):
                self.kv_cache_dtype = model_dtype
            else:
                raise ValueError(
                    "KV cache is unsupported for model_dtype of %s",
                    model_dtype)
        else:
            self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[
                cache_config.cache_dtype]

        self._pre_async_results: AsyncPreResults | None = None
        self._substitute_placeholder_token_fn = _substitute_placeholder_token
        self.execute_model_state: ExecuteModelState | None = None

>       self.data_parallel_mlp_sharding = NamedSharding(
            self.mesh, PartitionSpec(ShardingAxisName.MLP_DATA))
E       RuntimeError: std::bad_cast

tpu_inference/runner/tpu_jax_runner.py:247: RuntimeError
______ ERROR at setup of TestTPUJaxRunner.test_get_supported_tasks_runner ______

self = <test_tpu_jax_runner.TestTPUJaxRunner object at 0x7825fb0baab0>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_mesh = MagicMock()
        self.mock_rng_key = MagicMock()

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            speculative_config = SpeculativeConfig(
                model='ngram',
                num_speculative_tokens=5,
                prompt_lookup_max=4,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                speculative_config=speculative_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_tpu_jax_runner.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tpu_inference.runner.tpu_jax_runner.TPUModelRunner object at 0x7825af5c54c0>
vllm_config = VllmConfig(model_config=ModelConfig(model='Qwen/Qwen3-0.6B', runner='auto', convert='auto', task=None, tokenizer='Qwen...e_dir': None}, kv_transfer_config=None, kv_events_config=None, additional_config={}, instance_id='1762324585984656919')
devices = [<MagicMock id='132103250734176'>, <MagicMock id='132103250732784'>, <MagicMock id='132103250726880'>, <MagicMock id='132103250728704'>]

    def __init__(
        self,
        vllm_config: VllmConfig,
        devices: List[Any],
    ):
        self.vllm_config = vllm_config
        self.model_config = vllm_config.model_config
        # TODO(jevinjiang): override block size based on RPA v3.
        self.cache_config = vllm_config.cache_config
        self.lora_config = vllm_config.lora_config
        self.load_config = vllm_config.load_config
        self.parallel_config = vllm_config.parallel_config
        self.scheduler_config = vllm_config.scheduler_config
        self.speculative_config = vllm_config.speculative_config
        self.observability_config = vllm_config.observability_config
        self.device_config = vllm_config.device_config

        self.devices = devices
        self.dtype = self.model_config.dtype
        self.maybe_forbid_compile = runner_utils.ForbidCompile(
        ) if envs.VLLM_XLA_CHECK_RECOMPILATION else nullcontext()
        self.dp_size = self.vllm_config.sharding_config.total_dp_size

        self._init_random()
        self._init_mesh()
        self._init_phased_profiling()
        self._init_mm()
        self._init_inputs()
        self._init_speculative_decoding()

        # Delegate functions to specific manager classes.
        self.compilation_manager = CompilationManager(self)
        self.speculative_decoding_manager = SpeculativeDecodingManager(self)
        self.structured_decoding_manager = StructuredDecodingManager(self)
        self.kv_cache_manager = KVCacheManager(self)
        self.mm_manager = MultiModalManager(self)
        self.persistent_batch_manager = PersistentBatchManager(
            self.requests, self.input_batch, self.encoder_cache,
            self.uses_mrope, self.model_config)
        self.lora_utils = LoraUtils(self)

        cache_config = self.cache_config
        if cache_config.cache_dtype == "auto":
            model_dtype = self.dtype
            if isinstance(model_dtype, str):
                self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[model_dtype]
            elif isinstance(getattr(model_dtype, 'dtype', None), jnp.dtype):
                self.kv_cache_dtype = j2t_dtype(model_dtype.dtype)
            elif isinstance(model_dtype, torch.dtype):
                self.kv_cache_dtype = model_dtype
            else:
                raise ValueError(
                    "KV cache is unsupported for model_dtype of %s",
                    model_dtype)
        else:
            self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[
                cache_config.cache_dtype]

        self._pre_async_results: AsyncPreResults | None = None
        self._substitute_placeholder_token_fn = _substitute_placeholder_token
        self.execute_model_state: ExecuteModelState | None = None

>       self.data_parallel_mlp_sharding = NamedSharding(
            self.mesh, PartitionSpec(ShardingAxisName.MLP_DATA))
E       RuntimeError: std::bad_cast

tpu_inference/runner/tpu_jax_runner.py:247: RuntimeError
_________ ERROR at setup of TestTPUJaxRunner.test_get_input_ids_embeds _________

self = <test_tpu_jax_runner.TestTPUJaxRunner object at 0x7825fb0bac60>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_mesh = MagicMock()
        self.mock_rng_key = MagicMock()

        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=MagicMock()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            speculative_config = SpeculativeConfig(
                model='ngram',
                num_speculative_tokens=5,
                prompt_lookup_max=4,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                speculative_config=speculative_config,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_tpu_jax_runner.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tpu_inference.runner.tpu_jax_runner.TPUModelRunner object at 0x7825af517aa0>
vllm_config = VllmConfig(model_config=ModelConfig(model='Qwen/Qwen3-0.6B', runner='auto', convert='auto', task=None, tokenizer='Qwen...e_dir': None}, kv_transfer_config=None, kv_events_config=None, additional_config={}, instance_id='1762324586206865661')
devices = [<MagicMock id='132103250942752'>, <MagicMock id='132103254865440'>, <MagicMock id='132103250950576'>, <MagicMock id='132103250512112'>]

    def __init__(
        self,
        vllm_config: VllmConfig,
        devices: List[Any],
    ):
        self.vllm_config = vllm_config
        self.model_config = vllm_config.model_config
        # TODO(jevinjiang): override block size based on RPA v3.
        self.cache_config = vllm_config.cache_config
        self.lora_config = vllm_config.lora_config
        self.load_config = vllm_config.load_config
        self.parallel_config = vllm_config.parallel_config
        self.scheduler_config = vllm_config.scheduler_config
        self.speculative_config = vllm_config.speculative_config
        self.observability_config = vllm_config.observability_config
        self.device_config = vllm_config.device_config

        self.devices = devices
        self.dtype = self.model_config.dtype
        self.maybe_forbid_compile = runner_utils.ForbidCompile(
        ) if envs.VLLM_XLA_CHECK_RECOMPILATION else nullcontext()
        self.dp_size = self.vllm_config.sharding_config.total_dp_size

        self._init_random()
        self._init_mesh()
        self._init_phased_profiling()
        self._init_mm()
        self._init_inputs()
        self._init_speculative_decoding()

        # Delegate functions to specific manager classes.
        self.compilation_manager = CompilationManager(self)
        self.speculative_decoding_manager = SpeculativeDecodingManager(self)
        self.structured_decoding_manager = StructuredDecodingManager(self)
        self.kv_cache_manager = KVCacheManager(self)
        self.mm_manager = MultiModalManager(self)
        self.persistent_batch_manager = PersistentBatchManager(
            self.requests, self.input_batch, self.encoder_cache,
            self.uses_mrope, self.model_config)
        self.lora_utils = LoraUtils(self)

        cache_config = self.cache_config
        if cache_config.cache_dtype == "auto":
            model_dtype = self.dtype
            if isinstance(model_dtype, str):
                self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[model_dtype]
            elif isinstance(getattr(model_dtype, 'dtype', None), jnp.dtype):
                self.kv_cache_dtype = j2t_dtype(model_dtype.dtype)
            elif isinstance(model_dtype, torch.dtype):
                self.kv_cache_dtype = model_dtype
            else:
                raise ValueError(
                    "KV cache is unsupported for model_dtype of %s",
                    model_dtype)
        else:
            self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[
                cache_config.cache_dtype]

        self._pre_async_results: AsyncPreResults | None = None
        self._substitute_placeholder_token_fn = _substitute_placeholder_token
        self.execute_model_state: ExecuteModelState | None = None

>       self.data_parallel_mlp_sharding = NamedSharding(
            self.mesh, PartitionSpec(ShardingAxisName.MLP_DATA))
E       RuntimeError: std::bad_cast

tpu_inference/runner/tpu_jax_runner.py:247: RuntimeError
_ ERROR at setup of TestTPUJaxRunnerMultimodalModelLoadedForTextOnly.test_is_multimodal_model _

self = <test_tpu_jax_runner.TestTPUJaxRunnerMultimodalModelLoadedForTextOnly object at 0x7825fb0baf60>

    def setup_method(self):
        # Mock JAX dependencies
        self.mock_devices = [MagicMock(coords=i) for i in range(4)]
        self.mock_mesh = MagicMock()
        self.mock_rng_key = MagicMock()

        # Setup the runner with the model_config.is_multimodal_model set to True but get_model returning None for get_multimodal_embeddings_fn and get_input_embeddings_fn.
        with patch('jax.devices', return_value=self.mock_devices), \
             patch('jax.make_mesh', return_value=self.mock_mesh), \
             patch('jax.random.key', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.nnx.Rngs', return_value=self.mock_rng_key), \
             patch('tpu_inference.runner.tpu_jax_runner.get_model', return_value=self._model_get_model()):

            model_config = ModelConfig(tokenizer_mode="auto",
                                       trust_remote_code=False,
                                       seed=0,
                                       dtype='bfloat16')
            # Set multimodal_config to not None, such that the is_multimodal_model property of model_config is True.
            model_config.multimodal_config = MagicMock()

            cache_config = CacheConfig(
                block_size=16,
                gpu_memory_utilization=0.9,
                swap_space=4,
                cache_dtype="auto",
            )
            scheduler_config = SchedulerConfig(max_num_seqs=16, )
            parallel_config = ParallelConfig(
                pipeline_parallel_size=1,
                tensor_parallel_size=1,
                worker_use_ray=False,
            )
            vllm_config = VllmConfig(
                model_config=model_config,
                cache_config=cache_config,
                scheduler_config=scheduler_config,
                parallel_config=parallel_config,
                speculative_config=None,
                observability_config={},
                additional_config={},
            )

>           self.runner = TPUModelRunner(vllm_config,
                                         devices=self.mock_devices)

tests/runner/test_tpu_jax_runner.py:145:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tpu_inference.runner.tpu_jax_runner.TPUModelRunner object at 0x7825af9531d0>
vllm_config = VllmConfig(model_config=ModelConfig(model='Qwen/Qwen3-0.6B', runner='auto', convert='auto', task=None, tokenizer='Qwen...e_dir': None}, kv_transfer_config=None, kv_events_config=None, additional_config={}, instance_id='1762324586461724373')
devices = [<MagicMock id='132103251003632'>, <MagicMock id='132103284177760'>, <MagicMock id='132103251011264'>, <MagicMock id='132103251015344'>]

    def __init__(
        self,
        vllm_config: VllmConfig,
        devices: List[Any],
    ):
        self.vllm_config = vllm_config
        self.model_config = vllm_config.model_config
        # TODO(jevinjiang): override block size based on RPA v3.
        self.cache_config = vllm_config.cache_config
        self.lora_config = vllm_config.lora_config
        self.load_config = vllm_config.load_config
        self.parallel_config = vllm_config.parallel_config
        self.scheduler_config = vllm_config.scheduler_config
        self.speculative_config = vllm_config.speculative_config
        self.observability_config = vllm_config.observability_config
        self.device_config = vllm_config.device_config

        self.devices = devices
        self.dtype = self.model_config.dtype
        self.maybe_forbid_compile = runner_utils.ForbidCompile(
        ) if envs.VLLM_XLA_CHECK_RECOMPILATION else nullcontext()
        self.dp_size = self.vllm_config.sharding_config.total_dp_size

        self._init_random()
        self._init_mesh()
        self._init_phased_profiling()
        self._init_mm()
        self._init_inputs()
        self._init_speculative_decoding()

        # Delegate functions to specific manager classes.
        self.compilation_manager = CompilationManager(self)
        self.speculative_decoding_manager = SpeculativeDecodingManager(self)
        self.structured_decoding_manager = StructuredDecodingManager(self)
        self.kv_cache_manager = KVCacheManager(self)
        self.mm_manager = MultiModalManager(self)
        self.persistent_batch_manager = PersistentBatchManager(
            self.requests, self.input_batch, self.encoder_cache,
            self.uses_mrope, self.model_config)
        self.lora_utils = LoraUtils(self)

        cache_config = self.cache_config
        if cache_config.cache_dtype == "auto":
            model_dtype = self.dtype
            if isinstance(model_dtype, str):
                self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[model_dtype]
            elif isinstance(getattr(model_dtype, 'dtype', None), jnp.dtype):
                self.kv_cache_dtype = j2t_dtype(model_dtype.dtype)
            elif isinstance(model_dtype, torch.dtype):
                self.kv_cache_dtype = model_dtype
            else:
                raise ValueError(
                    "KV cache is unsupported for model_dtype of %s",
                    model_dtype)
        else:
            self.kv_cache_dtype = TPU_STR_DTYPE_TO_TORCH_DTYPE[
                cache_config.cache_dtype]

        self._pre_async_results: AsyncPreResults | None = None
        self._substitute_placeholder_token_fn = _substitute_placeholder_token
        self.execute_model_state: ExecuteModelState | None = None

>       self.data_parallel_mlp_sharding = NamedSharding(
            self.mesh, PartitionSpec(ShardingAxisName.MLP_DATA))
E       RuntimeError: std::bad_cast

tpu_inference/runner/tpu_jax_runner.py:247: RuntimeError
=================================== FAILURES ===================================
_____________________________ test_get_flax_model ______________________________

vllm_config = <MagicMock spec='VllmConfig' id='132114577348352'>
mesh = Mesh(axis_sizes=(8,), axis_names=('model',), axis_types=(Auto,))

    def test_get_flax_model(vllm_config, mesh):
        """
        An integration test for the main public function `get_flax_model`.
        It verifies that the function returns two valid, JIT-compiled functions
        that execute correctly and produce outputs with the expected sharding.
        """
        rng = jax.random.PRNGKey(42)

        # 1. Get the compiled model and logit computation functions
>       model_fn, compute_logits_fn, *_ = model_loader.get_flax_model(
            vllm_config, rng, mesh)

tests/models/common/test_model_loader.py:215:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tpu_inference/models/common/model_loader.py:200: in get_flax_model
    jit_model = _get_nnx_model(model_class, vllm_config, rng, mesh)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tpu_inference/models/common/model_loader.py:175: in _get_nnx_model
    model = nnx.eval_shape(abstract_model_fn)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/flax/nnx/transforms/transforms.py:151: in eval_shape
    out = jax.eval_shape(_eval_shape_fn, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/flax/nnx/transforms/transforms.py:148: in _eval_shape_fn
    out = f(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^
tpu_inference/models/common/model_loader.py:78: in create_abstract_model
    return model_class(vllm_config, rng, mesh)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/flax/nnx/pytreelib.py:289: in __call__
    return _graph_node_meta_call(cls, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/flax/nnx/pytreelib.py:301: in _graph_node_meta_call
    cls._pytree_meta_construct(node, *args, **kwargs)
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/flax/nnx/pytreelib.py:292: in _pytree_meta_construct
    self.__init__(*args, **kwargs)
tpu_inference/models/jax/qwen3.py:238: in __init__
    self.model = Qwen3Model(
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/flax/nnx/pytreelib.py:289: in __call__
    return _graph_node_meta_call(cls, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/flax/nnx/pytreelib.py:301: in _graph_node_meta_call
    cls._pytree_meta_construct(node, *args, **kwargs)
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/flax/nnx/pytreelib.py:292: in _pytree_meta_construct
    self.__init__(*args, **kwargs)
tpu_inference/models/jax/qwen3.py:205: in __init__
    Qwen3DecoderLayer(
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/flax/nnx/pytreelib.py:289: in __call__
    return _graph_node_meta_call(cls, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/flax/nnx/pytreelib.py:301: in _graph_node_meta_call
    cls._pytree_meta_construct(node, *args, **kwargs)
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/flax/nnx/pytreelib.py:292: in _pytree_meta_construct
    self.__init__(*args, **kwargs)
tpu_inference/models/jax/qwen3.py:166: in __init__
    self.self_attn = Qwen3Attention(config=config,
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/flax/nnx/pytreelib.py:289: in __call__
    return _graph_node_meta_call(cls, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/flax/nnx/pytreelib.py:301: in _graph_node_meta_call
    cls._pytree_meta_construct(node, *args, **kwargs)
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/flax/nnx/pytreelib.py:292: in _pytree_meta_construct
    self.__init__(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = Qwen3Attention(
  hidden_size=1024,
  num_heads=16,
  num_kv_heads=8,
  rope_theta=1000000,
  rope_scaling=None,
  rms_norm_eps=1e-06,
  head_dim_original=128,
  head_dim=128
)
config = Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
...true,
  "transformers_version": "4.56.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

dtype = <class 'jax.numpy.bfloat16'>
rng = Rngs( # RngState: 2 (12 B)
  default=RngStream( # RngState: 2 (12 B)
    tag='default',
    key=RngKey( # 1 (8 B)
    ...    tag='default'
    ),
    count=RngCount( # 1 (4 B)
      value=JitTracer<uint32[]>,
      tag='default'
    )
  )
)
mesh = Mesh(axis_sizes=(8,), axis_names=('model',), axis_types=(Auto,))
kv_cache_dtype = 'auto'

    def __init__(self, config: Qwen3Config, dtype: jnp.dtype, rng: nnx.Rngs,
                 mesh: Mesh, kv_cache_dtype: str):
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.num_kv_heads = config.num_key_value_heads
        self.rope_theta = config.rope_theta
        self.rope_scaling = getattr(config, "rope_scaling", None)
        self.rms_norm_eps = config.rms_norm_eps

        self.head_dim_original = getattr(config, "head_dim",
                                         self.hidden_size // self.num_heads)
        self.head_dim = utils.get_padded_head_dim(self.head_dim_original)

>       sharding_size = mesh.shape["model"] * mesh.shape["attn_dp"]
                                              ^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'attn_dp'
E       --------------------
E       For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

tpu_inference/models/jax/qwen3.py:42: KeyError
_____________________________ test_get_vllm_model ______________________________

mesh = Mesh(axis_sizes=(8,), axis_names=('model',), axis_types=(Auto,))

    def test_get_vllm_model(mesh):
        """
        An integration test for the main public function `get_vllm_model`.
        It verifies that the function returns two valid, JIT-compiled functions
        that execute correctly and produce outputs with the expected sharding.
        """
        rng = jax.random.PRNGKey(42)

        engine_args = EngineArgs(model="Qwen/Qwen3-0.6B")
        vllm_config = engine_args.create_engine_config()
        vllm_config.model_config.dtype = torch.bfloat16

        with set_current_vllm_config(vllm_config):
            temp_file = tempfile.mkstemp()[1]
            init_distributed_environment(
                world_size=1,
                rank=0,
                local_rank=0,
                distributed_init_method=f"file://{temp_file}",
                backend="gloo",
            )
            ensure_model_parallel_initialized(
                tensor_model_parallel_size=1,
                pipeline_model_parallel_size=1,
            )

>       model_fn, compute_logits_fn, *_ = model_loader.get_vllm_model(
            vllm_config, rng, mesh)

tests/models/common/test_model_loader.py:248:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tpu_inference/models/common/model_loader.py:302: in get_vllm_model
    params, lora_manager = model.load_weights()
                           ^^^^^^^^^^^^^^^^^^^^
tpu_inference/models/vllm/vllm_model_wrapper.py:110: in load_weights
    vllm_model = vllm_get_model(vllm_config=vllm_config_for_load)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../vllm/vllm/model_executor/model_loader/__init__.py:130: in get_model
    return loader.load_model(vllm_config=vllm_config, model_config=model_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../vllm/vllm/model_executor/model_loader/base_loader.py:56: in load_model
    process_weights_after_loading(model, model_config, target_device)
../vllm/vllm/model_executor/model_loader/utils.py:107: in process_weights_after_loading
    quant_method.process_weights_after_loading(module)
tpu_inference/layers/vllm/quantization/unquantized.py:81: in process_weights_after_loading
    NamedSharding(self.jax_config.mesh,
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:462: in check_pspec
    _check_mesh_resource_axis(mesh, spec)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

mesh = Mesh(axis_sizes=(8,), axis_names=('model',), axis_types=(Auto,))
pspec = PartitionSpec(None, ('attn_dp', 'model', 'expert'))

    def _check_mesh_resource_axis(mesh, pspec):
      for p in pspec:
        if p is PartitionSpec.UNCONSTRAINED or p is None:
          continue
        p = p if isinstance(p, tuple) else (p,)
        for r in p:
          if r not in mesh.axis_names:
>           raise ValueError(
                f"Resource axis: {r} of {pspec} "
                f"is not found in mesh: {tuple(mesh.shape.keys())}.")
E           ValueError: Resource axis: attn_dp of PartitionSpec(None, ('attn_dp', 'model', 'expert')) is not found in mesh: ('model',).

../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:517: ValueError
------------------------------ Captured log call -------------------------------
WARNING  root:ops_registry.py:36 Duplicate op registration for aten.__and__
___________________ test_get_vllm_model_random_weights[True] ___________________

mesh = Mesh(axis_sizes=(8,), axis_names=('model',), axis_types=(Auto,))
set_in_config = True

    @pytest.mark.parametrize("set_in_config", [True, False])
    def test_get_vllm_model_random_weights(mesh, set_in_config):
        rng = jax.random.PRNGKey(42)

        engine_args = EngineArgs(model="Qwen/Qwen3-0.6B")
        vllm_config = engine_args.create_engine_config()
        vllm_config.model_config.dtype = torch.bfloat16
        if set_in_config:
            vllm_config.load_config.load_format = "dummy"
        else:
            os.environ["JAX_RANDOM_WEIGHTS"] = "True"

        with set_current_vllm_config(vllm_config):
            temp_file = tempfile.mkstemp()[1]
            init_distributed_environment(
                world_size=1,
                rank=0,
                local_rank=0,
                distributed_init_method=f"file://{temp_file}",
                backend="gloo",
            )
            ensure_model_parallel_initialized(
                tensor_model_parallel_size=1,
                pipeline_model_parallel_size=1,
            )

        with patch(
                "vllm.model_executor.model_loader.dummy_loader.DummyModelLoader.load_weights"
        ) as mock_load:
>           model_fn, compute_logits_fn, *_ = model_loader.get_vllm_model(
                vllm_config, rng, mesh)

tests/models/common/test_model_loader.py:284:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tpu_inference/models/common/model_loader.py:302: in get_vllm_model
    params, lora_manager = model.load_weights()
                           ^^^^^^^^^^^^^^^^^^^^
tpu_inference/models/vllm/vllm_model_wrapper.py:110: in load_weights
    vllm_model = vllm_get_model(vllm_config=vllm_config_for_load)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../vllm/vllm/model_executor/model_loader/__init__.py:130: in get_model
    return loader.load_model(vllm_config=vllm_config, model_config=model_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../vllm/vllm/model_executor/model_loader/base_loader.py:56: in load_model
    process_weights_after_loading(model, model_config, target_device)
../vllm/vllm/model_executor/model_loader/utils.py:107: in process_weights_after_loading
    quant_method.process_weights_after_loading(module)
tpu_inference/layers/vllm/quantization/unquantized.py:81: in process_weights_after_loading
    NamedSharding(self.jax_config.mesh,
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:462: in check_pspec
    _check_mesh_resource_axis(mesh, spec)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

mesh = Mesh(axis_sizes=(8,), axis_names=('model',), axis_types=(Auto,))
pspec = PartitionSpec(None, ('attn_dp', 'model', 'expert'))

    def _check_mesh_resource_axis(mesh, pspec):
      for p in pspec:
        if p is PartitionSpec.UNCONSTRAINED or p is None:
          continue
        p = p if isinstance(p, tuple) else (p,)
        for r in p:
          if r not in mesh.axis_names:
>           raise ValueError(
                f"Resource axis: {r} of {pspec} "
                f"is not found in mesh: {tuple(mesh.shape.keys())}.")
E           ValueError: Resource axis: attn_dp of PartitionSpec(None, ('attn_dp', 'model', 'expert')) is not found in mesh: ('model',).

../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:517: ValueError
__________________ test_get_vllm_model_random_weights[False] ___________________

mesh = Mesh(axis_sizes=(8,), axis_names=('model',), axis_types=(Auto,))
set_in_config = False

    @pytest.mark.parametrize("set_in_config", [True, False])
    def test_get_vllm_model_random_weights(mesh, set_in_config):
        rng = jax.random.PRNGKey(42)

        engine_args = EngineArgs(model="Qwen/Qwen3-0.6B")
        vllm_config = engine_args.create_engine_config()
        vllm_config.model_config.dtype = torch.bfloat16
        if set_in_config:
            vllm_config.load_config.load_format = "dummy"
        else:
            os.environ["JAX_RANDOM_WEIGHTS"] = "True"

        with set_current_vllm_config(vllm_config):
            temp_file = tempfile.mkstemp()[1]
            init_distributed_environment(
                world_size=1,
                rank=0,
                local_rank=0,
                distributed_init_method=f"file://{temp_file}",
                backend="gloo",
            )
            ensure_model_parallel_initialized(
                tensor_model_parallel_size=1,
                pipeline_model_parallel_size=1,
            )

        with patch(
                "vllm.model_executor.model_loader.dummy_loader.DummyModelLoader.load_weights"
        ) as mock_load:
>           model_fn, compute_logits_fn, *_ = model_loader.get_vllm_model(
                vllm_config, rng, mesh)

tests/models/common/test_model_loader.py:284:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tpu_inference/models/common/model_loader.py:302: in get_vllm_model
    params, lora_manager = model.load_weights()
                           ^^^^^^^^^^^^^^^^^^^^
tpu_inference/models/vllm/vllm_model_wrapper.py:110: in load_weights
    vllm_model = vllm_get_model(vllm_config=vllm_config_for_load)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../vllm/vllm/model_executor/model_loader/__init__.py:130: in get_model
    return loader.load_model(vllm_config=vllm_config, model_config=model_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../vllm/vllm/model_executor/model_loader/base_loader.py:56: in load_model
    process_weights_after_loading(model, model_config, target_device)
../vllm/vllm/model_executor/model_loader/utils.py:107: in process_weights_after_loading
    quant_method.process_weights_after_loading(module)
tpu_inference/layers/vllm/quantization/unquantized.py:81: in process_weights_after_loading
    NamedSharding(self.jax_config.mesh,
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:462: in check_pspec
    _check_mesh_resource_axis(mesh, spec)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

mesh = Mesh(axis_sizes=(8,), axis_names=('model',), axis_types=(Auto,))
pspec = PartitionSpec(None, ('attn_dp', 'model', 'expert'))

    def _check_mesh_resource_axis(mesh, pspec):
      for p in pspec:
        if p is PartitionSpec.UNCONSTRAINED or p is None:
          continue
        p = p if isinstance(p, tuple) else (p,)
        for r in p:
          if r not in mesh.axis_names:
>           raise ValueError(
                f"Resource axis: {r} of {pspec} "
                f"is not found in mesh: {tuple(mesh.shape.keys())}.")
E           ValueError: Resource axis: attn_dp of PartitionSpec(None, ('attn_dp', 'model', 'expert')) is not found in mesh: ('model',).

../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:517: ValueError
________________________________ test_attention ________________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7825c0551010>
mesh = Mesh(axis_sizes=(8, 1, 1), axis_names=('data', 'attn_dp', 'model'), axis_types=(Auto, Auto, Auto))

    def test_attention(monkeypatch, mesh):
        """
        Tests the main `attention` function.

        Verifies that:
        1. It calls the `sharded_ragged_paged_attention` kernel with correct metadata.
        2. The final outputs (kv_cache and attention output) have the correct shapes.
        """
        # 1. Arrange

        # Create input tensors
        q_dtype = jnp.float32
        kv_dtype = jnp.float32
        q = jnp.ones((TOTAL_TOKENS, NUM_HEADS, PADDED_HEAD_DIM), dtype=q_dtype)
        k = jnp.ones((TOTAL_TOKENS, NUM_KV_HEADS, PADDED_HEAD_DIM), dtype=kv_dtype)
        v = jnp.ones((TOTAL_TOKENS, NUM_KV_HEADS, PADDED_HEAD_DIM), dtype=kv_dtype)

        kv_cache_shape = get_kv_cache_shape_with_mesh(mesh, NUM_BLOCKS, BLOCK_SIZE,
                                                      NUM_KV_HEADS, HEAD_DIM,
                                                      kv_dtype)
        kv_cache = jnp.zeros(kv_cache_shape, dtype=kv_dtype)

        # Mock ragged_paged_attention to return a tensor of the correct shape
        mock_paged_attn_kernel = MagicMock(
            return_value=(jnp.ones((TOTAL_TOKENS, NUM_HEADS, PADDED_HEAD_DIM)),
                          kv_cache))
        monkeypatch.setattr(
            "tpu_inference.layers.jax.attention_interface.ragged_paged_attention",
            mock_paged_attn_kernel,
        )

        # Create AttentionMetadata
        attention_metadata = AttentionMetadata(
            input_positions=jnp.arange(TOTAL_TOKENS, dtype=jnp.int32),
            block_tables=jnp.zeros((MAX_NUM_SEQS * MAX_BLOCKS_PER_SEQ, ),
                                   dtype=jnp.int32),
            seq_lens=jnp.array([5, 5, 0, 0], dtype=jnp.int32),
            query_start_loc=jnp.array([0, 5, 10, 10, 10], dtype=jnp.int32),
            request_distribution=jnp.array([0, 0, NUM_SEQS], dtype=jnp.int32),
        )

        # 2. Act
>       final_kv_cache, output = attention(
            kv_cache=kv_cache,
            q=q,
            k=k,
            v=v,
            attention_metadata=attention_metadata,
            mesh=mesh,
            head_dim_original=HEAD_DIM,
        )

tests/models/jax/test_attention_interface.py:94:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

kv_cache = Array([[[[[0., 0., 0., ..., 0., 0., 0.]],

         [[0., 0., 0., ..., 0., 0., 0.]],

         [[0., 0., 0., ..., 0., ..., 0., 0., 0.]],

         [[0., 0., 0., ..., 0., 0., 0.]],

         [[0., 0., 0., ..., 0., 0., 0.]]]]], dtype=float32)
q = Array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
 ...., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.]]], dtype=float32)
k = Array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
 ...., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.]]], dtype=float32)
v = Array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
 ...., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.]]], dtype=float32)
attention_metadata = <[AttributeError("'AttentionMetadata' object has no attribute 'query_start_loc_cpu'") raised in repr()] AttentionMetadata object at 0x7825c0555c70>
mesh = Mesh(axis_sizes=(8, 1, 1), axis_names=('data', 'attn_dp', 'model'), axis_types=(Auto, Auto, Auto))
head_dim_original = 64, attention_chunk_size = None, q_scale = None
k_scale = None, v_scale = None

    def attention(
        kv_cache: jax.Array,
        q: jax.Array,
        k: jax.Array,
        v: jax.Array,
        attention_metadata: AttentionMetadata,
        mesh: Mesh,
        head_dim_original: int | None = None,  # before padding,
        attention_chunk_size: int | None = None,
        q_scale: float | None = None,
        k_scale: float | None = None,
        v_scale: float | None = None,
    ) -> Tuple[jax.Array, jax.Array]:
        # T: seq_len
        # N: num_heads
        # K: num_kv_heads
        # D: hidden_size
        # H: head_dim
        # L: num_blocks
        # S: block_size

        # TODO(jevinjiang, cuiq): transpose q weight offline.
        # q: (T, N, H)
        # k,v: (T, K, H)

        if head_dim_original is None:
            head_dim_original = q.shape[-1]

        md = attention_metadata

        # (T, N, H)
>       output, kv_cache = sharded_ragged_paged_attention(
            head_dim_original**-0.5, mesh, attention_chunk_size, q_scale, k_scale,
            v_scale)(
                q,
                k,
                v,
                kv_cache,
                md.seq_lens,
                md.block_tables,
                md.query_start_loc,
                md.request_distribution,
            )
E       ValueError: shard_map applied to the function '_ragged_paged_attention' was given argument arrays with axis sizes that are not evenly divisible by the corresponding mesh axis sizes:
E
E       The mesh given has shape (8, 1, 1) with corresponding axis names ('data', 'attn_dp', 'model').
E
E       * args[0] of shape float32[10,8,64], where args[0] is bound to _ragged_paged_attention's parameter 'args', corresponds to in_specs[0] of value PartitionSpec(('data', 'attn_dp'), 'model', None), which maps array axis 0 (of size 10) to mesh axes ('data', 'attn_dp') (of total size 8), but 8 does not evenly divide 10
E
E       * args[1] of shape float32[10,4,64], where args[1] is the index 1 component of _ragged_paged_attention's varargs parameter 'args', corresponds to in_specs[1] of value PartitionSpec(('data', 'attn_dp'), 'model', None), which maps array axis 0 (of size 10) to mesh axes ('data', 'attn_dp') (of total size 8), but 8 does not evenly divide 10
E
E       * args[2] of shape float32[10,4,64], where args[2] is the index 2 component of _ragged_paged_attention's varargs parameter 'args', corresponds to in_specs[2] of value PartitionSpec(('data', 'attn_dp'), 'model', None), which maps array axis 0 (of size 10) to mesh axes ('data', 'attn_dp') (of total size 8), but 8 does not evenly divide 10
E
E       * args[4] of shape int32[4], where args[4] is the index 4 component of _ragged_paged_attention's varargs parameter 'args', corresponds to in_specs[4] of value PartitionSpec(('data', 'attn_dp'),), which maps array axis 0 (of size 4) to mesh axes ('data', 'attn_dp') (of total size 8), but 8 does not evenly divide 4
E
E       * args[6] of shape int32[5], where args[6] is the index 6 component of _ragged_paged_attention's varargs parameter 'args', corresponds to in_specs[6] of value PartitionSpec(('data', 'attn_dp'),), which maps array axis 0 (of size 5) to mesh axes ('data', 'attn_dp') (of total size 8), but 8 does not evenly divide 5
E
E       * args[7] of shape int32[3], where args[7] is the index 7 component of _ragged_paged_attention's varargs parameter 'args', corresponds to in_specs[7] of value PartitionSpec(('data', 'attn_dp'),), which maps array axis 0 (of size 3) to mesh axes ('data', 'attn_dp') (of total size 8), but 8 does not evenly divide 3
E
E       Array arguments' axis sizes must be evenly divisible by the mesh axis or axes indicated by the corresponding elements of the argument's in_specs entry. Consider checking that in_specs are correct, and if so consider changing the mesh axis sizes or else padding the input and adapting '_ragged_paged_attention' appropriately.
E       --------------------
E       For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

tpu_inference/layers/jax/attention_interface.py:345: ValueError
___________ TestLlamaForCausalLM.test_llama32_1b[mock_vllm_config0] ____________

self = <test_llama3.TestLlamaForCausalLM object at 0x782603377710>
mock_vllm_config = <test_llama3.MockVllmConfig object at 0x78260335af90>
rng = Array([ 0, 42], dtype=uint32)
mesh = Mesh(axis_sizes=(1, 1, 1, 1), axis_names=('data', 'attn_dp', 'expert', 'model'), axis_types=(Auto, Auto, Auto, Auto))
mock_model_inputs = (Array([1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), <[AttributeError("'AttentionMetadata' object has no attribute 'query_start_loc_cpu'") raised in repr()] AttentionMetadata object at 0x7825c0550cb0>, Array([1], dtype=int32))

    @pytest.mark.parametrize("mock_vllm_config", [
        MockVllmConfig("meta-llama/Llama-3.2-1B", "auto"),
        MockVllmConfig("meta-llama/Llama-3.2-1B", "fp8")
    ])
    def test_llama32_1b(self, mock_vllm_config, rng, mesh, mock_model_inputs):
        """Tests model init and model forward for the 8B model variant."""

        # Test model init
        model = LlamaForCausalLM(mock_vllm_config, rng, mesh)

        model_config = mock_vllm_config.model_config
        hf_config = model_config.hf_config

>       assert model.mesh.shape == {"data": 1, "model": 1}
E       AssertionError: assert OrderedDict({..., 'model': 1}) == {'data': 1, 'model': 1}
E
E         Omitting 2 identical items, use -vv to show
E         Left contains 2 more items:
E         {'attn_dp': 1, 'expert': 1}
E
E         Full diff:
E         - {...
E
E         ...Full output truncated (7 lines hidden), use '-vv' to show

tests/models/jax/test_llama3.py:92: AssertionError
___________ TestLlamaForCausalLM.test_llama32_1b[mock_vllm_config1] ____________

self = <test_llama3.TestLlamaForCausalLM object at 0x7826033777d0>
mock_vllm_config = <test_llama3.MockVllmConfig object at 0x782603374830>
rng = Array([ 0, 42], dtype=uint32)
mesh = Mesh(axis_sizes=(1, 1, 1, 1), axis_names=('data', 'attn_dp', 'expert', 'model'), axis_types=(Auto, Auto, Auto, Auto))
mock_model_inputs = (Array([1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), <[AttributeError("'AttentionMetadata' object has no attribute 'query_start_loc_cpu'") raised in repr()] AttentionMetadata object at 0x7825bf973200>, Array([1], dtype=int32))

    @pytest.mark.parametrize("mock_vllm_config", [
        MockVllmConfig("meta-llama/Llama-3.2-1B", "auto"),
        MockVllmConfig("meta-llama/Llama-3.2-1B", "fp8")
    ])
    def test_llama32_1b(self, mock_vllm_config, rng, mesh, mock_model_inputs):
        """Tests model init and model forward for the 8B model variant."""

        # Test model init
        model = LlamaForCausalLM(mock_vllm_config, rng, mesh)

        model_config = mock_vllm_config.model_config
        hf_config = model_config.hf_config

>       assert model.mesh.shape == {"data": 1, "model": 1}
E       AssertionError: assert OrderedDict({..., 'model': 1}) == {'data': 1, 'model': 1}
E
E         Omitting 2 identical items, use -vv to show
E         Left contains 2 more items:
E         {'attn_dp': 1, 'expert': 1}
E
E         Full diff:
E         - {...
E
E         ...Full output truncated (7 lines hidden), use '-vv' to show

tests/models/jax/test_llama3.py:92: AssertionError
____________ TestPhi3ForCausalLM.test_phi3_mini[mock_vllm_config0] _____________

self = <test_phi3.TestPhi3ForCausalLM object at 0x7826032486b0>
mock_vllm_config = <test_phi3.MockVllmConfig object at 0x7826032132f0>
rng = Array([ 0, 42], dtype=uint32)
mesh = Mesh(axis_sizes=(1, 1, 1, 1), axis_names=('data', 'attn_dp', 'expert', 'model'), axis_types=(Auto, Auto, Auto, Auto))
mock_model_inputs = (Array([1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), <[AttributeError("'AttentionMetadata' object has no attribute 'query_start_loc_cpu'") raised in repr()] AttentionMetadata object at 0x7825c01c5520>, Array([1], dtype=int32))

    @pytest.mark.parametrize("mock_vllm_config", [
        MockVllmConfig("microsoft/Phi-3.5-mini-instruct", "auto"),
        MockVllmConfig("microsoft/Phi-3.5-mini-instruct", "fp8")
    ])
    def test_phi3_mini(self, mock_vllm_config, rng, mesh, mock_model_inputs):
        """Tests model init and model forward."""

        # Test model init
        model = Phi3ForCausalLM(mock_vllm_config, rng, mesh)

        model_config = mock_vllm_config.model_config
        hf_config = model_config.hf_config

>       assert model.mesh.shape == {"data": 1, "model": 1}
E       AssertionError: assert OrderedDict({..., 'model': 1}) == {'data': 1, 'model': 1}
E
E         Omitting 2 identical items, use -vv to show
E         Left contains 2 more items:
E         {'attn_dp': 1, 'expert': 1}
E
E         Full diff:
E         - {...
E
E         ...Full output truncated (7 lines hidden), use '-vv' to show

tests/models/jax/test_phi3.py:91: AssertionError
____________ TestPhi3ForCausalLM.test_phi3_mini[mock_vllm_config1] _____________

self = <test_phi3.TestPhi3ForCausalLM object at 0x7826032487d0>
mock_vllm_config = <test_phi3.MockVllmConfig object at 0x782603231ee0>
rng = Array([ 0, 42], dtype=uint32)
mesh = Mesh(axis_sizes=(1, 1, 1, 1), axis_names=('data', 'attn_dp', 'expert', 'model'), axis_types=(Auto, Auto, Auto, Auto))
mock_model_inputs = (Array([1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), <[AttributeError("'AttentionMetadata' object has no attribute 'query_start_loc_cpu'") raised in repr()] AttentionMetadata object at 0x7825b5d7f5f0>, Array([1], dtype=int32))

    @pytest.mark.parametrize("mock_vllm_config", [
        MockVllmConfig("microsoft/Phi-3.5-mini-instruct", "auto"),
        MockVllmConfig("microsoft/Phi-3.5-mini-instruct", "fp8")
    ])
    def test_phi3_mini(self, mock_vllm_config, rng, mesh, mock_model_inputs):
        """Tests model init and model forward."""

        # Test model init
        model = Phi3ForCausalLM(mock_vllm_config, rng, mesh)

        model_config = mock_vllm_config.model_config
        hf_config = model_config.hf_config

>       assert model.mesh.shape == {"data": 1, "model": 1}
E       AssertionError: assert OrderedDict({..., 'model': 1}) == {'data': 1, 'model': 1}
E
E         Omitting 2 identical items, use -vv to show
E         Left contains 2 more items:
E         {'attn_dp': 1, 'expert': 1}
E
E         Full diff:
E         - {...
E
E         ...Full output truncated (7 lines hidden), use '-vv' to show

tests/models/jax/test_phi3.py:91: AssertionError
___________ TestQwen3ForCausalLM.test_qwen3_600M[mock_vllm_config0] ____________

self = <test_qwen3.TestQwen3ForCausalLM object at 0x7826032ac230>
mock_vllm_config = <test_qwen3.MockVllmConfig object at 0x782603283ce0>
rng = Array([ 0, 42], dtype=uint32)
mesh = Mesh(axis_sizes=(1, 1, 1), axis_names=('data', 'attn_dp', 'model'), axis_types=(Auto, Auto, Auto))
mock_model_inputs = (Array([1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), <[AttributeError("'AttentionMetadata' object has no attribute 'query_start_loc_cpu'") raised in repr()] AttentionMetadata object at 0x7825b9198380>, Array([1], dtype=int32))

    @pytest.mark.parametrize("mock_vllm_config", [
        MockVllmConfig("Qwen/Qwen3-0.6B", "auto"),
        MockVllmConfig("Qwen/Qwen3-0.6B", "fp8")
    ])
    def test_qwen3_600M(self, mock_vllm_config, rng, mesh, mock_model_inputs):
        """Tests model init and model forward for the 0.6B model variant."""

        # Test model init
        model = Qwen3ForCausalLM(mock_vllm_config, rng, mesh)

        model_config = mock_vllm_config.model_config
        hf_config = model_config.hf_config

>       assert model.mesh.shape == {"data": 1, "model": 1}
E       AssertionError: assert OrderedDict({..., 'model': 1}) == {'data': 1, 'model': 1}
E
E         Omitting 2 identical items, use -vv to show
E         Left contains 1 more item:
E         {'attn_dp': 1}
E
E         Full diff:
E         - {...
E
E         ...Full output truncated (6 lines hidden), use '-vv' to show

tests/models/jax/test_qwen3.py:90: AssertionError
___________ TestQwen3ForCausalLM.test_qwen3_600M[mock_vllm_config1] ____________

self = <test_qwen3.TestQwen3ForCausalLM object at 0x7826032ac350>
mock_vllm_config = <test_qwen3.MockVllmConfig object at 0x782603213b30>
rng = Array([ 0, 42], dtype=uint32)
mesh = Mesh(axis_sizes=(1, 1, 1), axis_names=('data', 'attn_dp', 'model'), axis_types=(Auto, Auto, Auto))
mock_model_inputs = (Array([1, 1, 1, 1, 1, 1, 1, 1], dtype=int32), <[AttributeError("'AttentionMetadata' object has no attribute 'query_start_loc_cpu'") raised in repr()] AttentionMetadata object at 0x7825b05ec770>, Array([1], dtype=int32))

    @pytest.mark.parametrize("mock_vllm_config", [
        MockVllmConfig("Qwen/Qwen3-0.6B", "auto"),
        MockVllmConfig("Qwen/Qwen3-0.6B", "fp8")
    ])
    def test_qwen3_600M(self, mock_vllm_config, rng, mesh, mock_model_inputs):
        """Tests model init and model forward for the 0.6B model variant."""

        # Test model init
        model = Qwen3ForCausalLM(mock_vllm_config, rng, mesh)

        model_config = mock_vllm_config.model_config
        hf_config = model_config.hf_config

>       assert model.mesh.shape == {"data": 1, "model": 1}
E       AssertionError: assert OrderedDict({..., 'model': 1}) == {'data': 1, 'model': 1}
E
E         Omitting 2 identical items, use -vv to show
E         Left contains 1 more item:
E         {'attn_dp': 1}
E
E         Full diff:
E         - {...
E
E         ...Full output truncated (6 lines hidden), use '-vv' to show

tests/models/jax/test_qwen3.py:90: AssertionError
____________________________ test_create_kv_caches _____________________________

mesh = Mesh(axis_sizes=(8,), axis_names=('model',), axis_types=(Auto,))

    def test_create_kv_caches(mesh: Mesh):
        """
        Tests that `create_kv_caches` correctly allocates and shards the KV caches
        for all specified layers.
        """
        num_blocks = 64
        block_size = 16
        num_kv_heads = 8
        head_size = 128
        layer_names = ["decoder.0", "decoder.1", "decoder.2"]  # Test with 3 layers

        expected_sharding = NamedSharding(mesh, PartitionSpec(None, None, "model"))
        expected_dtype = jnp.bfloat16
        expected_shape = get_kv_cache_shape_with_mesh(mesh, num_blocks, block_size,
                                                      num_kv_heads, head_size,
                                                      expected_dtype)

        with patch("tpu_inference.logger.init_logger",
                   return_value=MagicMock()), patch(
                       "tpu_inference.utils.hbm_usage_gb",
                       return_value=[(0.0, 0.0), (0.0, 0.0)]):
>           kv_caches = create_kv_caches(
                num_blocks=num_blocks,
                block_size=block_size,
                num_kv_heads=num_kv_heads,
                head_size=head_size,
                mesh=mesh,
                layer_names=layer_names,
            )

tests/runner/test_kv_cache.py:39:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tpu_inference/runner/kv_cache.py:79: in create_kv_caches
    sharding = NamedSharding(
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:462: in check_pspec
    _check_mesh_resource_axis(mesh, spec)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

mesh = Mesh(axis_sizes=(8,), axis_names=('model',), axis_types=(Auto,))
pspec = PartitionSpec(('data', 'attn_dp'), None, 'model')

    def _check_mesh_resource_axis(mesh, pspec):
      for p in pspec:
        if p is PartitionSpec.UNCONSTRAINED or p is None:
          continue
        p = p if isinstance(p, tuple) else (p,)
        for r in p:
          if r not in mesh.axis_names:
>           raise ValueError(
                f"Resource axis: {r} of {pspec} "
                f"is not found in mesh: {tuple(mesh.shape.keys())}.")
E           ValueError: Resource axis: data of PartitionSpec(('data', 'attn_dp'), None, 'model') is not found in mesh: ('model',).

../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/jax/_src/named_sharding.py:517: ValueError
______________ TestTPUWorker.test_init_device_autodetects_devices ______________

self = <tpu_worker_jax_test.TestTPUWorker object at 0x7825faf68800>
mock_ensure_kv_transfer_initialized = <MagicMock name='ensure_kv_transfer_initialized' id='132103397318416'>
mock_jax = <MagicMock name='jax' id='132103292986496'>
mock_utils = <MagicMock name='utils' id='132103292983568'>
mock_runner_cls = <MagicMock name='TPUModelRunner' id='132103347456912'>
mock_vllm_config = <MagicMock id='132103305748048'>

    @patch('tpu_inference.worker.tpu_worker_jax.TPUModelRunner')
    @patch('tpu_inference.worker.tpu_worker_jax.utils')
    @patch('tpu_inference.worker.tpu_worker_jax.jax')
    @patch(
        'tpu_inference.worker.tpu_worker_jax.ensure_kv_transfer_initialized')
    def test_init_device_autodetects_devices(
            self, mock_ensure_kv_transfer_initialized, mock_jax, mock_utils,
            mock_runner_cls, mock_vllm_config):
        """Tests init_device when devices are auto-detected via JAX."""
        worker = TPUWorker(
            vllm_config=mock_vllm_config,
            local_rank=0,
            rank=0,
            distributed_init_method="test_method",
            devices=[]  # No devices provided, should trigger auto-detection
        )
        mock_jax.devices.return_value = ['tpu:0', 'tpu:1', 'tpu:2', 'tpu:3']

>       worker.init_device()

tests/worker/tpu_worker_jax_test.py:139:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tpu_inference.worker.tpu_worker_jax.TPUWorker object at 0x7825b5196ba0>

    def init_device(self):
        if not self.devices:
            sharding_config: ShardingConfigManager = self.vllm_config.sharding_config
            device_indexes = sharding_config.device_indexes
            if device_indexes is not None:
                # Enforcing the devices sequence to be consistent with the specified device indexes
                self.devices = [jax.devices()[i] for i in device_indexes]
                all_devices = jax.devices()
>               device_dict = {device.id: device for device in all_devices}
                               ^^^^^^^^^
E               AttributeError: 'str' object has no attribute 'id'

tpu_inference/worker/tpu_worker_jax.py:121: AttributeError
=============================== warnings summary ===============================
../miniconda3/envs/wenxin_test/lib/python3.12/site-packages/tpu_info/device.py:32
  /home/wenxindong_google_com/miniconda3/envs/wenxin_test/lib/python3.12/site-packages/tpu_info/device.py:32: DeprecationWarning: In 3.13 classes created inside an enum will not become a member.  Use the `member` decorator to keep the current behavior.
    class Info(typing.NamedTuple):

<frozen importlib._bootstrap>:488
  <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute

<frozen importlib._bootstrap>:488
  <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute

tests/models/jax/test_weight_loading.py::WeightTransfer::test_transfer_state
tests/models/jax/test_weight_loading.py::WeightTransfer::test_transfer_state
  /home/wenxindong_google_com/miniconda3/envs/wenxin_test/lib/python3.12/site-packages/flax/nnx/statelib.py:317: DeprecationWarning: `flax.nnx.State` will be deprecated and be replaced by the built-in Python dict. Please use the equivalent `nnx.to_flat_state` instead.
    warnings.warn(

tests/models/jax/test_weight_loading.py::WeightTransfer::test_transfer_state
  /home/wenxindong_google_com/miniconda3/envs/wenxin_test/lib/python3.12/site-packages/flax/nnx/statelib.py:330: DeprecationWarning: `flax.nnx.State` will be deprecated and be replaced by the built-in Python dict. Please use the equivalent `nnx.from_flat_state` instead.
    warnings.warn(

tests/worker/tpu_worker_jax_test.py::TestTPUWorker::test_add_lora_not_implemented
  /home/wenxindong_google_com/miniconda3/envs/wenxin_test/lib/python3.12/site-packages/_pytest/threadexception.py:58: PytestUnhandledThreadExceptionWarning: Exception in thread Thread-4 (_report_usage_worker)

  Traceback (most recent call last):
    File "/home/wenxindong_google_com/miniconda3/envs/wenxin_test/lib/python3.12/threading.py", line 1075, in _bootstrap_inner
      self.run()
    File "/home/wenxindong_google_com/miniconda3/envs/wenxin_test/lib/python3.12/threading.py", line 1012, in run
      self._target(*self._args, **self._kwargs)
    File "/home/wenxindong_google_com/vllm/vllm/usage/usage_lib.py", line 176, in _report_usage_worker
      self._report_usage_once(model_architecture, usage_context, extra_kvs)
    File "/home/wenxindong_google_com/vllm/vllm/usage/usage_lib.py", line 260, in _report_usage_once
      self._write_to_file(data)
    File "/home/wenxindong_google_com/vllm/vllm/usage/usage_lib.py", line 292, in _write_to_file
      json.dump(data, f)
    File "/home/wenxindong_google_com/miniconda3/envs/wenxin_test/lib/python3.12/json/__init__.py", line 179, in dump
      for chunk in iterable:
                   ^^^^^^^^
    File "/home/wenxindong_google_com/miniconda3/envs/wenxin_test/lib/python3.12/json/encoder.py", line 432, in _iterencode
      yield from _iterencode_dict(o, _current_indent_level)
    File "/home/wenxindong_google_com/miniconda3/envs/wenxin_test/lib/python3.12/json/encoder.py", line 406, in _iterencode_dict
      yield from chunks
    File "/home/wenxindong_google_com/miniconda3/envs/wenxin_test/lib/python3.12/json/encoder.py", line 439, in _iterencode
      o = _default(o)
          ^^^^^^^^^^^
    File "/home/wenxindong_google_com/miniconda3/envs/wenxin_test/lib/python3.12/json/encoder.py", line 180, in default
      raise TypeError(f'Object of type {o.__class__.__name__} '
  TypeError: Object of type MagicMock is not JSON serializable

  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.
    warnings.warn(pytest.PytestUnhandledThreadExceptionWarning(msg))

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/models/jax/test_llama3.py::TestLlamaForCausalLM::test_llama32_1b[mock_vllm_config0]
FAILED tests/models/jax/test_llama3.py::TestLlamaForCausalLM::test_llama32_1b[mock_vllm_config1]
====== 13 failed, 152 passed, 7 warnings, 25 errors in 107.51s (0:01:47) =======
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
