ARG NIGHTLY_DATE="20250714"
ARG BASE_IMAGE="us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:nightly_3.12_tpuvm_$NIGHTLY_DATE"

FROM $BASE_IMAGE

# Remove existing versions of dependencies
RUN pip uninstall -y torch torch_xla torchvision

# Install some basic utilities
RUN apt-get update && apt-get install -y \
    git \
    libopenblas-base libopenmpi-dev libomp-dev

# Install tpu_inference
WORKDIR /workspace/tpu_inference
COPY . .
RUN export TPU_INFERENCE_VERSION=$(pip index versions tpu-inference --pre 2>/dev/null | grep -oE "[0-9]+\.[0-9]+\.[0-9]+\.dev[0-9]+" | head -n 1) && \
    echo -n "TPU_INFERENCE_VERSION: $TPU_INFERENCE_VERSION" > /tmp/tpu_inference_version

# Clone vLLM
WORKDIR /workspace/vllm
ARG VLLM_REPO=https://github.com/vllm-project/vllm.git
RUN git clone $VLLM_REPO /workspace/vllm
RUN export TPU_INFERENCE_VERSION=$(cat /tmp/tpu_inference_version) && \
    echo "Using tpu-inference version: $TPU_INFERENCE_VERSION" && \
    sed -i "s/^tpu-inference==.*/tpu-inference==${TPU_INFERENCE_VERSION}/" requirements/tpu.txt && \
    echo "Starting build.sh" && \
    bash tools/vllm-tpu/build.sh ${TPU_INFERENCE_VERSION}

# Install vllm-tpu from wheel
WORKDIR /workspace/vllm/dist
RUN ls -lah *.whl
RUN echo "pip installing vllm-tpu" && \
    pip install *.whl
# RUN export TPU_INFERENCE_VERSION=$(cat /tmp/tpu_inference_version) && \
  #   export PIP_VERSION=$(echo "$TPU_INFERENCE_VERSION" | sed 's/^v//') && \
    # echo "Installing vllm-tpu version: $PIP_VERSION" && \
    # python3 -m pip install -v --no-cache-dir "vllm-tpu==$PIP_VERSION"

# Install test dependencies
RUN python3 -m pip install -e tests/vllm_test_utils
RUN python3 -m pip install --no-cache-dir \
    git+https://github.com/thuml/depyf.git \
    pytest-asyncio \
    git+https://github.com/EleutherAI/lm-evaluation-harness.git@206b7722158f58c35b7ffcd53b035fdbdda5126d#egg=lm-eval[api] \
    pytest-cov \
    tblib

CMD ["/bin/bash"]
