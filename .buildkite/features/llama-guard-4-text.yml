# llama-guard-4-text
steps:
  - label: "Correctness tests for llama-guard-4-text"
    key: "llama-guard-4-text_CorrectnessTest"
    soft_fail: true
    agents:
      queue: tpu_v6e_8_queue
    commands:
      - .buildkite/scripts/run_in_docker.sh python3 -m pytest -s -v \
         SKIP_JAX_PRECOMPILE=1 python examples/offline_llama_guard_4_inference.py \
         --max_model_len=2048 --tensor_parallel_size=1   --max_num_batched_tokens=3072  \
         --chat-template examples/chat_templates/chat_template_llama_guard_4.jinja  # TODO : replace with your correctness test command
  - label: "Record correctness test result for llama-guard-4-text"
    key: "record_llama-guard-4-text_CorrectnessTest"
    depends_on: "llama-guard-4-text_CorrectnessTest"
    env:
      CI_TARGET: "llama-guard-4-text"
      CI_STAGE: "CorrectnessTest"
    agents:
      queue: cpu
    commands:
      - |
        .buildkite/scripts/record_step_result.sh llama-guard-4-text_CorrectnessTest

  - label: "Performance tests for llama-guard-4-text"
    key: "llama-guard-4-text_PerformanceTest"
    depends_on: "record_llama-guard-4-text_CorrectnessTest"
    soft_fail: true
    agents:
      queue: tpu_v6e_8_queue
    commands:
      - echo "TO BE INCLUDED"  # TODO : replace with your performance test command
  - label: "Record performance test result for llama-guard-4-text"
    key: "record_llama-guard-4-text_PerformanceTest"
    depends_on: "llama-guard-4-text_PerformanceTest"
    env:
      CI_TARGET: "llama-guard-4-text"
      CI_STAGE: "PerformanceTest"
    agents:
      queue: cpu
    commands:
      - |
        .buildkite/scripts/record_step_result.sh llama-guard-4-text_PerformanceTest
