STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 4, 2, 128, 16384)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.44M of 64.00M vmem. Exceeded vmem capacity by 452.0K.\n\nProgram vmem requirement 64.44M:\n    scoped           64.44M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.82M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 81.09M of 64.00M vmem. Exceeded vmem capacity by 17.09M.\n\nProgram vmem requirement 81.09M:\n    scoped           81.09M\n\n  Largest program allocations in vmem:\n\n  1. Size: 47.84M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune0 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=2, num_kv_heads=2, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune1 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=2, num_kv_heads=2, head_dim=128, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 114.30M of 64.00M vmem. Exceeded vmem capacity by 50.30M.\n\nProgram vmem requirement 114.30M:\n    scoped          114.30M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Skip (page_size=256, num_kv_pages_per_block=128) because num_kv_pages_per_block=128 > pages_per_seq=64
[Debug] Skip (page_size=256, num_kv_pages_per_block=256) because num_kv_pages_per_block=256 > pages_per_seq=64
Best block size: (64, 32)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 4, 2, 128, 65536)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.44M of 64.00M vmem. Exceeded vmem capacity by 452.0K.\n\nProgram vmem requirement 64.44M:\n    scoped           64.44M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.82M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 81.09M of 64.00M vmem. Exceeded vmem capacity by 17.09M.\n\nProgram vmem requirement 81.09M:\n    scoped           81.09M\n\n  Largest program allocations in vmem:\n\n  1. Size: 47.84M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 114.30M of 64.00M vmem. Exceeded vmem capacity by 50.30M.\n\nProgram vmem requirement 114.30M:\n    scoped          114.30M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 99.96M of 64.00M vmem. Exceeded vmem capacity by 35.96M.\n\nProgram vmem requirement 99.96M:\n    scoped           99.96M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 35.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 104.04M of 64.00M vmem. Exceeded vmem capacity by 40.04M.\n\nProgram vmem requirement 104.04M:\n    scoped          104.04M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 39.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 112.15M of 64.00M vmem. Exceeded vmem capacity by 48.15M.\n\nProgram vmem requirement 112.15M:\n    scoped          112.15M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 47.84M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 128.44M of 64.00M vmem. Exceeded vmem capacity by 64.44M.\n\nProgram vmem requirement 128.44M:\n    scoped          128.44M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 63.82M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 161.11M of 64.00M vmem. Exceeded vmem capacity by 97.11M.\n\nProgram vmem requirement 161.11M:\n    scoped          161.11M\n\n  Largest program allocations in vmem:\n\n  1. Size: 95.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 234.48M of 64.00M vmem. Exceeded vmem capacity by 170.48M.\n\nProgram vmem requirement 234.48M:\n    scoped          234.48M\n\n  Largest program allocations in vmem:\n\n  1. Size: 167.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune1 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=2, num_kv_heads=2, head_dim=128, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune10 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=2, num_kv_heads=4, head_dim=256, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[       OK ] Autotune.test_autotune10 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=2, num_kv_heads=4, head_dim=256, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune11 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=2, num_kv_heads=4, head_dim=256, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[       OK ] Autotune.test_autotune11 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=2, num_kv_heads=4, head_dim=256, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune12 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=2, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (64, 32)
Skip because num_q_heads % num_kv_heads != 0
Skip because num_q_heads % num_kv_heads != 0
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 4, 2, 128, 16384)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.44M of 64.00M vmem. Exceeded vmem capacity by 452.0K.\n\nProgram vmem requirement 64.44M:\n    scoped           64.44M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.82M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 81.09M of 64.00M vmem. Exceeded vmem capacity by 17.09M.\n\nProgram vmem requirement 81.09M:\n    scoped           81.09M\n\n  Largest program allocations in vmem:\n\n  1. Size: 47.84M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 114.30M of 64.00M vmem. Exceeded vmem capacity by 50.30M.\n\nProgram vmem requirement 114.30M:\n    scoped          114.30M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune12 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=2, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune13 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=2, head_dim=128, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Skip (page_size=256, num_kv_pages_per_block=128) because num_kv_pages_per_block=128 > pages_per_seq=64
[Debug] Skip (page_size=256, num_kv_pages_per_block=256) because num_kv_pages_per_block=256 > pages_per_seq=64
Best block size: (64, 32)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 4, 2, 128, 65536)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.44M of 64.00M vmem. Exceeded vmem capacity by 452.0K.\n\nProgram vmem requirement 64.44M:\n    scoped           64.44M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.82M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 81.09M of 64.00M vmem. Exceeded vmem capacity by 17.09M.\n\nProgram vmem requirement 81.09M:\n    scoped           81.09M\n\n  Largest program allocations in vmem:\n\n  1. Size: 47.84M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 114.30M of 64.00M vmem. Exceeded vmem capacity by 50.30M.\n\nProgram vmem requirement 114.30M:\n    scoped          114.30M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 99.96M of 64.00M vmem. Exceeded vmem capacity by 35.96M.\n\nProgram vmem requirement 99.96M:\n    scoped           99.96M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 35.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 104.04M of 64.00M vmem. Exceeded vmem capacity by 40.04M.\n\nProgram vmem requirement 104.04M:\n    scoped          104.04M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 39.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 112.15M of 64.00M vmem. Exceeded vmem capacity by 48.15M.\n\nProgram vmem requirement 112.15M:\n    scoped          112.15M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 47.84M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 128.44M of 64.00M vmem. Exceeded vmem capacity by 64.44M.\n\nProgram vmem requirement 128.44M:\n    scoped          128.44M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 63.82M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 161.11M of 64.00M vmem. Exceeded vmem capacity by 97.11M.\n\nProgram vmem requirement 161.11M:\n    scoped          161.11M\n\n  Largest program allocations in vmem:\n\n  1. Size: 95.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune13 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=2, head_dim=128, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune14 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=2, head_dim=128, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 234.48M of 64.00M vmem. Exceeded vmem capacity by 170.48M.\n\nProgram vmem requirement 234.48M:\n    scoped          234.48M\n\n  Largest program allocations in vmem:\n\n  1. Size: 167.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (64, 32)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 4, 2, 128, 131072)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.44M of 64.00M vmem. Exceeded vmem capacity by 452.0K.\n\nProgram vmem requirement 64.44M:\n    scoped           64.44M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.82M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 81.11M of 64.00M vmem. Exceeded vmem capacity by 17.11M.\n\nProgram vmem requirement 81.11M:\n    scoped           81.11M\n\n  Largest program allocations in vmem:\n\n  1. Size: 47.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 114.30M of 64.00M vmem. Exceeded vmem capacity by 50.30M.\n\nProgram vmem requirement 114.30M:\n    scoped          114.30M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 99.96M of 64.00M vmem. Exceeded vmem capacity by 35.96M.\n\nProgram vmem requirement 99.96M:\n    scoped           99.96M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 35.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 104.04M of 64.00M vmem. Exceeded vmem capacity by 40.04M.\n\nProgram vmem requirement 104.04M:\n    scoped          104.04M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 39.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 112.15M of 64.00M vmem. Exceeded vmem capacity by 48.15M.\n\nProgram vmem requirement 112.15M:\n    scoped          112.15M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 47.84M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 128.44M of 64.00M vmem. Exceeded vmem capacity by 64.44M.\n\nProgram vmem requirement 128.44M:\n    scoped          128.44M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 63.82M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 161.11M of 64.00M vmem. Exceeded vmem capacity by 97.11M.\n\nProgram vmem requirement 161.11M:\n    scoped          161.11M\n\n  Largest program allocations in vmem:\n\n  1. Size: 95.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune14 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=2, head_dim=128, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune15 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=2, head_dim=256, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 234.48M of 64.00M vmem. Exceeded vmem capacity by 170.48M.\n\nProgram vmem requirement 234.48M:\n    scoped          234.48M\n\n  Largest program allocations in vmem:\n\n  1. Size: 167.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (64, 32)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 4, 2, 256, 16384)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_128-bkvp_32-p_256.1 = (bf16[2,1024,1,2,256]{4,3,2,1,0:T(2,128)(2,1)}, bf16[1000,256,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}) custom-call(%copy-done.4, %copy-done.2, %copy-done.3, %copy-done.5, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.88M and limit 60.00M exceeded scoped vmem limit by 1.88M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 79.95M of 64.00M vmem. Exceeded vmem capacity by 15.95M.\n\nProgram vmem requirement 79.95M:\n    scoped           79.95M\n\n  Largest program allocations in vmem:\n\n  1. Size: 43.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 89.76M of 64.00M vmem. Exceeded vmem capacity by 25.76M.\n\nProgram vmem requirement 89.76M:\n    scoped           89.76M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 25.63M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,256]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 91.86M of 64.00M vmem. Exceeded vmem capacity by 27.86M.\n\nProgram vmem requirement 91.86M:\n    scoped           91.86M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 27.61M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 96.12M of 64.00M vmem. Exceeded vmem capacity by 32.12M.\n\nProgram vmem requirement 96.12M:\n    scoped           96.12M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.62M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 104.61M of 64.00M vmem. Exceeded vmem capacity by 40.61M.\n\nProgram vmem requirement 104.61M:\n    scoped          104.61M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 39.61M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 121.91M of 64.00M vmem. Exceeded vmem capacity by 57.91M.\n\nProgram vmem requirement 121.91M:\n    scoped          121.91M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.91M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune15 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=2, head_dim=256, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune16 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=2, head_dim=256, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 156.24M of 64.00M vmem. Exceeded vmem capacity by 92.24M.\n\nProgram vmem requirement 156.24M:\n    scoped          156.24M\n\n  Largest program allocations in vmem:\n\n  1. Size: 88.24M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Skip (page_size=256, num_kv_pages_per_block=128) because num_kv_pages_per_block=128 > pages_per_seq=64
[Debug] Skip (page_size=256, num_kv_pages_per_block=256) because num_kv_pages_per_block=256 > pages_per_seq=64
Best block size: (32, 64)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 4, 2, 256, 65536)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_128-bkvp_32-p_256.1 = (bf16[2,1024,1,2,256]{4,3,2,1,0:T(2,128)(2,1)}, bf16[1000,256,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}) custom-call(%copy-done.4, %copy-done.2, %copy-done.3, %copy-done.5, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.88M and limit 60.00M exceeded scoped vmem limit by 1.88M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 79.95M of 64.00M vmem. Exceeded vmem capacity by 15.95M.\n\nProgram vmem requirement 79.95M:\n    scoped           79.95M\n\n  Largest program allocations in vmem:\n\n  1. Size: 43.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 89.76M of 64.00M vmem. Exceeded vmem capacity by 25.76M.\n\nProgram vmem requirement 89.76M:\n    scoped           89.76M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 25.63M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,256]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 91.86M of 64.00M vmem. Exceeded vmem capacity by 27.86M.\n\nProgram vmem requirement 91.86M:\n    scoped           91.86M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 27.61M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 96.12M of 64.00M vmem. Exceeded vmem capacity by 32.12M.\n\nProgram vmem requirement 96.12M:\n    scoped           96.12M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.62M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 104.61M of 64.00M vmem. Exceeded vmem capacity by 40.61M.\n\nProgram vmem requirement 104.61M:\n    scoped          104.61M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 39.61M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 121.91M of 64.00M vmem. Exceeded vmem capacity by 57.91M.\n\nProgram vmem requirement 121.91M:\n    scoped          121.91M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.91M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 156.24M of 64.00M vmem. Exceeded vmem capacity by 92.24M.\n\nProgram vmem requirement 156.24M:\n    scoped          156.24M\n\n  Largest program allocations in vmem:\n\n  1. Size: 88.24M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune16 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=2, head_dim=256, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune17 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=2, head_dim=256, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (32, 64)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 4, 2, 256, 131072)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_128-bkvp_32-p_256.1 = (bf16[2,1024,1,2,256]{4,3,2,1,0:T(2,128)(2,1)}, bf16[1000,256,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}) custom-call(%copy-done.4, %copy-done.2, %copy-done.3, %copy-done.5, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.88M and limit 60.00M exceeded scoped vmem limit by 1.88M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 79.95M of 64.00M vmem. Exceeded vmem capacity by 15.95M.\n\nProgram vmem requirement 79.95M:\n    scoped           79.95M\n\n  Largest program allocations in vmem:\n\n  1. Size: 43.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 89.76M of 64.00M vmem. Exceeded vmem capacity by 25.76M.\n\nProgram vmem requirement 89.76M:\n    scoped           89.76M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 25.63M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,256]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 91.86M of 64.00M vmem. Exceeded vmem capacity by 27.86M.\n\nProgram vmem requirement 91.86M:\n    scoped           91.86M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 27.61M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 96.12M of 64.00M vmem. Exceeded vmem capacity by 32.12M.\n\nProgram vmem requirement 96.12M:\n    scoped           96.12M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.62M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 104.61M of 64.00M vmem. Exceeded vmem capacity by 40.61M.\n\nProgram vmem requirement 104.61M:\n    scoped          104.61M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 39.61M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 121.91M of 64.00M vmem. Exceeded vmem capacity by 57.91M.\n\nProgram vmem requirement 121.91M:\n    scoped          121.91M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.91M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 156.24M of 64.00M vmem. Exceeded vmem capacity by 92.24M.\n\nProgram vmem requirement 156.24M:\n    scoped          156.24M\n\n  Largest program allocations in vmem:\n\n  1. Size: 88.24M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune17 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=2, head_dim=256, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune18 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=4, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (32, 64)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 8, 4, 128, 16384)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 74.09M of 64.00M vmem. Exceeded vmem capacity by 10.09M.\n\nProgram vmem requirement 74.09M:\n    scoped           74.09M\n\n  Largest program allocations in vmem:\n\n  1. Size: 39.59M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 108.19M of 64.00M vmem. Exceeded vmem capacity by 44.19M.\n\nProgram vmem requirement 108.19M:\n    scoped          108.19M\n\n  Largest program allocations in vmem:\n\n  1. Size: 71.19M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 82.95M of 64.00M vmem. Exceeded vmem capacity by 18.95M.\n\nProgram vmem requirement 82.95M:\n    scoped           82.95M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 18.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 88.12M of 64.00M vmem. Exceeded vmem capacity by 24.12M.\n\nProgram vmem requirement 88.12M:\n    scoped           88.12M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 23.81M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 96.31M of 64.00M vmem. Exceeded vmem capacity by 32.31M.\n\nProgram vmem requirement 96.31M:\n    scoped           96.31M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.68M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 112.94M of 64.00M vmem. Exceeded vmem capacity by 48.94M.\n\nProgram vmem requirement 112.94M:\n    scoped          112.94M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 47.69M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 146.33M of 64.00M vmem. Exceeded vmem capacity by 82.33M.\n\nProgram vmem requirement 146.33M:\n    scoped          146.33M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.83M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune18 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=4, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune19 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=4, head_dim=128, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 212.59M of 64.00M vmem. Exceeded vmem capacity by 148.59M.\n\nProgram vmem requirement 212.59M:\n    scoped          212.59M\n\n  Largest program allocations in vmem:\n\n  1. Size: 143.59M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Skip (page_size=256, num_kv_pages_per_block=128) because num_kv_pages_per_block=128 > pages_per_seq=64
[Debug] Skip (page_size=256, num_kv_pages_per_block=256) because num_kv_pages_per_block=256 > pages_per_seq=64
Best block size: (32, 64)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 8, 4, 128, 65536)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 74.09M of 64.00M vmem. Exceeded vmem capacity by 10.09M.\n\nProgram vmem requirement 74.09M:\n    scoped           74.09M\n\n  Largest program allocations in vmem:\n\n  1. Size: 39.59M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 108.18M of 64.00M vmem. Exceeded vmem capacity by 44.18M.\n\nProgram vmem requirement 108.18M:\n    scoped          108.18M\n\n  Largest program allocations in vmem:\n\n  1. Size: 71.18M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 82.95M of 64.00M vmem. Exceeded vmem capacity by 18.95M.\n\nProgram vmem requirement 82.95M:\n    scoped           82.95M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 18.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 88.12M of 64.00M vmem. Exceeded vmem capacity by 24.12M.\n\nProgram vmem requirement 88.12M:\n    scoped           88.12M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 23.81M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 96.31M of 64.00M vmem. Exceeded vmem capacity by 32.31M.\n\nProgram vmem requirement 96.31M:\n    scoped           96.31M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.68M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 112.94M of 64.00M vmem. Exceeded vmem capacity by 48.94M.\n\nProgram vmem requirement 112.94M:\n    scoped          112.94M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 47.69M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 146.33M of 64.00M vmem. Exceeded vmem capacity by 82.33M.\n\nProgram vmem requirement 146.33M:\n    scoped          146.33M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.83M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 212.59M of 64.00M vmem. Exceeded vmem capacity by 148.59M.\n\nProgram vmem requirement 212.59M:\n    scoped          212.59M\n\n  Largest program allocations in vmem:\n\n  1. Size: 143.59M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")[       OK ] Autotune.test_autotune19 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=4, head_dim=128, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune2 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=2, num_kv_heads=2, head_dim=128, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (32, 64)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 4, 2, 128, 131072)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.44M of 64.00M vmem. Exceeded vmem capacity by 452.0K.\n\nProgram vmem requirement 64.44M:\n    scoped           64.44M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.82M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 81.11M of 64.00M vmem. Exceeded vmem capacity by 17.11M.\n\nProgram vmem requirement 81.11M:\n    scoped           81.11M\n\n  Largest program allocations in vmem:\n\n  1. Size: 47.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 114.30M of 64.00M vmem. Exceeded vmem capacity by 50.30M.\n\nProgram vmem requirement 114.30M:\n    scoped          114.30M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 99.96M of 64.00M vmem. Exceeded vmem capacity by 35.96M.\n\nProgram vmem requirement 99.96M:\n    scoped           99.96M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 35.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 104.04M of 64.00M vmem. Exceeded vmem capacity by 40.04M.\n\nProgram vmem requirement 104.04M:\n    scoped          104.04M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 39.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 112.15M of 64.00M vmem. Exceeded vmem capacity by 48.15M.\n\nProgram vmem requirement 112.15M:\n    scoped          112.15M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 47.84M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 128.44M of 64.00M vmem. Exceeded vmem capacity by 64.44M.\n\nProgram vmem requirement 128.44M:\n    scoped          128.44M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 63.82M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 161.11M of 64.00M vmem. Exceeded vmem capacity by 97.11M.\n\nProgram vmem requirement 161.11M:\n    scoped          161.11M\n\n  Largest program allocations in vmem:\n\n  1. Size: 95.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune2 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=2, num_kv_heads=2, head_dim=128, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune20 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=4, head_dim=128, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 234.48M of 64.00M vmem. Exceeded vmem capacity by 170.48M.\n\nProgram vmem requirement 234.48M:\n    scoped          234.48M\n\n  Largest program allocations in vmem:\n\n  1. Size: 167.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (64, 32)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 8, 4, 128, 131072)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 74.09M of 64.00M vmem. Exceeded vmem capacity by 10.09M.\n\nProgram vmem requirement 74.09M:\n    scoped           74.09M\n\n  Largest program allocations in vmem:\n\n  1. Size: 39.59M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 108.18M of 64.00M vmem. Exceeded vmem capacity by 44.18M.\n\nProgram vmem requirement 108.18M:\n    scoped          108.18M\n\n  Largest program allocations in vmem:\n\n  1. Size: 71.18M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 82.95M of 64.00M vmem. Exceeded vmem capacity by 18.95M.\n\nProgram vmem requirement 82.95M:\n    scoped           82.95M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 18.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 88.12M of 64.00M vmem. Exceeded vmem capacity by 24.12M.\n\nProgram vmem requirement 88.12M:\n    scoped           88.12M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 23.81M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 96.31M of 64.00M vmem. Exceeded vmem capacity by 32.31M.\n\nProgram vmem requirement 96.31M:\n    scoped           96.31M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.68M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 112.95M of 64.00M vmem. Exceeded vmem capacity by 48.95M.\n\nProgram vmem requirement 112.95M:\n    scoped          112.95M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 47.70M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 146.33M of 64.00M vmem. Exceeded vmem capacity by 82.33M.\n\nProgram vmem requirement 146.33M:\n    scoped          146.33M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.83M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 212.59M of 64.00M vmem. Exceeded vmem capacity by 148.59M.\n\nProgram vmem requirement 212.59M:\n    scoped          212.59M\n\n  Largest program allocations in vmem:\n\n  1. Size: 143.59M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune20 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=4, head_dim=128, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune21 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=4, head_dim=256, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (32, 64)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 8, 4, 256, 16384)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=16, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 78.05M of 64.00M vmem. Exceeded vmem capacity by 14.05M.\n\nProgram vmem requirement 78.05M:\n    scoped           78.05M\n\n  Largest program allocations in vmem:\n\n  1. Size: 38.05M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_16-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4096,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 78.05M of 64.00M vmem. Exceeded vmem capacity by 14.05M.\n\nProgram vmem requirement 78.05M:\n    scoped           78.05M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 13.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 80.31M of 64.00M vmem. Exceeded vmem capacity by 16.31M.\n\nProgram vmem requirement 80.31M:\n    scoped           80.31M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 15.81M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 84.77M of 64.00M vmem. Exceeded vmem capacity by 20.77M.\n\nProgram vmem requirement 84.77M:\n    scoped           84.77M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 19.77M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 93.84M of 64.00M vmem. Exceeded vmem capacity by 29.84M.\n\nProgram vmem requirement 93.84M:\n    scoped           93.84M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 27.84M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 111.91M of 64.00M vmem. Exceeded vmem capacity by 47.91M.\n\nProgram vmem requirement 111.91M:\n    scoped          111.91M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 43.91M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 148.07M of 64.00M vmem. Exceeded vmem capacity by 84.07M.\n\nProgram vmem requirement 148.07M:\n    scoped          148.07M\n\n  Largest program allocations in vmem:\n\n  1. Size: 76.07M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune21 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=4, head_dim=256, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune22 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=4, head_dim=256, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_64-p_256.1")
[Debug] Skip (page_size=256, num_kv_pages_per_block=128) because num_kv_pages_per_block=128 > pages_per_seq=64
[Debug] Skip (page_size=256, num_kv_pages_per_block=256) because num_kv_pages_per_block=256 > pages_per_seq=64
Best block size: (16, 128)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 8, 4, 256, 65536)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=16, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 78.05M of 64.00M vmem. Exceeded vmem capacity by 14.05M.\n\nProgram vmem requirement 78.05M:\n    scoped           78.05M\n\n  Largest program allocations in vmem:\n\n  1. Size: 38.05M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_16-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4096,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 78.05M of 64.00M vmem. Exceeded vmem capacity by 14.05M.\n\nProgram vmem requirement 78.05M:\n    scoped           78.05M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 13.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 80.31M of 64.00M vmem. Exceeded vmem capacity by 16.31M.\n\nProgram vmem requirement 80.31M:\n    scoped           80.31M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 15.81M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 84.77M of 64.00M vmem. Exceeded vmem capacity by 20.77M.\n\nProgram vmem requirement 84.77M:\n    scoped           84.77M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 19.77M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 93.84M of 64.00M vmem. Exceeded vmem capacity by 29.84M.\n\nProgram vmem requirement 93.84M:\n    scoped           93.84M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 27.84M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 111.91M of 64.00M vmem. Exceeded vmem capacity by 47.91M.\n\nProgram vmem requirement 111.91M:\n    scoped          111.91M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 43.91M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 148.07M of 64.00M vmem. Exceeded vmem capacity by 84.07M.\n\nProgram vmem requirement 148.07M:\n    scoped          148.07M\n\n  Largest program allocations in vmem:\n\n  1. Size: 76.07M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune22 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=4, head_dim=256, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune23 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=4, head_dim=256, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (16, 128)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 8, 4, 256, 131072)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=16, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 78.05M of 64.00M vmem. Exceeded vmem capacity by 14.05M.\n\nProgram vmem requirement 78.05M:\n    scoped           78.05M\n\n  Largest program allocations in vmem:\n\n  1. Size: 38.05M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_16-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4096,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 78.05M of 64.00M vmem. Exceeded vmem capacity by 14.05M.\n\nProgram vmem requirement 78.05M:\n    scoped           78.05M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 13.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 80.31M of 64.00M vmem. Exceeded vmem capacity by 16.31M.\n\nProgram vmem requirement 80.31M:\n    scoped           80.31M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 15.81M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 84.77M of 64.00M vmem. Exceeded vmem capacity by 20.77M.\n\nProgram vmem requirement 84.77M:\n    scoped           84.77M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 19.77M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 93.84M of 64.00M vmem. Exceeded vmem capacity by 29.84M.\n\nProgram vmem requirement 93.84M:\n    scoped           93.84M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 27.84M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 111.90M of 64.00M vmem. Exceeded vmem capacity by 47.90M.\n\nProgram vmem requirement 111.90M:\n    scoped          111.90M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 43.90M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 148.07M of 64.00M vmem. Exceeded vmem capacity by 84.07M.\n\nProgram vmem requirement 148.07M:\n    scoped          148.07M\n\n  Largest program allocations in vmem:\n\n  1. Size: 76.07M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune23 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=4, num_kv_heads=4, head_dim=256, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune24 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=2, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (16, 128)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 8, 2, 128, 16384)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 94.64M of 64.00M vmem. Exceeded vmem capacity by 30.64M.\n\nProgram vmem requirement 94.64M:\n    scoped           94.64M\n\n  Largest program allocations in vmem:\n\n  1. Size: 73.64M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.44M of 64.00M vmem. Exceeded vmem capacity by 452.0K.\n\nProgram vmem requirement 64.44M:\n    scoped           64.44M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.82M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 81.09M of 64.00M vmem. Exceeded vmem capacity by 17.09M.\n\nProgram vmem requirement 81.09M:\n    scoped           81.09M\n\n  Largest program allocations in vmem:\n\n  1. Size: 47.84M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 114.30M of 64.00M vmem. Exceeded vmem capacity by 50.30M.\n\nProgram vmem requirement 114.30M:\n    scoped          114.30M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 186.19M of 64.00M vmem. Exceeded vmem capacity by 122.19M.\n\nProgram vmem requirement 186.19M:\n    scoped          186.19M\n\n  Largest program allocations in vmem:\n\n  1. Size: 149.19M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune24 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=2, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune25 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=2, head_dim=128, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Skip (page_size=256, num_kv_pages_per_block=128) because num_kv_pages_per_block=128 > pages_per_seq=64
[Debug] Skip (page_size=256, num_kv_pages_per_block=256) because num_kv_pages_per_block=256 > pages_per_seq=64
Best block size: (64, 16)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 8, 2, 128, 65536)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 94.63M of 64.00M vmem. Exceeded vmem capacity by 30.63M.\n\nProgram vmem requirement 94.63M:\n    scoped           94.63M\n\n  Largest program allocations in vmem:\n\n  1. Size: 73.63M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.44M of 64.00M vmem. Exceeded vmem capacity by 452.0K.\n\nProgram vmem requirement 64.44M:\n    scoped           64.44M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.82M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 81.09M of 64.00M vmem. Exceeded vmem capacity by 17.09M.\n\nProgram vmem requirement 81.09M:\n    scoped           81.09M\n\n  Largest program allocations in vmem:\n\n  1. Size: 47.84M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 114.30M of 64.00M vmem. Exceeded vmem capacity by 50.30M.\n\nProgram vmem requirement 114.30M:\n    scoped          114.30M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 186.19M of 64.00M vmem. Exceeded vmem capacity by 122.19M.\n\nProgram vmem requirement 186.19M:\n    scoped          186.19M\n\n  Largest program allocations in vmem:\n\n  1. Size: 149.19M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 104.04M of 64.00M vmem. Exceeded vmem capacity by 40.04M.\n\nProgram vmem requirement 104.04M:\n    scoped          104.04M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 39.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 112.15M of 64.00M vmem. Exceeded vmem capacity by 48.15M.\n\nProgram vmem requirement 112.15M:\n    scoped          112.15M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 47.84M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 128.44M of 64.00M vmem. Exceeded vmem capacity by 64.44M.\n\nProgram vmem requirement 128.44M:\n    scoped          128.44M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 63.82M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 161.11M of 64.00M vmem. Exceeded vmem capacity by 97.11M.\n\nProgram vmem requirement 161.11M:\n    scoped          161.11M\n\n  Largest program allocations in vmem:\n\n  1. Size: 95.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 234.48M of 64.00M vmem. Exceeded vmem capacity by 170.48M.\n\nProgram vmem requirement 234.48M:\n    scoped          234.48M\n\n  Largest program allocations in vmem:\n\n  1. Size: 167.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune25 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=2, head_dim=128, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune26 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=2, head_dim=128, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 390.38M of 64.00M vmem. Exceeded vmem capacity by 326.38M.\n\nProgram vmem requirement 390.38M:\n    scoped          390.38M\n\n  Largest program allocations in vmem:\n\n  1. Size: 321.38M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (64, 16)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 8, 2, 128, 131072)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 94.63M of 64.00M vmem. Exceeded vmem capacity by 30.63M.\n\nProgram vmem requirement 94.63M:\n    scoped           94.63M\n\n  Largest program allocations in vmem:\n\n  1. Size: 73.63M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.44M of 64.00M vmem. Exceeded vmem capacity by 452.0K.\n\nProgram vmem requirement 64.44M:\n    scoped           64.44M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.82M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 81.11M of 64.00M vmem. Exceeded vmem capacity by 17.11M.\n\nProgram vmem requirement 81.11M:\n    scoped           81.11M\n\n  Largest program allocations in vmem:\n\n  1. Size: 47.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 114.30M of 64.00M vmem. Exceeded vmem capacity by 50.30M.\n\nProgram vmem requirement 114.30M:\n    scoped          114.30M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 186.19M of 64.00M vmem. Exceeded vmem capacity by 122.19M.\n\nProgram vmem requirement 186.19M:\n    scoped          186.19M\n\n  Largest program allocations in vmem:\n\n  1. Size: 149.19M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 104.04M of 64.00M vmem. Exceeded vmem capacity by 40.04M.\n\nProgram vmem requirement 104.04M:\n    scoped          104.04M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 39.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 112.15M of 64.00M vmem. Exceeded vmem capacity by 48.15M.\n\nProgram vmem requirement 112.15M:\n    scoped          112.15M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 47.84M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 128.44M of 64.00M vmem. Exceeded vmem capacity by 64.44M.\n\nProgram vmem requirement 128.44M:\n    scoped          128.44M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 63.82M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 161.11M of 64.00M vmem. Exceeded vmem capacity by 97.11M.\n\nProgram vmem requirement 161.11M:\n    scoped          161.11M\n\n  Largest program allocations in vmem:\n\n  1. Size: 95.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 234.48M of 64.00M vmem. Exceeded vmem capacity by 170.48M.\n\nProgram vmem requirement 234.48M:\n    scoped          234.48M\n\n  Largest program allocations in vmem:\n\n  1. Size: 167.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune26 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=2, head_dim=128, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune27 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=2, head_dim=256, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 390.38M of 64.00M vmem. Exceeded vmem capacity by 326.38M.\n\nProgram vmem requirement 390.38M:\n    scoped          390.38M\n\n  Largest program allocations in vmem:\n\n  1. Size: 321.38M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,32768,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, bf16[1024,2,2,128]{3,2,1,0}, bf16[1000,256,2,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (64, 16)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 8, 2, 256, 16384)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=16, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_256-bkvp_16-p_256.1 = (bf16[2,1024,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}, bf16[1000,256,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 63.00M and limit 60.00M exceeded scoped vmem limit by 3.00M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_64-bkvp_32-p_256.1 = (bf16[2,1024,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}, bf16[1000,256,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.88M and limit 60.00M exceeded scoped vmem limit by 1.88M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 79.95M of 64.00M vmem. Exceeded vmem capacity by 15.95M.\n\nProgram vmem requirement 79.95M:\n    scoped           79.95M\n\n  Largest program allocations in vmem:\n\n  1. Size: 43.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 117.00M of 64.00M vmem. Exceeded vmem capacity by 53.00M.\n\nProgram vmem requirement 117.00M:\n    scoped          117.00M\n\n  Largest program allocations in vmem:\n\n  1. Size: 77.00M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 91.86M of 64.00M vmem. Exceeded vmem capacity by 27.86M.\n\nProgram vmem requirement 91.86M:\n    scoped           91.86M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 27.61M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 96.12M of 64.00M vmem. Exceeded vmem capacity by 32.12M.\n\nProgram vmem requirement 96.12M:\n    scoped           96.12M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.62M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 104.61M of 64.00M vmem. Exceeded vmem capacity by 40.61M.\n\nProgram vmem requirement 104.61M:\n    scoped          104.61M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 39.61M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 121.91M of 64.00M vmem. Exceeded vmem capacity by 57.91M.\n\nProgram vmem requirement 121.91M:\n    scoped          121.91M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.91M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 156.24M of 64.00M vmem. Exceeded vmem capacity by 92.24M.\n\nProgram vmem requirement 156.24M:\n    scoped          156.24M\n\n  Largest program allocations in vmem:\n\n  1. Size: 88.24M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune27 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=2, head_dim=256, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune28 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=2, head_dim=256, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 225.21M of 64.00M vmem. Exceeded vmem capacity by 161.21M.\n\nProgram vmem requirement 225.21M:\n    scoped          225.21M\n\n  Largest program allocations in vmem:\n\n  1. Size: 153.21M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Skip (page_size=256, num_kv_pages_per_block=128) because num_kv_pages_per_block=128 > pages_per_seq=64
[Debug] Skip (page_size=256, num_kv_pages_per_block=256) because num_kv_pages_per_block=256 > pages_per_seq=64
Best block size: (32, 32)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 8, 2, 256, 65536)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=16, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_256-bkvp_16-p_256.1 = (bf16[2,1024,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}, bf16[1000,256,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 63.00M and limit 60.00M exceeded scoped vmem limit by 3.00M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_64-bkvp_32-p_256.1 = (bf16[2,1024,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}, bf16[1000,256,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.88M and limit 60.00M exceeded scoped vmem limit by 1.88M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 79.95M of 64.00M vmem. Exceeded vmem capacity by 15.95M.\n\nProgram vmem requirement 79.95M:\n    scoped           79.95M\n\n  Largest program allocations in vmem:\n\n  1. Size: 43.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 117.00M of 64.00M vmem. Exceeded vmem capacity by 53.00M.\n\nProgram vmem requirement 117.00M:\n    scoped          117.00M\n\n  Largest program allocations in vmem:\n\n  1. Size: 77.00M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 91.86M of 64.00M vmem. Exceeded vmem capacity by 27.86M.\n\nProgram vmem requirement 91.86M:\n    scoped           91.86M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 27.61M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 96.12M of 64.00M vmem. Exceeded vmem capacity by 32.12M.\n\nProgram vmem requirement 96.12M:\n    scoped           96.12M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.62M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 104.61M of 64.00M vmem. Exceeded vmem capacity by 40.61M.\n\nProgram vmem requirement 104.61M:\n    scoped          104.61M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 39.61M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 121.91M of 64.00M vmem. Exceeded vmem capacity by 57.91M.\n\nProgram vmem requirement 121.91M:\n    scoped          121.91M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.91M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 156.24M of 64.00M vmem. Exceeded vmem capacity by 92.24M.\n\nProgram vmem requirement 156.24M:\n    scoped          156.24M\n\n  Largest program allocations in vmem:\n\n  1. Size: 88.24M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 225.21M of 64.00M vmem. Exceeded vmem capacity by 161.21M.\n\nProgram vmem requirement 225.21M:\n    scoped          225.21M\n\n  Largest program allocations in vmem:\n\n  1. Size: 153.21M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune28 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=2, head_dim=256, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune29 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=2, head_dim=256, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (32, 32)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 8, 2, 256, 131072)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=16, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_256-bkvp_16-p_256.1 = (bf16[2,1024,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}, bf16[1000,256,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 63.00M and limit 60.00M exceeded scoped vmem limit by 3.00M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_64-bkvp_32-p_256.1 = (bf16[2,1024,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}, bf16[1000,256,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.88M and limit 60.00M exceeded scoped vmem limit by 1.88M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 79.95M of 64.00M vmem. Exceeded vmem capacity by 15.95M.\n\nProgram vmem requirement 79.95M:\n    scoped           79.95M\n\n  Largest program allocations in vmem:\n\n  1. Size: 43.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 117.00M of 64.00M vmem. Exceeded vmem capacity by 53.00M.\n\nProgram vmem requirement 117.00M:\n    scoped          117.00M\n\n  Largest program allocations in vmem:\n\n  1. Size: 77.00M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 91.86M of 64.00M vmem. Exceeded vmem capacity by 27.86M.\n\nProgram vmem requirement 91.86M:\n    scoped           91.86M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 27.61M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 96.12M of 64.00M vmem. Exceeded vmem capacity by 32.12M.\n\nProgram vmem requirement 96.12M:\n    scoped           96.12M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.62M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 104.61M of 64.00M vmem. Exceeded vmem capacity by 40.61M.\n\nProgram vmem requirement 104.61M:\n    scoped          104.61M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 39.61M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 121.91M of 64.00M vmem. Exceeded vmem capacity by 57.91M.\n\nProgram vmem requirement 121.91M:\n    scoped          121.91M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.91M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 156.24M of 64.00M vmem. Exceeded vmem capacity by 92.24M.\n\nProgram vmem requirement 156.24M:\n    scoped          156.24M\n\n  Largest program allocations in vmem:\n\n  1. Size: 88.24M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 225.21M of 64.00M vmem. Exceeded vmem capacity by 161.21M.\n\nProgram vmem requirement 225.21M:\n    scoped          225.21M\n\n  Largest program allocations in vmem:\n\n  1. Size: 153.21M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")[       OK ] Autotune.test_autotune29 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=2, head_dim=256, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune3 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=2, num_kv_heads=2, head_dim=256, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (32, 32)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 4, 2, 256, 16384)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_128-bkvp_32-p_256.1 = (bf16[2,1024,1,2,256]{4,3,2,1,0:T(2,128)(2,1)}, bf16[1000,256,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}) custom-call(%copy-done.4, %copy-done.2, %copy-done.3, %copy-done.5, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.88M and limit 60.00M exceeded scoped vmem limit by 1.88M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 79.95M of 64.00M vmem. Exceeded vmem capacity by 15.95M.\n\nProgram vmem requirement 79.95M:\n    scoped           79.95M\n\n  Largest program allocations in vmem:\n\n  1. Size: 43.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 89.76M of 64.00M vmem. Exceeded vmem capacity by 25.76M.\n\nProgram vmem requirement 89.76M:\n    scoped           89.76M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 25.63M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,256]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 91.86M of 64.00M vmem. Exceeded vmem capacity by 27.86M.\n\nProgram vmem requirement 91.86M:\n    scoped           91.86M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 27.61M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 96.12M of 64.00M vmem. Exceeded vmem capacity by 32.12M.\n\nProgram vmem requirement 96.12M:\n    scoped           96.12M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.62M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 104.61M of 64.00M vmem. Exceeded vmem capacity by 40.61M.\n\nProgram vmem requirement 104.61M:\n    scoped          104.61M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 39.61M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 121.91M of 64.00M vmem. Exceeded vmem capacity by 57.91M.\n\nProgram vmem requirement 121.91M:\n    scoped          121.91M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.91M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 156.24M of 64.00M vmem. Exceeded vmem capacity by 92.24M.\n\nProgram vmem requirement 156.24M:\n    scoped          156.24M\n\n  Largest program allocations in vmem:\n\n  1. Size: 88.24M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune3 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=2, num_kv_heads=2, head_dim=256, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune30 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=4, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Skip (page_size=256, num_kv_pages_per_block=128) because num_kv_pages_per_block=128 > pages_per_seq=64
[Debug] Skip (page_size=256, num_kv_pages_per_block=256) because num_kv_pages_per_block=256 > pages_per_seq=64
Best block size: (32, 64)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 8, 4, 128, 16384)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 74.09M of 64.00M vmem. Exceeded vmem capacity by 10.09M.\n\nProgram vmem requirement 74.09M:\n    scoped           74.09M\n\n  Largest program allocations in vmem:\n\n  1. Size: 39.59M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 108.19M of 64.00M vmem. Exceeded vmem capacity by 44.19M.\n\nProgram vmem requirement 108.19M:\n    scoped          108.19M\n\n  Largest program allocations in vmem:\n\n  1. Size: 71.19M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 82.95M of 64.00M vmem. Exceeded vmem capacity by 18.95M.\n\nProgram vmem requirement 82.95M:\n    scoped           82.95M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 18.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 88.12M of 64.00M vmem. Exceeded vmem capacity by 24.12M.\n\nProgram vmem requirement 88.12M:\n    scoped           88.12M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 23.81M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 96.31M of 64.00M vmem. Exceeded vmem capacity by 32.31M.\n\nProgram vmem requirement 96.31M:\n    scoped           96.31M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.68M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 112.94M of 64.00M vmem. Exceeded vmem capacity by 48.94M.\n\nProgram vmem requirement 112.94M:\n    scoped          112.94M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 47.69M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 146.33M of 64.00M vmem. Exceeded vmem capacity by 82.33M.\n\nProgram vmem requirement 146.33M:\n    scoped          146.33M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.83M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 212.59M of 64.00M vmem. Exceeded vmem capacity by 148.59M.\n\nProgram vmem requirement 212.59M:\n    scoped          212.59M\n\n  Largest program allocations in vmem:\n\n  1. Size: 143.59M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune30 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=4, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune31 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=4, head_dim=128, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Skip (page_size=256, num_kv_pages_per_block=128) because num_kv_pages_per_block=128 > pages_per_seq=64
[Debug] Skip (page_size=256, num_kv_pages_per_block=256) because num_kv_pages_per_block=256 > pages_per_seq=64
Best block size: (32, 64)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 8, 4, 128, 65536)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 74.09M of 64.00M vmem. Exceeded vmem capacity by 10.09M.\n\nProgram vmem requirement 74.09M:\n    scoped           74.09M\n\n  Largest program allocations in vmem:\n\n  1. Size: 39.59M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 108.18M of 64.00M vmem. Exceeded vmem capacity by 44.18M.\n\nProgram vmem requirement 108.18M:\n    scoped          108.18M\n\n  Largest program allocations in vmem:\n\n  1. Size: 71.18M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 82.95M of 64.00M vmem. Exceeded vmem capacity by 18.95M.\n\nProgram vmem requirement 82.95M:\n    scoped           82.95M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 18.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 88.12M of 64.00M vmem. Exceeded vmem capacity by 24.12M.\n\nProgram vmem requirement 88.12M:\n    scoped           88.12M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 23.81M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 96.31M of 64.00M vmem. Exceeded vmem capacity by 32.31M.\n\nProgram vmem requirement 96.31M:\n    scoped           96.31M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.68M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 112.94M of 64.00M vmem. Exceeded vmem capacity by 48.94M.\n\nProgram vmem requirement 112.94M:\n    scoped          112.94M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 47.69M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 146.33M of 64.00M vmem. Exceeded vmem capacity by 82.33M.\n\nProgram vmem requirement 146.33M:\n    scoped          146.33M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.83M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 212.59M of 64.00M vmem. Exceeded vmem capacity by 148.59M.\n\nProgram vmem requirement 212.59M:\n    scoped          212.59M\n\n  Largest program allocations in vmem:\n\n  1. Size: 143.59M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune31 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=4, head_dim=128, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune32 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=4, head_dim=128, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (32, 64)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 8, 4, 128, 131072)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 74.09M of 64.00M vmem. Exceeded vmem capacity by 10.09M.\n\nProgram vmem requirement 74.09M:\n    scoped           74.09M\n\n  Largest program allocations in vmem:\n\n  1. Size: 39.59M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 108.18M of 64.00M vmem. Exceeded vmem capacity by 44.18M.\n\nProgram vmem requirement 108.18M:\n    scoped          108.18M\n\n  Largest program allocations in vmem:\n\n  1. Size: 71.18M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 82.95M of 64.00M vmem. Exceeded vmem capacity by 18.95M.\n\nProgram vmem requirement 82.95M:\n    scoped           82.95M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 18.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 88.12M of 64.00M vmem. Exceeded vmem capacity by 24.12M.\n\nProgram vmem requirement 88.12M:\n    scoped           88.12M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 23.81M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 96.31M of 64.00M vmem. Exceeded vmem capacity by 32.31M.\n\nProgram vmem requirement 96.31M:\n    scoped           96.31M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.68M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 112.95M of 64.00M vmem. Exceeded vmem capacity by 48.95M.\n\nProgram vmem requirement 112.95M:\n    scoped          112.95M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 47.70M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 146.33M of 64.00M vmem. Exceeded vmem capacity by 82.33M.\n\nProgram vmem requirement 146.33M:\n    scoped          146.33M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.83M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 212.59M of 64.00M vmem. Exceeded vmem capacity by 148.59M.\n\nProgram vmem requirement 212.59M:\n    scoped          212.59M\n\n  Largest program allocations in vmem:\n\n  1. Size: 143.59M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, bf16[1024,4,2,128]{3,2,1,0}, bf16[1000,256,4,2,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")[       OK ] Autotune.test_autotune32 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=4, head_dim=128, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune33 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=4, head_dim=256, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,128]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (32, 64)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 8, 4, 256, 16384)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=16, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 78.05M of 64.00M vmem. Exceeded vmem capacity by 14.05M.\n\nProgram vmem requirement 78.05M:\n    scoped           78.05M\n\n  Largest program allocations in vmem:\n\n  1. Size: 38.05M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_16-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4096,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 78.05M of 64.00M vmem. Exceeded vmem capacity by 14.05M.\n\nProgram vmem requirement 78.05M:\n    scoped           78.05M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 13.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 80.31M of 64.00M vmem. Exceeded vmem capacity by 16.31M.\n\nProgram vmem requirement 80.31M:\n    scoped           80.31M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 15.81M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 84.77M of 64.00M vmem. Exceeded vmem capacity by 20.77M.\n\nProgram vmem requirement 84.77M:\n    scoped           84.77M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 19.77M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 93.84M of 64.00M vmem. Exceeded vmem capacity by 29.84M.\n\nProgram vmem requirement 93.84M:\n    scoped           93.84M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 27.84M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 111.91M of 64.00M vmem. Exceeded vmem capacity by 47.91M.\n\nProgram vmem requirement 111.91M:\n    scoped          111.91M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 43.91M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 148.07M of 64.00M vmem. Exceeded vmem capacity by 84.07M.\n\nProgram vmem requirement 148.07M:\n    scoped          148.07M\n\n  Largest program allocations in vmem:\n\n  1. Size: 76.07M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_64-p_256.1")
[Debug] Skip (page_size=256, num_kv_pages_per_block=128) because num_kv_pages_per_block=128 > pages_per_seq=64
[Debug] Skip (page_size=256, num_kv_pages_per_block=256) because num_kv_pages_per_block=256 > pages_per_seq=64[       OK ] Autotune.test_autotune33 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=4, head_dim=256, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune34 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=4, head_dim=256, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

Best block size: (16, 128)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 8, 4, 256, 65536)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=16, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 78.05M of 64.00M vmem. Exceeded vmem capacity by 14.05M.\n\nProgram vmem requirement 78.05M:\n    scoped           78.05M\n\n  Largest program allocations in vmem:\n\n  1. Size: 38.05M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_16-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4096,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 78.05M of 64.00M vmem. Exceeded vmem capacity by 14.05M.\n\nProgram vmem requirement 78.05M:\n    scoped           78.05M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 13.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 80.31M of 64.00M vmem. Exceeded vmem capacity by 16.31M.\n\nProgram vmem requirement 80.31M:\n    scoped           80.31M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 15.81M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 84.77M of 64.00M vmem. Exceeded vmem capacity by 20.77M.\n\nProgram vmem requirement 84.77M:\n    scoped           84.77M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 19.77M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 93.84M of 64.00M vmem. Exceeded vmem capacity by 29.84M.\n\nProgram vmem requirement 93.84M:\n    scoped           93.84M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 27.84M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 111.91M of 64.00M vmem. Exceeded vmem capacity by 47.91M.\n\nProgram vmem requirement 111.91M:\n    scoped          111.91M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 43.91M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 148.07M of 64.00M vmem. Exceeded vmem capacity by 84.07M.\n\nProgram vmem requirement 148.07M:\n    scoped          148.07M\n\n  Largest program allocations in vmem:\n\n  1. Size: 76.07M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_128-p_256.1")[       OK ] Autotune.test_autotune34 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=4, head_dim=256, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune35 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=4, head_dim=256, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (16, 128)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 8, 4, 256, 131072)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=16, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 78.05M of 64.00M vmem. Exceeded vmem capacity by 14.05M.\n\nProgram vmem requirement 78.05M:\n    scoped           78.05M\n\n  Largest program allocations in vmem:\n\n  1. Size: 38.05M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_16-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4096,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_16-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 78.05M of 64.00M vmem. Exceeded vmem capacity by 14.05M.\n\nProgram vmem requirement 78.05M:\n    scoped           78.05M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 13.80M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 80.31M of 64.00M vmem. Exceeded vmem capacity by 16.31M.\n\nProgram vmem requirement 80.31M:\n    scoped           80.31M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 15.81M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 84.77M of 64.00M vmem. Exceeded vmem capacity by 20.77M.\n\nProgram vmem requirement 84.77M:\n    scoped           84.77M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 19.77M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 93.84M of 64.00M vmem. Exceeded vmem capacity by 29.84M.\n\nProgram vmem requirement 93.84M:\n    scoped           93.84M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 27.84M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 111.90M of 64.00M vmem. Exceeded vmem capacity by 47.90M.\n\nProgram vmem requirement 111.90M:\n    scoped          111.90M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 43.90M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 148.07M of 64.00M vmem. Exceeded vmem capacity by 84.07M.\n\nProgram vmem requirement 148.07M:\n    scoped          148.07M\n\n  Largest program allocations in vmem:\n\n  1. Size: 76.07M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, bf16[1024,4,2,256]{3,2,1,0}, bf16[1000,256,4,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune35 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=8, num_kv_heads=4, head_dim=256, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune36 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=2, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,16384,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_64-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=536870912) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,4,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x20000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (16, 128)
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 4, 2, 128, 16384)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_128-bkvp_64-p_256.1 = (bf16[2,1024,1,2,128]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%kv_lens.1, %page_indices.1, %cu_q_lens.1, %distribution.1, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %copy.7, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.17M and limit 60.00M exceeded scoped vmem limit by 1.17M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 94.45M of 64.00M vmem. Exceeded vmem capacity by 30.45M.\n\nProgram vmem requirement 94.45M:\n    scoped           94.45M\n\n  Largest program allocations in vmem:\n\n  1. Size: 75.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune36 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=2, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune37 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=2, head_dim=128, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Skip (page_size=256, num_kv_pages_per_block=128) because num_kv_pages_per_block=128 > pages_per_seq=64
[Debug] Skip (page_size=256, num_kv_pages_per_block=256) because num_kv_pages_per_block=256 > pages_per_seq=64
Best block size: (64, 8)
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 4, 2, 128, 65536)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_128-bkvp_64-p_256.1 = (bf16[2,1024,1,2,128]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%kv_lens.1, %page_indices.1, %cu_q_lens.1, %distribution.1, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %copy.7, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.17M and limit 60.00M exceeded scoped vmem limit by 1.17M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 94.45M of 64.00M vmem. Exceeded vmem capacity by 30.45M.\n\nProgram vmem requirement 94.45M:\n    scoped           94.45M\n\n  Largest program allocations in vmem:\n\n  1. Size: 75.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_8-bkvp_128-p_256.1 = (bf16[2,1024,1,2,128]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%kv_lens.1, %page_indices.1, %cu_q_lens.1, %distribution.1, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %copy.7, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 60.06M and limit 60.00M exceeded scoped vmem limit by 64.0K. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.04M of 64.00M vmem. Exceeded vmem capacity by 40.0K.\n\nProgram vmem requirement 64.04M:\n    scoped           64.04M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 72.18M of 64.00M vmem. Exceeded vmem capacity by 8.18M.\n\nProgram vmem requirement 72.18M:\n    scoped           72.18M\n\n  Largest program allocations in vmem:\n\n  1. Size: 39.87M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 88.52M of 64.00M vmem. Exceeded vmem capacity by 24.52M.\n\nProgram vmem requirement 88.52M:\n    scoped           88.52M\n\n  Largest program allocations in vmem:\n\n  1. Size: 55.90M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 121.23M of 64.00M vmem. Exceeded vmem capacity by 57.23M.\n\nProgram vmem requirement 121.23M:\n    scoped          121.23M\n\n  Largest program allocations in vmem:\n\n  1. Size: 87.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 197.80M of 64.00M vmem. Exceeded vmem capacity by 133.80M.\n\nProgram vmem requirement 197.80M:\n    scoped          197.80M\n\n  Largest program allocations in vmem:\n\n  1. Size: 163.30M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 120.06M of 64.00M vmem. Exceeded vmem capacity by 56.06M.\n\nProgram vmem requirement 120.06M:\n    scoped          120.06M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 128.02M of 64.00M vmem. Exceeded vmem capacity by 64.02M.\n\nProgram vmem requirement 128.02M:\n    scoped          128.02M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 63.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 144.19M of 64.00M vmem. Exceeded vmem capacity by 80.19M.\n\nProgram vmem requirement 144.19M:\n    scoped          144.19M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 176.52M of 64.00M vmem. Exceeded vmem capacity by 112.52M.\n\nProgram vmem requirement 176.52M:\n    scoped          176.52M\n\n  Largest program allocations in vmem:\n\n  1. Size: 111.90M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 241.23M of 64.00M vmem. Exceeded vmem capacity by 177.23M.\n\nProgram vmem requirement 241.23M:\n    scoped          241.23M\n\n  Largest program allocations in vmem:\n\n  1. Size: 175.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune37 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=2, head_dim=128, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune38 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=2, head_dim=128, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 414.99M of 64.00M vmem. Exceeded vmem capacity by 350.99M.\n\nProgram vmem requirement 414.99M:\n    scoped          414.99M\n\n  Largest program allocations in vmem:\n\n  1. Size: 348.49M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
Best block size: (64, 64)
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 4, 2, 128, 131072)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_128-bkvp_64-p_256.1 = (bf16[2,1024,1,2,128]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%kv_lens.1, %page_indices.1, %cu_q_lens.1, %distribution.1, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %copy.7, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.17M and limit 60.00M exceeded scoped vmem limit by 1.17M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 94.45M of 64.00M vmem. Exceeded vmem capacity by 30.45M.\n\nProgram vmem requirement 94.45M:\n    scoped           94.45M\n\n  Largest program allocations in vmem:\n\n  1. Size: 75.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_8-bkvp_128-p_256.1 = (bf16[2,1024,1,2,128]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%kv_lens.1, %page_indices.1, %cu_q_lens.1, %distribution.1, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %copy.7, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 60.06M and limit 60.00M exceeded scoped vmem limit by 64.0K. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.04M of 64.00M vmem. Exceeded vmem capacity by 40.0K.\n\nProgram vmem requirement 64.04M:\n    scoped           64.04M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 72.18M of 64.00M vmem. Exceeded vmem capacity by 8.18M.\n\nProgram vmem requirement 72.18M:\n    scoped           72.18M\n\n  Largest program allocations in vmem:\n\n  1. Size: 39.87M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 88.52M of 64.00M vmem. Exceeded vmem capacity by 24.52M.\n\nProgram vmem requirement 88.52M:\n    scoped           88.52M\n\n  Largest program allocations in vmem:\n\n  1. Size: 55.90M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 121.23M of 64.00M vmem. Exceeded vmem capacity by 57.23M.\n\nProgram vmem requirement 121.23M:\n    scoped          121.23M\n\n  Largest program allocations in vmem:\n\n  1. Size: 87.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 197.80M of 64.00M vmem. Exceeded vmem capacity by 133.80M.\n\nProgram vmem requirement 197.80M:\n    scoped          197.80M\n\n  Largest program allocations in vmem:\n\n  1. Size: 163.30M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 120.06M of 64.00M vmem. Exceeded vmem capacity by 56.06M.\n\nProgram vmem requirement 120.06M:\n    scoped          120.06M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 128.02M of 64.00M vmem. Exceeded vmem capacity by 64.02M.\n\nProgram vmem requirement 128.02M:\n    scoped          128.02M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 63.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 144.19M of 64.00M vmem. Exceeded vmem capacity by 80.19M.\n\nProgram vmem requirement 144.19M:\n    scoped          144.19M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 176.52M of 64.00M vmem. Exceeded vmem capacity by 112.52M.\n\nProgram vmem requirement 176.52M:\n    scoped          176.52M\n\n  Largest program allocations in vmem:\n\n  1. Size: 111.90M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 241.23M of 64.00M vmem. Exceeded vmem capacity by 177.23M.\n\nProgram vmem requirement 241.23M:\n    scoped          241.23M\n\n  Largest program allocations in vmem:\n\n  1. Size: 175.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune38 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=2, head_dim=128, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune39 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=2, head_dim=256, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 414.99M of 64.00M vmem. Exceeded vmem capacity by 350.99M.\n\nProgram vmem requirement 414.99M:\n    scoped          414.99M\n\n  Largest program allocations in vmem:\n\n  1. Size: 348.49M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
Best block size: (64, 64)
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 4, 2, 256, 16384)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.29M of 64.00M vmem. Exceeded vmem capacity by 292.0K.\n\nProgram vmem requirement 64.29M:\n    scoped           64.29M\n\n  Largest program allocations in vmem:\n\n  1. Size: 44.29M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,8192,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_16-bkvp_64-p_256.1 = (bf16[2,1024,1,2,256]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %copy.7, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 60.06M and limit 60.00M exceeded scoped vmem limit by 64.0K. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.27M of 64.00M vmem. Exceeded vmem capacity by 280.0K.\n\nProgram vmem requirement 64.27M:\n    scoped           64.27M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.77M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 72.85M of 64.00M vmem. Exceeded vmem capacity by 8.85M.\n\nProgram vmem requirement 72.85M:\n    scoped           72.85M\n\n  Largest program allocations in vmem:\n\n  1. Size: 39.85M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 90.10M of 64.00M vmem. Exceeded vmem capacity by 26.10M.\n\nProgram vmem requirement 90.10M:\n    scoped           90.10M\n\n  Largest program allocations in vmem:\n\n  1. Size: 56.10M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 124.62M of 64.00M vmem. Exceeded vmem capacity by 60.62M.\n\nProgram vmem requirement 124.62M:\n    scoped          124.62M\n\n  Largest program allocations in vmem:\n\n  1. Size: 88.62M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune39 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=2, head_dim=256, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune4 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=2, num_kv_heads=2, head_dim=256, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Skip (page_size=256, num_kv_pages_per_block=128) because num_kv_pages_per_block=128 > pages_per_seq=64
[Debug] Skip (page_size=256, num_kv_pages_per_block=256) because num_kv_pages_per_block=256 > pages_per_seq=64
Best block size: (32, 128)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 4, 2, 256, 65536)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_128-bkvp_32-p_256.1 = (bf16[2,1024,1,2,256]{4,3,2,1,0:T(2,128)(2,1)}, bf16[1000,256,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}) custom-call(%copy-done.4, %copy-done.2, %copy-done.3, %copy-done.5, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.88M and limit 60.00M exceeded scoped vmem limit by 1.88M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 79.95M of 64.00M vmem. Exceeded vmem capacity by 15.95M.\n\nProgram vmem requirement 79.95M:\n    scoped           79.95M\n\n  Largest program allocations in vmem:\n\n  1. Size: 43.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 89.76M of 64.00M vmem. Exceeded vmem capacity by 25.76M.\n\nProgram vmem requirement 89.76M:\n    scoped           89.76M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 25.63M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,256]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 91.86M of 64.00M vmem. Exceeded vmem capacity by 27.86M.\n\nProgram vmem requirement 91.86M:\n    scoped           91.86M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 27.61M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 96.12M of 64.00M vmem. Exceeded vmem capacity by 32.12M.\n\nProgram vmem requirement 96.12M:\n    scoped           96.12M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.62M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 104.61M of 64.00M vmem. Exceeded vmem capacity by 40.61M.\n\nProgram vmem requirement 104.61M:\n    scoped          104.61M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 39.61M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 121.91M of 64.00M vmem. Exceeded vmem capacity by 57.91M.\n\nProgram vmem requirement 121.91M:\n    scoped          121.91M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.91M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 156.24M of 64.00M vmem. Exceeded vmem capacity by 92.24M.\n\nProgram vmem requirement 156.24M:\n    scoped          156.24M\n\n  Largest program allocations in vmem:\n\n  1. Size: 88.24M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")[       OK ] Autotune.test_autotune4 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=2, num_kv_heads=2, head_dim=256, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune40 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=2, head_dim=256, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (32, 64)
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 4, 2, 256, 65536)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.29M of 64.00M vmem. Exceeded vmem capacity by 292.0K.\n\nProgram vmem requirement 64.29M:\n    scoped           64.29M\n\n  Largest program allocations in vmem:\n\n  1. Size: 44.29M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,8192,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_16-bkvp_64-p_256.1 = (bf16[2,1024,1,2,256]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %copy.7, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 60.06M and limit 60.00M exceeded scoped vmem limit by 64.0K. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.27M of 64.00M vmem. Exceeded vmem capacity by 280.0K.\n\nProgram vmem requirement 64.27M:\n    scoped           64.27M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.77M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 72.85M of 64.00M vmem. Exceeded vmem capacity by 8.85M.\n\nProgram vmem requirement 72.85M:\n    scoped           72.85M\n\n  Largest program allocations in vmem:\n\n  1. Size: 39.85M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 90.10M of 64.00M vmem. Exceeded vmem capacity by 26.10M.\n\nProgram vmem requirement 90.10M:\n    scoped           90.10M\n\n  Largest program allocations in vmem:\n\n  1. Size: 56.10M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 124.62M of 64.00M vmem. Exceeded vmem capacity by 60.62M.\n\nProgram vmem requirement 124.62M:\n    scoped          124.62M\n\n  Largest program allocations in vmem:\n\n  1. Size: 88.62M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 115.99M of 64.00M vmem. Exceeded vmem capacity by 51.99M.\n\nProgram vmem requirement 115.99M:\n    scoped          115.99M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 51.87M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,256]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 120.11M of 64.00M vmem. Exceeded vmem capacity by 56.11M.\n\nProgram vmem requirement 120.11M:\n    scoped          120.11M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 128.27M of 64.00M vmem. Exceeded vmem capacity by 64.27M.\n\nProgram vmem requirement 128.27M:\n    scoped          128.27M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 63.77M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 144.85M of 64.00M vmem. Exceeded vmem capacity by 80.85M.\n\nProgram vmem requirement 144.85M:\n    scoped          144.85M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.85M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 178.23M of 64.00M vmem. Exceeded vmem capacity by 114.23M.\n\nProgram vmem requirement 178.23M:\n    scoped          178.23M\n\n  Largest program allocations in vmem:\n\n  1. Size: 112.23M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 246.07M of 64.00M vmem. Exceeded vmem capacity by 182.07M.\n\nProgram vmem requirement 246.07M:\n    scoped          246.07M\n\n  Largest program allocations in vmem:\n\n  1. Size: 178.07M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune40 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=2, head_dim=256, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune41 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=2, head_dim=256, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (32, 128)
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 4, 2, 256, 131072)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.29M of 64.00M vmem. Exceeded vmem capacity by 292.0K.\n\nProgram vmem requirement 64.29M:\n    scoped           64.29M\n\n  Largest program allocations in vmem:\n\n  1. Size: 44.29M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,8192,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_16-bkvp_64-p_256.1 = (bf16[2,1024,1,2,256]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %copy.7, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 60.06M and limit 60.00M exceeded scoped vmem limit by 64.0K. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.27M of 64.00M vmem. Exceeded vmem capacity by 280.0K.\n\nProgram vmem requirement 64.27M:\n    scoped           64.27M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.77M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 72.85M of 64.00M vmem. Exceeded vmem capacity by 8.85M.\n\nProgram vmem requirement 72.85M:\n    scoped           72.85M\n\n  Largest program allocations in vmem:\n\n  1. Size: 39.85M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 90.10M of 64.00M vmem. Exceeded vmem capacity by 26.10M.\n\nProgram vmem requirement 90.10M:\n    scoped           90.10M\n\n  Largest program allocations in vmem:\n\n  1. Size: 56.10M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 124.62M of 64.00M vmem. Exceeded vmem capacity by 60.62M.\n\nProgram vmem requirement 124.62M:\n    scoped          124.62M\n\n  Largest program allocations in vmem:\n\n  1. Size: 88.62M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 115.99M of 64.00M vmem. Exceeded vmem capacity by 51.99M.\n\nProgram vmem requirement 115.99M:\n    scoped          115.99M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 51.87M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,256]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 120.11M of 64.00M vmem. Exceeded vmem capacity by 56.11M.\n\nProgram vmem requirement 120.11M:\n    scoped          120.11M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 128.27M of 64.00M vmem. Exceeded vmem capacity by 64.27M.\n\nProgram vmem requirement 128.27M:\n    scoped          128.27M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 63.77M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 144.86M of 64.00M vmem. Exceeded vmem capacity by 80.86M.\n\nProgram vmem requirement 144.86M:\n    scoped          144.86M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 178.23M of 64.00M vmem. Exceeded vmem capacity by 114.23M.\n\nProgram vmem requirement 178.23M:\n    scoped          178.23M\n\n  Largest program allocations in vmem:\n\n  1. Size: 112.23M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 246.07M of 64.00M vmem. Exceeded vmem capacity by 182.07M.\n\nProgram vmem requirement 246.07M:\n    scoped          246.07M\n\n  Largest program allocations in vmem:\n\n  1. Size: 178.07M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune41 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=2, head_dim=256, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune42 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=4, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[       OK ] Autotune.test_autotune42 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=4, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune43 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=4, head_dim=128, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[       OK ] Autotune.test_autotune43 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=4, head_dim=128, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune44 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=4, head_dim=128, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[       OK ] Autotune.test_autotune44 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=4, head_dim=128, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune45 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=4, head_dim=256, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[       OK ] Autotune.test_autotune45 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=4, head_dim=256, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune46 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=4, head_dim=256, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[       OK ] Autotune.test_autotune46 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=4, head_dim=256, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune47 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=4, head_dim=256, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[       OK ] Autotune.test_autotune47 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=2, num_kv_heads=4, head_dim=256, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune48 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=2, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (32, 128)
Skip because num_q_heads % num_kv_heads != 0
Skip because num_q_heads % num_kv_heads != 0
Skip because num_q_heads % num_kv_heads != 0
Skip because num_q_heads % num_kv_heads != 0
Skip because num_q_heads % num_kv_heads != 0
Skip because num_q_heads % num_kv_heads != 0
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 4, 2, 128, 16384)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_128-bkvp_64-p_256.1 = (bf16[2,1024,1,2,128]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%kv_lens.1, %page_indices.1, %cu_q_lens.1, %distribution.1, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %copy.7, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.17M and limit 60.00M exceeded scoped vmem limit by 1.17M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 94.45M of 64.00M vmem. Exceeded vmem capacity by 30.45M.\n\nProgram vmem requirement 94.45M:\n    scoped           94.45M\n\n  Largest program allocations in vmem:\n\n  1. Size: 75.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune48 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=2, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune49 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=2, head_dim=128, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Skip (page_size=256, num_kv_pages_per_block=128) because num_kv_pages_per_block=128 > pages_per_seq=64
[Debug] Skip (page_size=256, num_kv_pages_per_block=256) because num_kv_pages_per_block=256 > pages_per_seq=64
Best block size: (64, 64)
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 4, 2, 128, 65536)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_128-bkvp_64-p_256.1 = (bf16[2,1024,1,2,128]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%kv_lens.1, %page_indices.1, %cu_q_lens.1, %distribution.1, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %copy.7, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.17M and limit 60.00M exceeded scoped vmem limit by 1.17M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 94.45M of 64.00M vmem. Exceeded vmem capacity by 30.45M.\n\nProgram vmem requirement 94.45M:\n    scoped           94.45M\n\n  Largest program allocations in vmem:\n\n  1. Size: 75.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_8-bkvp_128-p_256.1 = (bf16[2,1024,1,2,128]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%kv_lens.1, %page_indices.1, %cu_q_lens.1, %distribution.1, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %copy.7, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 60.06M and limit 60.00M exceeded scoped vmem limit by 64.0K. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.04M of 64.00M vmem. Exceeded vmem capacity by 40.0K.\n\nProgram vmem requirement 64.04M:\n    scoped           64.04M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 72.18M of 64.00M vmem. Exceeded vmem capacity by 8.18M.\n\nProgram vmem requirement 72.18M:\n    scoped           72.18M\n\n  Largest program allocations in vmem:\n\n  1. Size: 39.87M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 88.52M of 64.00M vmem. Exceeded vmem capacity by 24.52M.\n\nProgram vmem requirement 88.52M:\n    scoped           88.52M\n\n  Largest program allocations in vmem:\n\n  1. Size: 55.90M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 121.23M of 64.00M vmem. Exceeded vmem capacity by 57.23M.\n\nProgram vmem requirement 121.23M:\n    scoped          121.23M\n\n  Largest program allocations in vmem:\n\n  1. Size: 87.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 197.80M of 64.00M vmem. Exceeded vmem capacity by 133.80M.\n\nProgram vmem requirement 197.80M:\n    scoped          197.80M\n\n  Largest program allocations in vmem:\n\n  1. Size: 163.30M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 120.06M of 64.00M vmem. Exceeded vmem capacity by 56.06M.\n\nProgram vmem requirement 120.06M:\n    scoped          120.06M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 128.02M of 64.00M vmem. Exceeded vmem capacity by 64.02M.\n\nProgram vmem requirement 128.02M:\n    scoped          128.02M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 63.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 144.19M of 64.00M vmem. Exceeded vmem capacity by 80.19M.\n\nProgram vmem requirement 144.19M:\n    scoped          144.19M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 176.52M of 64.00M vmem. Exceeded vmem capacity by 112.52M.\n\nProgram vmem requirement 176.52M:\n    scoped          176.52M\n\n  Largest program allocations in vmem:\n\n  1. Size: 111.90M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 241.23M of 64.00M vmem. Exceeded vmem capacity by 177.23M.\n\nProgram vmem requirement 241.23M:\n    scoped          241.23M\n\n  Largest program allocations in vmem:\n\n  1. Size: 175.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune49 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=2, head_dim=128, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune5 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=2, num_kv_heads=2, head_dim=256, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 414.99M of 64.00M vmem. Exceeded vmem capacity by 350.99M.\n\nProgram vmem requirement 414.99M:\n    scoped          414.99M\n\n  Largest program allocations in vmem:\n\n  1. Size: 348.49M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
Best block size: (64, 64)
STARTING AUTOTUNE (256, 'bfloat16', 'bfloat16', 4, 2, 256, 131072)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_128-bkvp_32-p_256.1 = (bf16[2,1024,1,2,256]{4,3,2,1,0:T(2,128)(2,1)}, bf16[1000,256,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}) custom-call(%copy-done.4, %copy-done.2, %copy-done.3, %copy-done.5, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.88M and limit 60.00M exceeded scoped vmem limit by 1.88M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 79.95M of 64.00M vmem. Exceeded vmem capacity by 15.95M.\n\nProgram vmem requirement 79.95M:\n    scoped           79.95M\n\n  Largest program allocations in vmem:\n\n  1. Size: 43.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,8192,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 89.76M of 64.00M vmem. Exceeded vmem capacity by 25.76M.\n\nProgram vmem requirement 89.76M:\n    scoped           89.76M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 25.63M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,256]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 91.86M of 64.00M vmem. Exceeded vmem capacity by 27.86M.\n\nProgram vmem requirement 91.86M:\n    scoped           91.86M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 27.61M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 96.12M of 64.00M vmem. Exceeded vmem capacity by 32.12M.\n\nProgram vmem requirement 96.12M:\n    scoped           96.12M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.62M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 104.61M of 64.00M vmem. Exceeded vmem capacity by 40.61M.\n\nProgram vmem requirement 104.61M:\n    scoped          104.61M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 39.61M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 121.91M of 64.00M vmem. Exceeded vmem capacity by 57.91M.\n\nProgram vmem requirement 121.91M:\n    scoped          121.91M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.91M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 156.24M of 64.00M vmem. Exceeded vmem capacity by 92.24M.\n\nProgram vmem requirement 156.24M:\n    scoped          156.24M\n\n  Largest program allocations in vmem:\n\n  1. Size: 88.24M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,16384,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.4, copy-done.2, copy-done.3, copy-done.5, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, bf16[1024,2,2,256]{3,2,1,0}, bf16[1000,256,2,2,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune5 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=2, num_kv_heads=2, head_dim=256, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune50 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=2, head_dim=128, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,32768,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'bf16[2,65536,2,2,256]{4,3,2,1,0:T(2,128)(2,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (32, 64)
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 4, 2, 128, 131072)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_128-bkvp_64-p_256.1 = (bf16[2,1024,1,2,128]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%kv_lens.1, %page_indices.1, %cu_q_lens.1, %distribution.1, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %copy.7, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.17M and limit 60.00M exceeded scoped vmem limit by 1.17M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 94.45M of 64.00M vmem. Exceeded vmem capacity by 30.45M.\n\nProgram vmem requirement 94.45M:\n    scoped           94.45M\n\n  Largest program allocations in vmem:\n\n  1. Size: 75.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_8-bkvp_128-p_256.1 = (bf16[2,1024,1,2,128]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%kv_lens.1, %page_indices.1, %cu_q_lens.1, %distribution.1, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %copy.7, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 60.06M and limit 60.00M exceeded scoped vmem limit by 64.0K. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.04M of 64.00M vmem. Exceeded vmem capacity by 40.0K.\n\nProgram vmem requirement 64.04M:\n    scoped           64.04M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 72.18M of 64.00M vmem. Exceeded vmem capacity by 8.18M.\n\nProgram vmem requirement 72.18M:\n    scoped           72.18M\n\n  Largest program allocations in vmem:\n\n  1. Size: 39.87M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 88.52M of 64.00M vmem. Exceeded vmem capacity by 24.52M.\n\nProgram vmem requirement 88.52M:\n    scoped           88.52M\n\n  Largest program allocations in vmem:\n\n  1. Size: 55.90M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 121.23M of 64.00M vmem. Exceeded vmem capacity by 57.23M.\n\nProgram vmem requirement 121.23M:\n    scoped          121.23M\n\n  Largest program allocations in vmem:\n\n  1. Size: 87.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 197.80M of 64.00M vmem. Exceeded vmem capacity by 133.80M.\n\nProgram vmem requirement 197.80M:\n    scoped          197.80M\n\n  Largest program allocations in vmem:\n\n  1. Size: 163.30M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 120.06M of 64.00M vmem. Exceeded vmem capacity by 56.06M.\n\nProgram vmem requirement 120.06M:\n    scoped          120.06M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 128.02M of 64.00M vmem. Exceeded vmem capacity by 64.02M.\n\nProgram vmem requirement 128.02M:\n    scoped          128.02M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 63.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 144.19M of 64.00M vmem. Exceeded vmem capacity by 80.19M.\n\nProgram vmem requirement 144.19M:\n    scoped          144.19M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 176.52M of 64.00M vmem. Exceeded vmem capacity by 112.52M.\n\nProgram vmem requirement 176.52M:\n    scoped          176.52M\n\n  Largest program allocations in vmem:\n\n  1. Size: 111.90M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 241.23M of 64.00M vmem. Exceeded vmem capacity by 177.23M.\n\nProgram vmem requirement 241.23M:\n    scoped          241.23M\n\n  Largest program allocations in vmem:\n\n  1. Size: 175.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune50 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=2, head_dim=128, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune51 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=2, head_dim=256, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 414.99M of 64.00M vmem. Exceeded vmem capacity by 350.99M.\n\nProgram vmem requirement 414.99M:\n    scoped          414.99M\n\n  Largest program allocations in vmem:\n\n  1. Size: 348.49M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(kv_lens.1, page_indices.1, cu_q_lens.1, distribution.1, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
Best block size: (64, 64)
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 4, 2, 256, 16384)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.29M of 64.00M vmem. Exceeded vmem capacity by 292.0K.\n\nProgram vmem requirement 64.29M:\n    scoped           64.29M\n\n  Largest program allocations in vmem:\n\n  1. Size: 44.29M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,8192,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_16-bkvp_64-p_256.1 = (bf16[2,1024,1,2,256]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %copy.7, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 60.06M and limit 60.00M exceeded scoped vmem limit by 64.0K. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.27M of 64.00M vmem. Exceeded vmem capacity by 280.0K.\n\nProgram vmem requirement 64.27M:\n    scoped           64.27M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.77M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 72.85M of 64.00M vmem. Exceeded vmem capacity by 8.85M.\n\nProgram vmem requirement 72.85M:\n    scoped           72.85M\n\n  Largest program allocations in vmem:\n\n  1. Size: 39.85M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 90.10M of 64.00M vmem. Exceeded vmem capacity by 26.10M.\n\nProgram vmem requirement 90.10M:\n    scoped           90.10M\n\n  Largest program allocations in vmem:\n\n  1. Size: 56.10M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 124.62M of 64.00M vmem. Exceeded vmem capacity by 60.62M.\n\nProgram vmem requirement 124.62M:\n    scoped          124.62M\n\n  Largest program allocations in vmem:\n\n  1. Size: 88.62M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune51 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=2, head_dim=256, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune52 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=2, head_dim=256, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Skip (page_size=256, num_kv_pages_per_block=128) because num_kv_pages_per_block=128 > pages_per_seq=64
[Debug] Skip (page_size=256, num_kv_pages_per_block=256) because num_kv_pages_per_block=256 > pages_per_seq=64
Best block size: (32, 128)
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 4, 2, 256, 65536)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.29M of 64.00M vmem. Exceeded vmem capacity by 292.0K.\n\nProgram vmem requirement 64.29M:\n    scoped           64.29M\n\n  Largest program allocations in vmem:\n\n  1. Size: 44.29M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,8192,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_16-bkvp_64-p_256.1 = (bf16[2,1024,1,2,256]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %copy.7, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 60.06M and limit 60.00M exceeded scoped vmem limit by 64.0K. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.27M of 64.00M vmem. Exceeded vmem capacity by 280.0K.\n\nProgram vmem requirement 64.27M:\n    scoped           64.27M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.77M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 72.85M of 64.00M vmem. Exceeded vmem capacity by 8.85M.\n\nProgram vmem requirement 72.85M:\n    scoped           72.85M\n\n  Largest program allocations in vmem:\n\n  1. Size: 39.85M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 90.10M of 64.00M vmem. Exceeded vmem capacity by 26.10M.\n\nProgram vmem requirement 90.10M:\n    scoped           90.10M\n\n  Largest program allocations in vmem:\n\n  1. Size: 56.10M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 124.62M of 64.00M vmem. Exceeded vmem capacity by 60.62M.\n\nProgram vmem requirement 124.62M:\n    scoped          124.62M\n\n  Largest program allocations in vmem:\n\n  1. Size: 88.62M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 115.99M of 64.00M vmem. Exceeded vmem capacity by 51.99M.\n\nProgram vmem requirement 115.99M:\n    scoped          115.99M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 51.87M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,256]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 120.11M of 64.00M vmem. Exceeded vmem capacity by 56.11M.\n\nProgram vmem requirement 120.11M:\n    scoped          120.11M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 128.27M of 64.00M vmem. Exceeded vmem capacity by 64.27M.\n\nProgram vmem requirement 128.27M:\n    scoped          128.27M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 63.77M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 144.85M of 64.00M vmem. Exceeded vmem capacity by 80.85M.\n\nProgram vmem requirement 144.85M:\n    scoped          144.85M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.85M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 178.23M of 64.00M vmem. Exceeded vmem capacity by 114.23M.\n\nProgram vmem requirement 178.23M:\n    scoped          178.23M\n\n  Largest program allocations in vmem:\n\n  1. Size: 112.23M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 246.07M of 64.00M vmem. Exceeded vmem capacity by 182.07M.\n\nProgram vmem requirement 246.07M:\n    scoped          246.07M\n\n  Largest program allocations in vmem:\n\n  1. Size: 178.07M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size:[       OK ] Autotune.test_autotune52 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=2, head_dim=256, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune53 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=2, head_dim=256, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
 (32, 128)
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 4, 2, 256, 131072)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.29M of 64.00M vmem. Exceeded vmem capacity by 292.0K.\n\nProgram vmem requirement 64.29M:\n    scoped           64.29M\n\n  Largest program allocations in vmem:\n\n  1. Size: 44.29M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,8192,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_16-bkvp_64-p_256.1 = (bf16[2,1024,1,2,256]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %copy.7, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 60.06M and limit 60.00M exceeded scoped vmem limit by 64.0K. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.27M of 64.00M vmem. Exceeded vmem capacity by 280.0K.\n\nProgram vmem requirement 64.27M:\n    scoped           64.27M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.77M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 72.85M of 64.00M vmem. Exceeded vmem capacity by 8.85M.\n\nProgram vmem requirement 72.85M:\n    scoped           72.85M\n\n  Largest program allocations in vmem:\n\n  1. Size: 39.85M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 90.10M of 64.00M vmem. Exceeded vmem capacity by 26.10M.\n\nProgram vmem requirement 90.10M:\n    scoped           90.10M\n\n  Largest program allocations in vmem:\n\n  1. Size: 56.10M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 124.62M of 64.00M vmem. Exceeded vmem capacity by 60.62M.\n\nProgram vmem requirement 124.62M:\n    scoped          124.62M\n\n  Largest program allocations in vmem:\n\n  1. Size: 88.62M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 115.99M of 64.00M vmem. Exceeded vmem capacity by 51.99M.\n\nProgram vmem requirement 115.99M:\n    scoped          115.99M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 51.87M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,256]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 16.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 16.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 120.11M of 64.00M vmem. Exceeded vmem capacity by 56.11M.\n\nProgram vmem requirement 120.11M:\n    scoped          120.11M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 128.27M of 64.00M vmem. Exceeded vmem capacity by 64.27M.\n\nProgram vmem requirement 128.27M:\n    scoped          128.27M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 63.77M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 144.86M of 64.00M vmem. Exceeded vmem capacity by 80.86M.\n\nProgram vmem requirement 144.86M:\n    scoped          144.86M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 178.23M of 64.00M vmem. Exceeded vmem capacity by 114.23M.\n\nProgram vmem requirement 178.23M:\n    scoped          178.23M\n\n  Largest program allocations in vmem:\n\n  1. Size: 112.23M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 246.07M of 64.00M vmem. Exceeded vmem capacity by 182.07M.\n\nProgram vmem requirement 246.07M:\n    scoped          246.07M\n\n  Largest program allocations in vmem:\n\n  1. Size: 178.07M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,1,4,256]{3,2,1,0}, f8e4m3fn[1000,256,1,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,1,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size:[       OK ] Autotune.test_autotune53 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=2, head_dim=256, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune54 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=4, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
 (32, 128)
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 8, 4, 128, 16384)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 90.34M of 64.00M vmem. Exceeded vmem capacity by 26.34M.\n\nProgram vmem requirement 90.34M:\n    scoped           90.34M\n\n  Largest program allocations in vmem:\n\n  1. Size: 69.34M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,8192,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_32-bkvp_64-p_256.1 = (bf16[4,1024,1,2,128]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 60.52M and limit 60.00M exceeded scoped vmem limit by 528.0K. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 77.06M of 64.00M vmem. Exceeded vmem capacity by 13.06M.\n\nProgram vmem requirement 77.06M:\n    scoped           77.06M\n\n  Largest program allocations in vmem:\n\n  1. Size: 43.81M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 110.45M of 64.00M vmem. Exceeded vmem capacity by 46.45M.\n\nProgram vmem requirement 110.45M:\n    scoped          110.45M\n\n  Largest program allocations in vmem:\n\n  1. Size: 75.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 176.75M of 64.00M vmem. Exceeded vmem capacity by 112.75M.\n\nProgram vmem requirement 176.75M:\n    scoped          176.75M\n\n  Largest program allocations in vmem:\n\n  1. Size: 139.75M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune54 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=4, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune55 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=4, head_dim=128, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Skip (page_size=256, num_kv_pages_per_block=128) because num_kv_pages_per_block=128 > pages_per_seq=64
[Debug] Skip (page_size=256, num_kv_pages_per_block=256) because num_kv_pages_per_block=256 > pages_per_seq=64
Best block size: (64, 16)
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 8, 4, 128, 65536)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 90.34M of 64.00M vmem. Exceeded vmem capacity by 26.34M.\n\nProgram vmem requirement 90.34M:\n    scoped           90.34M\n\n  Largest program allocations in vmem:\n\n  1. Size: 69.34M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,8192,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_32-bkvp_64-p_256.1 = (bf16[4,1024,1,2,128]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 60.52M and limit 60.00M exceeded scoped vmem limit by 528.0K. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 77.06M of 64.00M vmem. Exceeded vmem capacity by 13.06M.\n\nProgram vmem requirement 77.06M:\n    scoped           77.06M\n\n  Largest program allocations in vmem:\n\n  1. Size: 43.81M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 110.45M of 64.00M vmem. Exceeded vmem capacity by 46.45M.\n\nProgram vmem requirement 110.45M:\n    scoped          110.45M\n\n  Largest program allocations in vmem:\n\n  1. Size: 75.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 176.75M of 64.00M vmem. Exceeded vmem capacity by 112.75M.\n\nProgram vmem requirement 176.75M:\n    scoped          176.75M\n\n  Largest program allocations in vmem:\n\n  1. Size: 139.75M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 108.16M of 64.00M vmem. Exceeded vmem capacity by 44.16M.\n\nProgram vmem requirement 108.16M:\n    scoped          108.16M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 44.00M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 112.26M of 64.00M vmem. Exceeded vmem capacity by 48.26M.\n\nProgram vmem requirement 112.26M:\n    scoped          112.26M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 47.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 120.54M of 64.00M vmem. Exceeded vmem capacity by 56.54M.\n\nProgram vmem requirement 120.54M:\n    scoped          120.54M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.91M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 153.29M of 64.00M vmem. Exceeded vmem capacity by 89.29M.\n\nProgram vmem requirement 153.29M:\n    scoped          153.29M\n\n  Largest program allocations in vmem:\n\n  1. Size: 88.04M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 218.93M of 64.00M vmem. Exceeded vmem capacity by 154.93M.\n\nProgram vmem requirement 218.93M:\n    scoped          218.93M\n\n  Largest program allocations in vmem:\n\n  1. Size: 152.43M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 350.33M of 64.00M vmem. Exceeded vmem capacity by 286.33M.\n\nProgram vmem requirement 350.33M:\n    scoped          350.33M\n\n  Largest program allocations in vmem:\n\n  1. Size: 281.33M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune55 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=4, head_dim=128, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune56 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=4, head_dim=128, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (64, 16)
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 8, 4, 128, 131072)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 90.34M of 64.00M vmem. Exceeded vmem capacity by 26.34M.\n\nProgram vmem requirement 90.34M:\n    scoped           90.34M\n\n  Largest program allocations in vmem:\n\n  1. Size: 69.34M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,8192,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_32-bkvp_64-p_256.1 = (bf16[4,1024,1,2,128]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 60.52M and limit 60.00M exceeded scoped vmem limit by 528.0K. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 77.07M of 64.00M vmem. Exceeded vmem capacity by 13.07M.\n\nProgram vmem requirement 77.07M:\n    scoped           77.07M\n\n  Largest program allocations in vmem:\n\n  1. Size: 43.82M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 110.45M of 64.00M vmem. Exceeded vmem capacity by 46.45M.\n\nProgram vmem requirement 110.45M:\n    scoped          110.45M\n\n  Largest program allocations in vmem:\n\n  1. Size: 75.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 176.75M of 64.00M vmem. Exceeded vmem capacity by 112.75M.\n\nProgram vmem requirement 176.75M:\n    scoped          176.75M\n\n  Largest program allocations in vmem:\n\n  1. Size: 139.75M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 108.16M of 64.00M vmem. Exceeded vmem capacity by 44.16M.\n\nProgram vmem requirement 108.16M:\n    scoped          108.16M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 44.00M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 112.26M of 64.00M vmem. Exceeded vmem capacity by 48.26M.\n\nProgram vmem requirement 112.26M:\n    scoped          112.26M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 47.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 120.54M of 64.00M vmem. Exceeded vmem capacity by 56.54M.\n\nProgram vmem requirement 120.54M:\n    scoped          120.54M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.91M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 153.29M of 64.00M vmem. Exceeded vmem capacity by 89.29M.\n\nProgram vmem requirement 153.29M:\n    scoped          153.29M\n\n  Largest program allocations in vmem:\n\n  1. Size: 88.04M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 218.93M of 64.00M vmem. Exceeded vmem capacity by 154.93M.\n\nProgram vmem requirement 218.93M:\n    scoped          218.93M\n\n  Largest program allocations in vmem:\n\n  1. Size: 152.43M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 350.33M of 64.00M vmem. Exceeded vmem capacity by 286.33M.\n\nProgram vmem requirement 350.33M:\n    scoped          350.33M\n\n  Largest program allocations in vmem:\n\n  1. Size: 281.33M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,128]{4,3,2,1,0}, f8e4m3fn[1024,2,4,128]{3,2,1,0}, f8e4m3fn[1000,256,2,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,128]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size:[       OK ] Autotune.test_autotune56 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=4, head_dim=128, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune57 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=4, head_dim=256, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
 (64, 16)
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 8, 4, 256, 16384)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=16, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_256-bkvp_16-p_256.1 = (bf16[4,1024,1,2,256]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 62.31M and limit 60.00M exceeded scoped vmem limit by 2.31M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_64-bkvp_32-p_256.1 = (bf16[4,1024,1,2,256]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.80M and limit 60.00M exceeded scoped vmem limit by 1.80M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 80.14M of 64.00M vmem. Exceeded vmem capacity by 16.14M.\n\nProgram vmem requirement 80.14M:\n    scoped           80.14M\n\n  Largest program allocations in vmem:\n\n  1. Size: 44.14M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,8192,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 116.32M of 64.00M vmem. Exceeded vmem capacity by 52.32M.\n\nProgram vmem requirement 116.32M:\n    scoped          116.32M\n\n  Largest program allocations in vmem:\n\n  1. Size: 76.32M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,8192,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 98.06M of 64.00M vmem. Exceeded vmem capacity by 34.06M.\n\nProgram vmem requirement 98.06M:\n    scoped           98.06M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 33.81M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 100.32M of 64.00M vmem. Exceeded vmem capacity by 36.32M.\n\nProgram vmem requirement 100.32M:\n    scoped          100.32M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 35.82M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 104.81M of 64.00M vmem. Exceeded vmem capacity by 40.81M.\n\nProgram vmem requirement 104.81M:\n    scoped          104.81M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 39.81M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 121.97M of 64.00M vmem. Exceeded vmem capacity by 57.97M.\n\nProgram vmem requirement 121.97M:\n    scoped          121.97M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.97M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 156.39M of 64.00M vmem. Exceeded vmem capacity by 92.39M.\n\nProgram vmem requirement 156.39M:\n    scoped          156.39M\n\n  Largest program allocations in vmem:\n\n  1. Size: 88.39M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune57 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=4, head_dim=256, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune58 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=4, head_dim=256, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 224.75M of 64.00M vmem. Exceeded vmem capacity by 160.75M.\n\nProgram vmem requirement 224.75M:\n    scoped          224.75M\n\n  Largest program allocations in vmem:\n\n  1. Size: 152.75M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Skip (page_size=256, num_kv_pages_per_block=128) because num_kv_pages_per_block=128 > pages_per_seq=64
[Debug] Skip (page_size=256, num_kv_pages_per_block=256) because num_kv_pages_per_block=256 > pages_per_seq=64
Best block size: (32, 32)
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 8, 4, 256, 65536)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=16, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_256-bkvp_16-p_256.1 = (bf16[4,1024,1,2,256]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 62.31M and limit 60.00M exceeded scoped vmem limit by 2.31M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_64-bkvp_32-p_256.1 = (bf16[4,1024,1,2,256]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.80M and limit 60.00M exceeded scoped vmem limit by 1.80M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 80.14M of 64.00M vmem. Exceeded vmem capacity by 16.14M.\n\nProgram vmem requirement 80.14M:\n    scoped           80.14M\n\n  Largest program allocations in vmem:\n\n  1. Size: 44.14M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,8192,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 116.32M of 64.00M vmem. Exceeded vmem capacity by 52.32M.\n\nProgram vmem requirement 116.32M:\n    scoped          116.32M\n\n  Largest program allocations in vmem:\n\n  1. Size: 76.32M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,8192,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 98.06M of 64.00M vmem. Exceeded vmem capacity by 34.06M.\n\nProgram vmem requirement 98.06M:\n    scoped           98.06M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 33.81M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 100.32M of 64.00M vmem. Exceeded vmem capacity by 36.32M.\n\nProgram vmem requirement 100.32M:\n    scoped          100.32M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 35.82M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 104.81M of 64.00M vmem. Exceeded vmem capacity by 40.81M.\n\nProgram vmem requirement 104.81M:\n    scoped          104.81M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 39.81M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 121.97M of 64.00M vmem. Exceeded vmem capacity by 57.97M.\n\nProgram vmem requirement 121.97M:\n    scoped          121.97M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.97M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 156.39M of 64.00M vmem. Exceeded vmem capacity by 92.39M.\n\nProgram vmem requirement 156.39M:\n    scoped          156.39M\n\n  Largest program allocations in vmem:\n\n  1. Size: 88.39M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 224.75M of 64.00M vmem. Exceeded vmem capacity by 160.75M.\n\nProgram vmem requirement 224.75M:\n    scoped          224.75M\n\n  Largest program allocations in vmem:\n\n  1. Size: 152.75M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune58 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=4, head_dim=256, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune59 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=4, head_dim=256, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,32768,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,32768,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,32768,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,32768,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,32768,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,32768,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (32, 32)
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 8, 4, 256, 131072)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=16, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_256-bkvp_16-p_256.1 = (bf16[4,1024,1,2,256]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_16-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 62.31M and limit 60.00M exceeded scoped vmem limit by 2.31M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_64-bkvp_32-p_256.1 = (bf16[4,1024,1,2,256]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %reshape.8, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.80M and limit 60.00M exceeded scoped vmem limit by 1.80M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 80.14M of 64.00M vmem. Exceeded vmem capacity by 16.14M.\n\nProgram vmem requirement 80.14M:\n    scoped           80.14M\n\n  Largest program allocations in vmem:\n\n  1. Size: 44.14M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,8192,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 116.32M of 64.00M vmem. Exceeded vmem capacity by 52.32M.\n\nProgram vmem requirement 116.32M:\n    scoped          116.32M\n\n  Largest program allocations in vmem:\n\n  1. Size: 76.32M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,8192,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 98.06M of 64.00M vmem. Exceeded vmem capacity by 34.06M.\n\nProgram vmem requirement 98.06M:\n    scoped           98.06M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 33.81M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,8,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,256]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,16,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 100.32M of 64.00M vmem. Exceeded vmem capacity by 36.32M.\n\nProgram vmem requirement 100.32M:\n    scoped          100.32M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 35.82M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,16,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,256]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 104.81M of 64.00M vmem. Exceeded vmem capacity by 40.81M.\n\nProgram vmem requirement 104.81M:\n    scoped          104.81M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 39.81M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,32,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,256]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 121.97M of 64.00M vmem. Exceeded vmem capacity by 57.97M.\n\nProgram vmem requirement 121.97M:\n    scoped          121.97M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 55.97M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,64,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,256]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 156.39M of 64.00M vmem. Exceeded vmem capacity by 92.39M.\n\nProgram vmem requirement 156.39M:\n    scoped          156.39M\n\n  Largest program allocations in vmem:\n\n  1. Size: 88.39M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,128,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,256]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 224.75M of 64.00M vmem. Exceeded vmem capacity by 160.75M.\n\nProgram vmem requirement 224.75M:\n    scoped          224.75M\n\n  Largest program allocations in vmem:\n\n  1. Size: 152.75M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,4,256,1,2,256]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 2.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,256]{2,1,0:T(8,128)}\n     Unpadded size: 2.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[4,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[4,1024,1,2,256]{4,3,2,1,0}, f8e4m3fn[1024,2,4,256]{3,2,1,0}, f8e4m3fn[1000,256,2,4,256]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,32768,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,32768,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,32768,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,32768,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,32768,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=134217728) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,32768,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x8000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_128-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_8-bkvp_256-p_256.1")[       OK ] Autotune.test_autotune59 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=4, num_kv_heads=4, head_dim=256, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune6 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=2, num_kv_heads=4, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[       OK ] Autotune.test_autotune6 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.bfloat16'>, num_q_heads=2, num_kv_heads=4, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune60 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=8, num_kv_heads=2, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_16-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_32-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_64-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_128-bkvp_256-p_256.1")
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError("RESOURCE_EXHAUSTED: Allocation (size=268435456) would exceed memory (size=67108864) :: #allocation2 [shape = 'f8e4m3fn[2,65536,2,4,256]{4,3,2,1,0:T(4,128)(4,1)}', space=vmem, size = 0x10000000, tag = 'scratch operand'] :: RPA-bq_256-bkvp_256-p_256.1")
Best block size: (32, 32)
Skip because num_q_heads % num_kv_heads != 0
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 8, 2, 128, 16384)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 85.46M of 64.00M vmem. Exceeded vmem capacity by 21.46M.\n\nProgram vmem requirement 85.46M:\n    scoped           85.46M\n\n  Largest program allocations in vmem:\n\n  1. Size: 72.46M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 8.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,8192,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 8.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_64-bkvp_64-p_256.1 = (bf16[2,1024,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %copy.7, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.17M and limit 60.00M exceeded scoped vmem limit by 1.17M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 94.45M of 64.00M vmem. Exceeded vmem capacity by 30.45M.\n\nProgram vmem requirement 94.45M:\n    scoped           94.45M\n\n  Largest program allocations in vmem:\n\n  1. Size: 75.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 168.34M of 64.00M vmem. Exceeded vmem capacity by 104.34M.\n\nProgram vmem requirement 168.34M:\n    scoped          168.34M\n\n  Largest program allocations in vmem:\n\n  1. Size: 147.34M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[16384]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune60 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=8, num_kv_heads=2, head_dim=128, max_model_len=16384, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune61 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=8, num_kv_heads=2, head_dim=128, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Skip (page_size=256, num_kv_pages_per_block=128) because num_kv_pages_per_block=128 > pages_per_seq=64
[Debug] Skip (page_size=256, num_kv_pages_per_block=256) because num_kv_pages_per_block=256 > pages_per_seq=64
Best block size: (64, 32)
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 8, 2, 128, 65536)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 85.45M of 64.00M vmem. Exceeded vmem capacity by 21.45M.\n\nProgram vmem requirement 85.45M:\n    scoped           85.45M\n\n  Largest program allocations in vmem:\n\n  1. Size: 72.45M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 8.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,8192,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 8.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_64-bkvp_64-p_256.1 = (bf16[2,1024,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %copy.7, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.17M and limit 60.00M exceeded scoped vmem limit by 1.17M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 94.45M of 64.00M vmem. Exceeded vmem capacity by 30.45M.\n\nProgram vmem requirement 94.45M:\n    scoped           94.45M\n\n  Largest program allocations in vmem:\n\n  1. Size: 75.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 168.34M of 64.00M vmem. Exceeded vmem capacity by 104.34M.\n\nProgram vmem requirement 168.34M:\n    scoped          168.34M\n\n  Largest program allocations in vmem:\n\n  1. Size: 147.34M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.04M of 64.00M vmem. Exceeded vmem capacity by 40.0K.\n\nProgram vmem requirement 64.04M:\n    scoped           64.04M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 72.18M of 64.00M vmem. Exceeded vmem capacity by 8.18M.\n\nProgram vmem requirement 72.18M:\n    scoped           72.18M\n\n  Largest program allocations in vmem:\n\n  1. Size: 39.87M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 88.52M of 64.00M vmem. Exceeded vmem capacity by 24.52M.\n\nProgram vmem requirement 88.52M:\n    scoped           88.52M\n\n  Largest program allocations in vmem:\n\n  1. Size: 55.90M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 121.23M of 64.00M vmem. Exceeded vmem capacity by 57.23M.\n\nProgram vmem requirement 121.23M:\n    scoped          121.23M\n\n  Largest program allocations in vmem:\n\n  1. Size: 87.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 197.80M of 64.00M vmem. Exceeded vmem capacity by 133.80M.\n\nProgram vmem requirement 197.80M:\n    scoped          197.80M\n\n  Largest program allocations in vmem:\n\n  1. Size: 163.30M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 354.61M of 64.00M vmem. Exceeded vmem capacity by 290.61M.\n\nProgram vmem requirement 354.61M:\n    scoped          354.61M\n\n  Largest program allocations in vmem:\n\n  1. Size: 317.61M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 128.02M of 64.00M vmem. Exceeded vmem capacity by 64.02M.\n\nProgram vmem requirement 128.02M:\n    scoped          128.02M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 63.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 144.19M of 64.00M vmem. Exceeded vmem capacity by 80.19M.\n\nProgram vmem requirement 144.19M:\n    scoped          144.19M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 176.52M of 64.00M vmem. Exceeded vmem capacity by 112.52M.\n\nProgram vmem requirement 176.52M:\n    scoped          176.52M\n\n  Largest program allocations in vmem:\n\n  1. Size: 111.90M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 241.23M of 64.00M vmem. Exceeded vmem capacity by 177.23M.\n\nProgram vmem requirement 241.23M:\n    scoped          241.23M\n\n  Largest program allocations in vmem:\n\n  1. Size: 175.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 414.99M of 64.00M vmem. Exceeded vmem capacity by 350.99M.\n\nProgram vmem requirement 414.99M:\n    scoped          414.99M\n\n  Largest program allocations in vmem:\n\n  1. Size: 348.49M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')[       OK ] Autotune.test_autotune61 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=8, num_kv_heads=2, head_dim=128, max_model_len=65536, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))
[ RUN      ] Autotune.test_autotune62 (page_size=256, q_dtype=<class 'jax.numpy.bfloat16'>, kv_dtype=<class 'jax.numpy.float8_e4m3fn'>, num_q_heads=8, num_kv_heads=2, head_dim=128, max_model_len=131072, max_num_tokens=1024, max_num_seqs=256, bkv_p_lst=(1, 2, 4, 8, 16, 32, 64, 128, 256), bq_sz_lst=(8, 16, 32, 64, 128, 256))

[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 763.91M of 64.00M vmem. Exceeded vmem capacity by 699.91M.\n\nProgram vmem requirement 763.91M:\n    scoped          763.91M\n\n  Largest program allocations in vmem:\n\n  1. Size: 694.91M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[65536]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
Best block size: (64, 32)
STARTING AUTOTUNE (256, 'bfloat16', 'float8_e4m3fn', 8, 2, 128, 131072)
[Debug] Failed with (page_size=256, num_kv_pages_per_block=32, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 85.45M of 64.00M vmem. Exceeded vmem capacity by 21.45M.\n\nProgram vmem requirement 85.45M:\n    scoped           85.45M\n\n  Largest program allocations in vmem:\n\n  1. Size: 72.45M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_32-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 8.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,8192,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 8.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_32-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_32-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: Ran out of memory in memory space vmem while allocating on stack for %RPA-bq_64-bkvp_64-p_256.1 = (bf16[2,1024,2,2,128]{4,3,2,1,0:T(2,128)(2,1)}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}) custom-call(%copy-done.3, %copy-done.1, %copy-done.2, %copy-done.4, %broadcast.5, /*index=5*/%broadcast.4, %broadcast.3, %copy_bitcast_fusion, %copy.7, %kv_cache.1), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}, metadata={op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72}. Scoped allocation with size 61.17M and limit 60.00M exceeded scoped vmem limit by 1.17M. It should not be possible to run out of scoped vmem -  see go/compile-time-vmem-oom#kernel-vmem-stack-oom for more information.')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 94.45M of 64.00M vmem. Exceeded vmem capacity by 30.45M.\n\nProgram vmem requirement 94.45M:\n    scoped           94.45M\n\n  Largest program allocations in vmem:\n\n  1. Size: 75.95M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=64, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 168.34M of 64.00M vmem. Exceeded vmem capacity by 104.34M.\n\nProgram vmem requirement 168.34M:\n    scoped          168.34M\n\n  Largest program allocations in vmem:\n\n  1. Size: 147.34M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_64-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 16.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,16384,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 16.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_64-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_64-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 64.04M of 64.00M vmem. Exceeded vmem capacity by 40.0K.\n\nProgram vmem requirement 64.04M:\n    scoped           64.04M\n\n  Largest program allocations in vmem:\n\n  1. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 31.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 72.18M of 64.00M vmem. Exceeded vmem capacity by 8.18M.\n\nProgram vmem requirement 72.18M:\n    scoped           72.18M\n\n  Largest program allocations in vmem:\n\n  1. Size: 39.87M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 88.52M of 64.00M vmem. Exceeded vmem capacity by 24.52M.\n\nProgram vmem requirement 88.52M:\n    scoped           88.52M\n\n  Largest program allocations in vmem:\n\n  1. Size: 55.90M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 121.23M of 64.00M vmem. Exceeded vmem capacity by 57.23M.\n\nProgram vmem requirement 121.23M:\n    scoped          121.23M\n\n  Largest program allocations in vmem:\n\n  1. Size: 87.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=128), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 197.80M of 64.00M vmem. Exceeded vmem capacity by 133.80M.\n\nProgram vmem requirement 197.80M:\n    scoped          197.80M\n\n  Largest program allocations in vmem:\n\n  1. Size: 163.30M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_128-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,128,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 512.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_128-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,512,128]{2,1,0:T(8,128)}\n     Unpadded size: 512.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_128-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=128, num_q_per_block=256), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 354.61M of 64.00M vmem. Exceeded vmem capacity by 290.61M.\n\nProgram vmem requirement 354.61M:\n    scoped          354.61M\n\n  Largest program allocations in vmem:\n\n  1. Size: 317.61M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_256-bkvp_128-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 32.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,32768,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 32.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,256,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 1.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_256-bkvp_128-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,1024,128]{2,1,0:T(8,128)}\n     Unpadded size: 1.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_256-bkvp_128-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=8), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 128.02M of 64.00M vmem. Exceeded vmem capacity by 64.02M.\n\nProgram vmem requirement 128.02M:\n    scoped          128.02M\n\n  Largest program allocations in vmem:\n\n  1. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 63.86M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_8-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,8,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 32.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_8-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,32,128]{2,1,0:T(8,128)}\n     Unpadded size: 32.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_8-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=16), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 144.19M of 64.00M vmem. Exceeded vmem capacity by 80.19M.\n\nProgram vmem requirement 144.19M:\n    scoped          144.19M\n\n  Largest program allocations in vmem:\n\n  1. Size: 79.88M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_16-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,16,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 64.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_16-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,64,128]{2,1,0:T(8,128)}\n     Unpadded size: 64.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_16-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=32), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 176.52M of 64.00M vmem. Exceeded vmem capacity by 112.52M.\n\nProgram vmem requirement 176.52M:\n    scoped          176.52M\n\n  Largest program allocations in vmem:\n\n  1. Size: 111.90M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_32-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,32,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 128.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_32-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,128,128]{2,1,0:T(8,128)}\n     Unpadded size: 128.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_32-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')
[Debug] Failed with (page_size=256, num_kv_pages_per_block=256, num_q_per_block=64), got error: err=JaxRuntimeError('RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space vmem. Used 241.23M of 64.00M vmem. Exceeded vmem capacity by 177.23M.\n\nProgram vmem requirement 241.23M:\n    scoped          241.23M\n\n  Largest program allocations in vmem:\n\n  1. Size: 175.98M\n     XLA label: register allocator spill slots in HLO :: RPA-bq_64-bkvp_256-p_256.1\n     Allocation type: scoped\n     ==========================\n\n  2. Size: 64.00M\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f8e4m3fn[2,65536,1,4,128]{4,3,2,1,0:T(4,128)(4,1)}\n     Unpadded size: 64.00M\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  3. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  4. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: bf16[2,2,64,2,2,128]{5,4,3,2,1,0:T(2,128)(2,1)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  5. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  6. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n  7. Size: 256.0K\n     Operator: op_name="jit(ragged_paged_attention)/RPA-bq_64-bkvp_256-p_256/pallas_call" source_file="/mnt/disks/jacobplatin/tpu-inference/tpu_inference/kernels/ragged_paged_attention/v3/kernel.py" source_line=1534 source_end_line=1534 source_column=29 source_end_column=72\n     Shape: f32[2,256,128]{2,1,0:T(8,128)}\n     Unpadded size: 256.0K\n     Tag: scratch operand\n     XLA label: RPA-bq_64-bkvp_256-p_256.1 = custom-call(copy-done.3, copy-done.1, copy-done.2, copy-done.4, ...(+6)), custom_call_target="tpu_custom_call", operand_layout_constraints={s32[256]{0}, s32[131072]{0}, s32[257]{0}, s32[3]{0}, s32[3]{0}, s32[4]{0}, s32[6]{0}, bf16[2,1024,2,2,128]{4,3,2,1,0}, f8e4m3fn[1024,1,4,128]{3,2,1,0}, f8e4m3fn[1000,256,1,4,128]{4,3,2,1,0}}, output_to_operand_aliasing={{0}: (7, {}), {1}: (9, {})}\n     Allocation type: scoped\n     ==========================\n\n')