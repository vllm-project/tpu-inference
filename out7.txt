INFO 08-08 22:27:28 [__init__.py:241] Automatically detected platform tpu.
Running vLLM without Pathways. Module pathwaysutils is not imported.
WARNING 08-08 22:27:29 [__init__.py:1683] argument 'task' is deprecated
INFO 08-08 22:27:29 [utils.py:326] non-default args: {'model': 'meta-llama/Meta-Llama-3-8B-Instruct', 'task': 'generate', 'max_model_len': 1024, 'tensor_parallel_size': 8, 'enable_lora': None, 'additional_config': {'sharding': {'sharding_strategy': {'data_parallelism': 2, 'tensor_parallelism': 4}}}}
WARNING 08-08 22:27:29 [config.py:535] The global random seed is set to 0. Since VLLM_ENABLE_V1_MULTIPROCESSING is set to False, this may affect the random state of the Python process that launched vLLM.
INFO 08-08 22:27:35 [config.py:726] Resolved architecture: LlamaForCausalLM
INFO 08-08 22:27:35 [config.py:1759] Using max model len 1024
INFO 08-08 22:27:36 [config.py:2588] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 08-08 22:27:36 [tpu_jax.py:140] The model dtype is not properly set for JAX backend. Overwriting it to bfloat16
WARNING 08-08 22:27:37 [tpu_jax.py:177] JAX requires to use uniproc_executor for single host.
INFO 08-08 22:27:38 [core.py:71] Initializing a V1 LLM engine (v0.1.dev8276+gc6541b1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=<class 'jax.numpy.bfloat16'>, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=None, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={"level":2,"debug_dump_path":"","cache_dir":"","backend":"openxla","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 08-08 22:27:38 [tpu_worker_jax.py:74] Pre-sliced devices by engine: []
wenxin init_device self.devices []
INFO 08-08 22:28:24 [tpu_worker_jax.py:102] Init devices | devices=[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=4, process_index=0, coords=(0,2,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(1,2,0), core_on_chip=0), TpuDevice(id=6, process_index=0, coords=(0,3,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,3,0), core_on_chip=0)] | hbm=[(0.0, 31.25), (0.0, 31.25), (0.0, 31.25), (0.0, 31.25), (0.0, 31.25), (0.0, 31.25), (0.0, 31.25), (0.0, 31.25)]Gb
INFO 08-08 22:28:24 [tpu_jax_runner.py:153] Init mesh | mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto))
INFO 08-08 22:28:24 [utils.py:98] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
wenxin _init_inputs self.block_size 64
scheduler_config.max_num_batched_tokens 8192
self.max_num_tokens 8192
wenxin _init_inputs self.max_num_reqs 256
wenxin _init_inputs self.max_model_len 1024
wenxin _init_inputs self.max_num_tokens 8192
INFO 08-08 22:28:24 [utils.py:64] Prepared request paddings: [8, 16, 32, 64, 128, 256]
INFO 08-08 22:28:24 [tpu_jax_runner.py:88] TPUModelRunner created!
INFO 08-08 22:28:24 [model_loader.py:223] Loading model, implementation type=flax_nnx
INFO 08-08 22:28:24 [model.py:60] Initializing abstract model for checkpoint loading.
INFO 08-08 22:28:24 [llama3.py:138] Initializing Llama3 8B model variant.
INFO 08-08 22:28:24 [llama3.py:157] Using the following config:
INFO 08-08 22:28:24 [llama3.py:157] {
INFO 08-08 22:28:24 [llama3.py:157]     "model": {
INFO 08-08 22:28:24 [llama3.py:157]         "hidden_size": 4096,
INFO 08-08 22:28:24 [llama3.py:157]         "num_layers": 32,
INFO 08-08 22:28:24 [llama3.py:157]         "num_attention_heads": 32,
INFO 08-08 22:28:24 [llama3.py:157]         "num_key_value_heads": 8,
INFO 08-08 22:28:24 [llama3.py:157]         "intermediate_size": 14336,
INFO 08-08 22:28:24 [llama3.py:157]         "dtype": "<class 'jax.numpy.bfloat16'>",
INFO 08-08 22:28:24 [llama3.py:157]         "head_dim": 128,
INFO 08-08 22:28:24 [llama3.py:157]         "rope_theta": 500000.0,
INFO 08-08 22:28:24 [llama3.py:157]         "vocab_size": 128256,
INFO 08-08 22:28:24 [llama3.py:157]         "rms_norm_eps": 1e-05,
INFO 08-08 22:28:24 [llama3.py:157]         "emb": {
INFO 08-08 22:28:24 [llama3.py:157]             "vocab_size": 128256,
INFO 08-08 22:28:24 [llama3.py:157]             "hidden_size": 4096,
INFO 08-08 22:28:24 [llama3.py:157]             "dtype": "<class 'jax.numpy.bfloat16'>",
INFO 08-08 22:28:24 [llama3.py:157]             "normalize_embeddings": false
INFO 08-08 22:28:24 [llama3.py:157]         },
INFO 08-08 22:28:24 [llama3.py:157]         "layers": {
INFO 08-08 22:28:24 [llama3.py:157]             "attention": {
INFO 08-08 22:28:24 [llama3.py:157]                 "hidden_size": 4096,
INFO 08-08 22:28:24 [llama3.py:157]                 "num_attention_heads": 32,
INFO 08-08 22:28:24 [llama3.py:157]                 "num_key_value_heads": 8,
INFO 08-08 22:28:24 [llama3.py:157]                 "head_dim": 128,
INFO 08-08 22:28:24 [llama3.py:157]                 "rope_scaling": {},
INFO 08-08 22:28:24 [llama3.py:157]                 "rope_theta": 500000.0,
INFO 08-08 22:28:24 [llama3.py:157]                 "dtype": "<class 'jax.numpy.bfloat16'>",
INFO 08-08 22:28:24 [llama3.py:157]                 "rope_input_ordering": "split",
INFO 08-08 22:28:24 [llama3.py:157]                 "attention_chunk_size": null
INFO 08-08 22:28:24 [llama3.py:157]             },
INFO 08-08 22:28:24 [llama3.py:157]             "dense_ffw": {
INFO 08-08 22:28:24 [llama3.py:157]                 "hidden_size": 4096,
INFO 08-08 22:28:24 [llama3.py:157]                 "intermediate_size": 14336,
INFO 08-08 22:28:24 [llama3.py:157]                 "hidden_act": "silu",
INFO 08-08 22:28:24 [llama3.py:157]                 "dtype": "<class 'jax.numpy.bfloat16'>"
INFO 08-08 22:28:24 [llama3.py:157]             },
INFO 08-08 22:28:24 [llama3.py:157]             "rms_norm_eps": 1e-05,
INFO 08-08 22:28:24 [llama3.py:157]             "moe": null
INFO 08-08 22:28:24 [llama3.py:157]         }
INFO 08-08 22:28:24 [llama3.py:157]     },
INFO 08-08 22:28:24 [llama3.py:157]     "serving": {}
INFO 08-08 22:28:24 [llama3.py:157] }
INFO 08-08 22:28:24 [llama3.py:158] Using the following sharding overrides:
INFO 08-08 22:28:24 [llama3.py:158]   Using Llama3ShardingRulesConfig logical rules.
INFO 08-08 22:28:24 [llama3.py:158]   Sharding overrides:
INFO 08-08 22:28:24 [llama3.py:158]     prefill logical_rule overrides:
INFO 08-08 22:28:24 [llama3.py:158]     {}
INFO 08-08 22:28:24 [llama3.py:158] 
INFO 08-08 22:28:24 [llama3.py:158]     generate logical_rule overrides:
INFO 08-08 22:28:24 [llama3.py:158]     {}
INFO 08-08 22:28:24 [llama3.py:158] 
INFO 08-08 22:28:24 [llama3.py:158] 
INFO 08-08 22:28:25 [weight_utils.py:80] Downloading weights from HF meta-llama/Meta-Llama-3-8B-Instruct
INFO 08-08 22:29:20 [weight_utils.py:106] Loading weights from /tmp/model/Meta-Llama-3-8B-Instruct/model-00001-of-00004.safetensors
INFO 08-08 22:29:23 [weight_utils.py:106] Loading weights from /tmp/model/Meta-Llama-3-8B-Instruct/model-00002-of-00004.safetensors
INFO 08-08 22:29:26 [weight_utils.py:106] Loading weights from /tmp/model/Meta-Llama-3-8B-Instruct/model-00003-of-00004.safetensors
INFO 08-08 22:29:29 [weight_utils.py:106] Loading weights from /tmp/model/Meta-Llama-3-8B-Instruct/model-00004-of-00004.safetensors
INFO 08-08 22:29:31 [tpu_jax_runner.py:237] Init model | hbm=[(3.5, 31.25), (3.5, 31.25), (3.5, 31.25), (3.5, 31.25), (3.5, 31.25), (3.5, 31.25), (3.5, 31.25), (3.5, 31.25)]Gb
INFO 08-08 22:29:31 [kv_cache_utils.py:829] GPU KV cache size: 1,636,736 tokens
INFO 08-08 22:29:31 [kv_cache_utils.py:833] Maximum concurrency for 1,024 tokens per request: 1598.38x
INFO 08-08 22:29:31 [utils.py:240] Init kv-cache | shape=32 * (512, 64, 16, 128) | sharding=NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', None, 'model'), memory_kind=device) | dtype=<class 'jax.numpy.bfloat16'> | hbm=[(4.0, 31.25), (4.0, 31.25), (4.0, 31.25), (4.0, 31.25), (4.0, 31.25), (4.0, 31.25), (4.0, 31.25), (4.0, 31.25)]Gb
INFO 08-08 22:29:31 [tpu_jax_runner.py:297] PJRT C API
INFO 08-08 22:29:31 [tpu_jax_runner.py:297] TFRT TPU v6 lite
INFO 08-08 22:29:31 [tpu_jax_runner.py:297] Built on Jun 17 2025 03:38:32 (1750156712) cl/772339112
INFO 08-08 22:29:31 [core.py:199] init engine (profile, create kv cache, warmup model) took 0.04 seconds
BlockPool blocks_per_device 256
BlockPool remainder_blocks 0
INFO 08-08 22:29:32 [llm.py:290] Supported_tasks: ('generate',)
WARNING 08-08 22:29:32 [config.py:1659] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
_get_preferred_device 0
_get_preferred_device 1

self.max_num_reqs 256
dp_rank_reqs [['0'], ['1']]
dp_rank_tokens [[128000, 791, 6864, 315, 9822, 374], [128000, 13409, 315, 279, 48713, 527]]
cumsum_num_reqs_per_dp_rank [0 1 2]
padded_num_reqs_per_dp_rank 128
padded_num_reqs_logits 16
---- dp_rank 0 -----
dp_token_offset 0
dp_req_offset 0
dp_req_padded_offset 0
num_reqs_dp 1
num_scheduled_tokens_per_req [6]
total_num_scheduled_tokens 6
req_indices [0 0 0 0 0 0]
arange [0 1 2 3 4 5]
positions_np [0 0 0 0 0 0]
self.input_batch.num_computed_tokens_cpu[req_indices] [0 0 0 0 0 0]
positions_np after add [0 1 2 3 4 5]
token_indices [0 1 2 3 4 5]
self.input_ids_cpu [128000    791   6864    315   9822    374]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [6 0 0 ... 0 0 0]
dp_req_offset 0
dp_token_offset 0
slices_start [0]
self.input_batch.num_computed_tokens_cpu [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
global_block_start_idx [0]
slot_mapping_metadata [[64  0  6]]
num_slices [1, []]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [128000    791   6864 ...      0      0      0]
---- dp_rank 1 -----
dp_token_offset 16
dp_req_offset 1
dp_req_padded_offset 128
num_reqs_dp 1
num_scheduled_tokens_per_req [6]
total_num_scheduled_tokens 6
req_indices [1 1 1 1 1 1]
arange [0 1 2 3 4 5]
positions_np [0 0 0 0 0 0]
self.input_batch.num_computed_tokens_cpu[req_indices] [0 0 0 0 0 0]
positions_np after add [0 1 2 3 4 5]
token_indices [1024 1025 1026 1027 1028 1029]
self.input_ids_cpu [128000  13409    315    279  48713    527]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [6 0 0 ... 0 0 0]
dp_req_offset 1
dp_token_offset 16
slices_start [0]
self.input_batch.num_computed_tokens_cpu [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
global_block_start_idx [16]
slot_mapping_metadata [[16384     0     6]]
num_slices [1, 1]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [128000    791   6864 ...      0      0      0]
---- --final start -- -----
input_ids (32,) [128000    791   6864    315   9822    374      0      0      0      0
      0      0      0      0      0      0 128000  13409    315    279
  48713    527      0      0      0      0      0      0      0      0
      0      0]
positions [0 1 2 3 4 5 0 0 0 0 0 0 0 0 0 0 0 1 2 3 4 5 0 0 0 0 0 0 0 0 0 0]
final slot_mapping_metadata (3, 32)
final slot_mapping_metadata [[   64     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0 16384     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    6     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     6     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]]
final slot_mapping_metadata []
block_tables [[1 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
query_start_loc [ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1 16  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0]
seq_lens [6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
logits_indices [ 5  0  0  0  0  0  0  0 21  0  0  0  0  0  0  0]
dp_rank_reqs [['0'], ['1']]
num_seqs [1 1]
---- --final finish-- -----
temp_tensor [ 6.0000002e-01 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  7.0064923e-45  8.4077908e-45  6.6710422e-30  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.3772934e-30  4.3646243e-41
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0995175e+13
  0.0000000e+00  0.0000000e+00  2.2420775e-44  0.0000000e+00
 -2.0683993e+15  2.2250378e-40  2.2420775e-44  0.0000000e+00
  4.6049755e-26  1.4012985e-45  0.0000000e+00  0.0000000e+00
  1.3772934e-30  4.3646243e-41  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  4.3644842e-41  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  1.2952631e-31  4.3646243e-41
  0.0000000e+00  0.0000000e+00  4.6758818e-30  0.0000000e+00
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  4.2038954e-45  0.0000000e+00
  2.3694278e-38  1.4012985e-45  1.4012985e-45  9.1835496e-41
  0.0000000e+00  5.6051939e-45  0.0000000e+00  2.3694278e-38
            nan  0.0000000e+00            nan  1.1210388e-44
  8.2284644e-38  0.0000000e+00            nan            nan
  0.0000000e+00  1.4012985e-45  5.7397185e-42  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1835496e-41  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1836897e-41  1.4012985e-45
  1.4012985e-45  7.0064923e-45  8.2284644e-38  0.0000000e+00
  0.0000000e+00  9.2195630e-41  8.2284644e-38  0.0000000e+00
  9.2195630e-41  9.1836897e-41  3.5873241e-43  0.0000000e+00
  1.1754944e-38  0.0000000e+00  1.1000000e+00  2.3509887e-38
            nan            nan  0.0000000e+00  3.5873241e-43
  2.1019477e-44  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.3312335e-43  2.3693561e-38  1.4012985e-40  0.0000000e+00
  9.1836897e-41  2.3509887e-38  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.4012985e-43  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  2.2420775e-44  0.0000000e+00  4.2038954e-45  3.5873241e-43
  2.3509887e-38  0.0000000e+00  2.2420775e-44  0.0000000e+00
  1.0000000e-01  7.0064923e-45  4.4841551e-44  1.4012985e-45
  1.4012985e-44  4.2038954e-44  4.4841551e-44  1.4012985e-45
            nan            nan  1.4012985e-45  0.0000000e+00
 -1.7014118e+38  0.0000000e+00  9.2194229e-41  2.3509887e-38
  2.3509887e-38  0.0000000e+00  0.0000000e+00  2.3510604e-38
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  8.2284644e-38  0.0000000e+00  2.8025969e-44  5.6051939e-44
  0.0000000e+00            nan            nan            nan]
top_k_tensor [        1         0         0         0         0         0         0
         0     -4096         0         0         0         0         0
         0         0     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1 233707072     31147 344830232         0     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707048     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -8192        -1     -8192        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707064     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1 233707032     31147
 344830208         0 233707032     31147 344830216         0     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1]
top_p_tensor [ 8.9999998e-01  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan            nan  1.0248949e+11  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316673e-13 -3.0316488e-13
            nan            nan  1.2113646e+13  4.7644148e-44
            nan            nan -3.0360897e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4933738e+15  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0319264e-13 -3.0360897e-13
            nan            nan -3.0360897e-13 -3.1071440e-13
            nan            nan  1.7651560e+17  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -1.1368684e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
  1.4307163e-26  0.0000000e+00  0.0000000e+00  0.0000000e+00
            nan            nan  2.1188873e+19  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -1.3020833e-03
            nan            nan -3.0316532e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  2.6222406e+21  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0494818e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4012985e-45  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0320188e-13 -3.0316488e-13
            nan            nan  8.5479206e-44  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.6261203e+24  4.3643441e-41
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  1.4692446e-39  0.0000000e+00
            nan            nan  1.4012985e-44  0.0000000e+00
            nan            nan  1.4012985e-45 -1.1368685e-13
            nan            nan  0.0000000e+00  4.7644148e-44
            nan            nan -3.0316488e-13 -4.1268447e+26
            nan            nan -3.0316488e-13 -3.0494124e-13
            nan            nan -3.1027031e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327764e-13
            nan            nan  4.3148833e+29  4.7644148e-44
            nan            nan -3.0319264e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -4.4307661e+35
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan  5.0080864e+31  4.7644148e-44
            nan            nan -3.3158659e-13 -7.8066734e-11
            nan            nan -3.0316491e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327590e-13
            nan            nan  5.7511240e+33  4.7644148e-44
            nan            nan -3.0316499e-13 -4.1685183e-13
            nan            nan            nan -3.0316491e-13
            nan            nan -7.1696835e-03 -3.0316532e-13
            nan            nan -3.0319264e-13 -4.4307597e+35]
2. new_kv (16, 4, 128)
2. kv_cache (16384, 4, 128)
2. slices (3, 16)
2. new kv Traced<bfloat16[]>with<DynamicJaxprTrace>
2. num_slices Traced<int32[1]>with<DynamicJaxprTrace>
2. num_slices_per_block 64
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
ragged_paged_attention_kernel <function ragged_paged_attention_kernel at 0x79aa097ac720>
num_seqs Traced<int32[]>with<DynamicJaxprTrace>
cu_q_lens_ref Traced<MemRef<smem>{int32[128]}>with<DynamicJaxprTrace>
here
here 2
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
-- kv_cache Traced<bfloat16[128]>with<DynamicJaxprTrace>
hidden_states after model_fn (32, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', None), memory_kind=device)
inputs[4] (16,) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(None,), memory_kind=device)
hidden_states after select (16, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(), memory_kind=device)
hidden_states 0 [0.9375 -1.00781 0.871094 1.58594 0.478516 0.71875 -2.375 1.89844 -1.46094
 -1.74219]
hidden_states 1 [-0.546875 1.99219 0.621094 -2.39062 -0.408203 1.1875 -0.46875 0.664062
 -0.671875 -1.21094]
hidden_states 2 [-0.546875 1.99219 0.621094 -2.39062 -0.408203 1.1875 -0.46875 0.664062
 -0.671875 -1.21094]
hidden_states 3 [-0.546875 1.99219 0.621094 -2.39062 -0.408203 1.1875 -0.46875 0.664062
 -0.671875 -1.21094]
hidden_states 4 [-0.546875 1.99219 0.621094 -2.39062 -0.408203 1.1875 -0.46875 0.664062
 -0.671875 -1.21094]
hidden_states 5 [-0.546875 1.99219 0.621094 -2.39062 -0.408203 1.1875 -0.46875 0.664062
 -0.671875 -1.21094]
hidden_states 6 [-0.546875 1.99219 0.621094 -2.39062 -0.408203 1.1875 -0.46875 0.664062
 -0.671875 -1.21094]
hidden_states 7 [-0.546875 1.99219 0.621094 -2.39062 -0.408203 1.1875 -0.46875 0.664062
 -0.671875 -1.21094]
hidden_states 8 [-1.49219 -3.04688 3.40625 1.125 -1.25781 -0.980469 -2.92188 -0.453125
 1.00781 -2.98438]
hidden_states 9 [-0.546875 1.99219 0.621094 -2.39062 -0.408203 1.1875 -0.46875 0.664062
 -0.671875 -1.21094]
hidden_states 10 [-0.546875 1.99219 0.621094 -2.39062 -0.408203 1.1875 -0.46875 0.664062
 -0.671875 -1.21094]
hidden_states 11 [-0.546875 1.99219 0.621094 -2.39062 -0.408203 1.1875 -0.46875 0.664062
 -0.671875 -1.21094]
logits (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits after select (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits 0 [4.84375 3.35938 1.70312 -0.271484 2.39062 1.60156 3.32812 4.0625 4.625
 0.515625]
logits 1 [1.5 -0.398438 -3.04688 0.408203 2.15625 -0.166992 0.40625 -0.460938
 3.9375 -1.40625]
logits 2 [1.5 -0.398438 -3.04688 0.408203 2.15625 -0.166992 0.40625 -0.460938
 3.9375 -1.40625]
logits 3 [1.5 -0.398438 -3.04688 0.408203 2.15625 -0.166992 0.40625 -0.460938
 3.9375 -1.40625]
logits 4 [1.5 -0.398438 -3.04688 0.408203 2.15625 -0.166992 0.40625 -0.460938
 3.9375 -1.40625]
logits 5 [1.5 -0.398438 -3.04688 0.408203 2.15625 -0.166992 0.40625 -0.460938
 3.9375 -1.40625]
logits 6 [1.5 -0.398438 -3.04688 0.408203 2.15625 -0.166992 0.40625 -0.460938
 3.9375 -1.40625]
logits 7 [1.5 -0.398438 -3.04688 0.408203 2.15625 -0.166992 0.40625 -0.460938
 3.9375 -1.40625]
logits 8 [2.5 0.707031 -1.11719 0.431641 0.105957 -0.0427246 1.69531 2.17188
 0.390625 -0.0849609]
logits 9 [1.5 -0.398438 -3.04688 0.408203 2.15625 -0.166992 0.40625 -0.460938
 3.9375 -1.40625]
logits 10 [1.5 -0.398438 -3.04688 0.408203 2.15625 -0.166992 0.40625 -0.460938
 3.9375 -1.40625]
logits 11 [1.5 -0.398438 -3.04688 0.408203 2.15625 -0.166992 0.40625 -0.460938
 3.9375 -1.40625]
next_tokens after sampling [264 315 315 315 315 315 315 315 279 315 315 315 315 315 315 315]
wenxin next_tokens [264 315 315 315 315 315 315 315 279 315 315 315 315 315 315 315]

self.max_num_reqs 256
dp_rank_reqs [['0'], ['1']]
dp_rank_tokens [[264], [315]]
cumsum_num_reqs_per_dp_rank [0 1 2]
padded_num_reqs_per_dp_rank 128
padded_num_reqs_logits 16
---- dp_rank 0 -----
dp_token_offset 0
dp_req_offset 0
dp_req_padded_offset 0
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [0]
arange [0]
positions_np [0]
self.input_batch.num_computed_tokens_cpu[req_indices] [6]
positions_np after add [6]
token_indices [6]
self.input_ids_cpu [264]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [7 0 0 ... 0 0 0]
dp_req_offset 0
dp_token_offset 0
slices_start [6]
self.input_batch.num_computed_tokens_cpu [6 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
global_block_start_idx [0]
slot_mapping_metadata [[70  0  1]]
num_slices [1, []]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [264   0   0 ...   0   0   0]
---- dp_rank 1 -----
dp_token_offset 16
dp_req_offset 1
dp_req_padded_offset 128
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [1]
arange [0]
positions_np [0]
self.input_batch.num_computed_tokens_cpu[req_indices] [6]
positions_np after add [6]
token_indices [1030]
self.input_ids_cpu [315]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [7 0 0 ... 0 0 0]
dp_req_offset 1
dp_token_offset 16
slices_start [6]
self.input_batch.num_computed_tokens_cpu [6 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
global_block_start_idx [16]
slot_mapping_metadata [[16390     0     1]]
num_slices [1, 1]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [264   0   0 ...   0   0   0]
---- --final start -- -----
input_ids (32,) [264   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 315   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
positions [6 1 2 3 4 5 0 0 0 0 0 0 0 0 0 0 6 1 2 3 4 5 0 0 0 0 0 0 0 0 0 0]
final slot_mapping_metadata (3, 32)
final slot_mapping_metadata [[   70     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0 16390     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    1     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     1     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]]
final slot_mapping_metadata []
block_tables [[1 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
query_start_loc [ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1 16  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0]
seq_lens [7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
logits_indices [ 0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0]
dp_rank_reqs [['0'], ['1']]
num_seqs [1 1]
---- --final finish-- -----
temp_tensor [-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  7.0064923e-45  8.4077908e-45  6.6710422e-30  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.3772934e-30  4.3646243e-41
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0995175e+13
  0.0000000e+00  0.0000000e+00  2.2420775e-44  0.0000000e+00
 -2.0683993e+15  2.2250378e-40  2.2420775e-44  0.0000000e+00
  4.6049755e-26  1.4012985e-45  0.0000000e+00  0.0000000e+00
  1.3772934e-30  4.3646243e-41  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  4.3644842e-41  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  1.2952631e-31  4.3646243e-41
  0.0000000e+00  0.0000000e+00  4.6758818e-30  0.0000000e+00
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  4.2038954e-45  0.0000000e+00
  2.3694278e-38  1.4012985e-45  1.4012985e-45  9.1835496e-41
  0.0000000e+00  5.6051939e-45  0.0000000e+00  2.3694278e-38
            nan  0.0000000e+00            nan  1.1210388e-44
  8.2284644e-38  0.0000000e+00            nan            nan
  0.0000000e+00  1.4012985e-45  5.7397185e-42  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1835496e-41  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1836897e-41  1.4012985e-45
  1.4012985e-45  7.0064923e-45  8.2284644e-38  0.0000000e+00
  0.0000000e+00  9.2195630e-41  8.2284644e-38  0.0000000e+00
  9.2195630e-41  9.1836897e-41  3.5873241e-43  0.0000000e+00
  1.1754944e-38  0.0000000e+00  1.1000000e+00  2.3509887e-38
            nan            nan  0.0000000e+00  3.5873241e-43
  2.1019477e-44  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.3312335e-43  2.3693561e-38  1.4012985e-40  0.0000000e+00
  9.1836897e-41  2.3509887e-38  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.4012985e-43  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  2.2420775e-44  0.0000000e+00  4.2038954e-45  3.5873241e-43
  2.3509887e-38  0.0000000e+00  2.2420775e-44  0.0000000e+00
  1.0000000e-01  7.0064923e-45  4.4841551e-44  1.4012985e-45
  1.4012985e-44  4.2038954e-44  4.4841551e-44  1.4012985e-45
            nan            nan  1.4012985e-45  0.0000000e+00
 -1.7014118e+38  0.0000000e+00  9.2194229e-41  2.3509887e-38
  2.3509887e-38  0.0000000e+00  0.0000000e+00  2.3510604e-38
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  8.2284644e-38  0.0000000e+00  2.8025969e-44  5.6051939e-44
  0.0000000e+00            nan            nan            nan]
top_k_tensor [        0         0         0         0         0         0         0
         0     -4096         0         0         0         0         0
         0         0     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1 233707072     31147 344830232         0     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707048     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -8192        -1     -8192        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707064     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1 233707032     31147
 344830208         0 233707032     31147 344830216         0     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1]
top_p_tensor [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan            nan  1.0248949e+11  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316673e-13 -3.0316488e-13
            nan            nan  1.2113646e+13  4.7644148e-44
            nan            nan -3.0360897e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4933738e+15  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0319264e-13 -3.0360897e-13
            nan            nan -3.0360897e-13 -3.1071440e-13
            nan            nan  1.7651560e+17  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -1.1368684e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
  1.4307163e-26  0.0000000e+00  0.0000000e+00  0.0000000e+00
            nan            nan  2.1188873e+19  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -1.3020833e-03
            nan            nan -3.0316532e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  2.6222406e+21  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0494818e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4012985e-45  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0320188e-13 -3.0316488e-13
            nan            nan  8.5479206e-44  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.6261203e+24  4.3643441e-41
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  1.4692446e-39  0.0000000e+00
            nan            nan  1.4012985e-44  0.0000000e+00
            nan            nan  1.4012985e-45 -1.1368685e-13
            nan            nan  0.0000000e+00  4.7644148e-44
            nan            nan -3.0316488e-13 -4.1268447e+26
            nan            nan -3.0316488e-13 -3.0494124e-13
            nan            nan -3.1027031e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327764e-13
            nan            nan  4.3148833e+29  4.7644148e-44
            nan            nan -3.0319264e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -4.4307661e+35
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan  5.0080864e+31  4.7644148e-44
            nan            nan -3.3158659e-13 -7.8066734e-11
            nan            nan -3.0316491e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327590e-13
            nan            nan  5.7511240e+33  4.7644148e-44
            nan            nan -3.0316499e-13 -4.1685183e-13
            nan            nan            nan -3.0316491e-13
            nan            nan -7.1696835e-03 -3.0316532e-13
            nan            nan -3.0319264e-13 -4.4307597e+35]
hidden_states after model_fn (32, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', None), memory_kind=device)
inputs[4] (16,) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(None,), memory_kind=device)
hidden_states after select (16, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(), memory_kind=device)
hidden_states 0 [3.20312 -1.6875 0.322266 -0.219727 1.96875 -3.14062 -2.1875 0.241211
 1.57031 0.761719]
hidden_states 1 [3.20312 -1.6875 0.322266 -0.219727 1.96875 -3.14062 -2.1875 0.241211
 1.57031 0.761719]
hidden_states 2 [3.20312 -1.6875 0.322266 -0.219727 1.96875 -3.14062 -2.1875 0.241211
 1.57031 0.761719]
hidden_states 3 [3.20312 -1.6875 0.322266 -0.219727 1.96875 -3.14062 -2.1875 0.241211
 1.57031 0.761719]
hidden_states 4 [3.20312 -1.6875 0.322266 -0.219727 1.96875 -3.14062 -2.1875 0.241211
 1.57031 0.761719]
hidden_states 5 [3.20312 -1.6875 0.322266 -0.219727 1.96875 -3.14062 -2.1875 0.241211
 1.57031 0.761719]
hidden_states 6 [3.20312 -1.6875 0.322266 -0.219727 1.96875 -3.14062 -2.1875 0.241211
 1.57031 0.761719]
hidden_states 7 [3.20312 -1.6875 0.322266 -0.219727 1.96875 -3.14062 -2.1875 0.241211
 1.57031 0.761719]
hidden_states 8 [0.566406 1.61719 0.753906 0.992188 -0.412109 0.183594 -0.789062 3.09375
 -1.10156 -0.628906]
hidden_states 9 [3.20312 -1.6875 0.322266 -0.219727 1.96875 -3.14062 -2.1875 0.241211
 1.57031 0.761719]
hidden_states 10 [3.20312 -1.6875 0.322266 -0.219727 1.96875 -3.14062 -2.1875 0.241211
 1.57031 0.761719]
hidden_states 11 [3.20312 -1.6875 0.322266 -0.219727 1.96875 -3.14062 -2.1875 0.241211
 1.57031 0.761719]
logits (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits after select (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits 0 [6.34375 5.625 1.52344 0.419922 -1.07031 0.208008 4.5625 3.125 4.46875
 0.769531]
logits 1 [6.34375 5.625 1.52344 0.419922 -1.07031 0.208008 4.5625 3.125 4.46875
 0.769531]
logits 2 [6.34375 5.625 1.52344 0.419922 -1.07031 0.208008 4.5625 3.125 4.46875
 0.769531]
logits 3 [6.34375 5.625 1.52344 0.419922 -1.07031 0.208008 4.5625 3.125 4.46875
 0.769531]
logits 4 [6.34375 5.625 1.52344 0.419922 -1.07031 0.208008 4.5625 3.125 4.46875
 0.769531]
logits 5 [6.34375 5.625 1.52344 0.419922 -1.07031 0.208008 4.5625 3.125 4.46875
 0.769531]
logits 6 [6.34375 5.625 1.52344 0.419922 -1.07031 0.208008 4.5625 3.125 4.46875
 0.769531]
logits 7 [6.34375 5.625 1.52344 0.419922 -1.07031 0.208008 4.5625 3.125 4.46875
 0.769531]
logits 8 [7.03125 6.03125 2.76562 2.1875 3.54688 2.71875 5.625 4.1875 4.59375 3.125]
logits 9 [6.34375 5.625 1.52344 0.419922 -1.07031 0.208008 4.5625 3.125 4.46875
 0.769531]
logits 10 [6.34375 5.625 1.52344 0.419922 -1.07031 0.208008 4.5625 3.125 4.46875
 0.769531]
logits 11 [6.34375 5.625 1.52344 0.419922 -1.07031 0.208008 4.5625 3.125 4.46875
 0.769531]
next_tokens after sampling [627 627 627 627 627 627 627 627 320 627 627 627 627 627 627 627]
wenxin next_tokens [627 627 627 627 627 627 627 627 320 627 627 627 627 627 627 627]

self.max_num_reqs 256
dp_rank_reqs [['0'], ['1']]
dp_rank_tokens [[627], [627]]
cumsum_num_reqs_per_dp_rank [0 1 2]
padded_num_reqs_per_dp_rank 128
padded_num_reqs_logits 16
---- dp_rank 0 -----
dp_token_offset 0
dp_req_offset 0
dp_req_padded_offset 0
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [0]
arange [0]
positions_np [6]
self.input_batch.num_computed_tokens_cpu[req_indices] [7]
positions_np after add [7]
token_indices [7]
self.input_ids_cpu [627]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [8 0 0 ... 0 0 0]
dp_req_offset 0
dp_token_offset 0
slices_start [7]
self.input_batch.num_computed_tokens_cpu [7 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
global_block_start_idx [0]
slot_mapping_metadata [[71  0  1]]
num_slices [1, []]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [627   0   0 ...   0   0   0]
---- dp_rank 1 -----
dp_token_offset 16
dp_req_offset 1
dp_req_padded_offset 128
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [1]
arange [0]
positions_np [6]
self.input_batch.num_computed_tokens_cpu[req_indices] [7]
positions_np after add [7]
token_indices [1031]
self.input_ids_cpu [627]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [8 0 0 ... 0 0 0]
dp_req_offset 1
dp_token_offset 16
slices_start [7]
self.input_batch.num_computed_tokens_cpu [7 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
global_block_start_idx [16]
slot_mapping_metadata [[16391     0     1]]
num_slices [1, 1]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [627   0   0 ...   0   0   0]
---- --final start -- -----
input_ids (32,) [627   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 627   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
positions [7 1 2 3 4 5 0 0 0 0 0 0 0 0 0 0 7 1 2 3 4 5 0 0 0 0 0 0 0 0 0 0]
final slot_mapping_metadata (3, 32)
final slot_mapping_metadata [[   71     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0 16391     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    1     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     1     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]]
final slot_mapping_metadata []
block_tables [[1 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
query_start_loc [ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1 16  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0]
seq_lens [8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
logits_indices [ 0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0]
dp_rank_reqs [['0'], ['1']]
num_seqs [1 1]
---- --final finish-- -----
temp_tensor [-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  7.0064923e-45  8.4077908e-45  6.6710422e-30  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.3772934e-30  4.3646243e-41
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0995175e+13
  0.0000000e+00  0.0000000e+00  2.2420775e-44  0.0000000e+00
 -2.0683993e+15  2.2250378e-40  2.2420775e-44  0.0000000e+00
  4.6049755e-26  1.4012985e-45  0.0000000e+00  0.0000000e+00
  1.3772934e-30  4.3646243e-41  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  4.3644842e-41  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  1.2952631e-31  4.3646243e-41
  0.0000000e+00  0.0000000e+00  4.6758818e-30  0.0000000e+00
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  4.2038954e-45  0.0000000e+00
  2.3694278e-38  1.4012985e-45  1.4012985e-45  9.1835496e-41
  0.0000000e+00  5.6051939e-45  0.0000000e+00  2.3694278e-38
            nan  0.0000000e+00            nan  1.1210388e-44
  8.2284644e-38  0.0000000e+00            nan            nan
  0.0000000e+00  1.4012985e-45  5.7397185e-42  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1835496e-41  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1836897e-41  1.4012985e-45
  1.4012985e-45  7.0064923e-45  8.2284644e-38  0.0000000e+00
  0.0000000e+00  9.2195630e-41  8.2284644e-38  0.0000000e+00
  9.2195630e-41  9.1836897e-41  3.5873241e-43  0.0000000e+00
  1.1754944e-38  0.0000000e+00  1.1000000e+00  2.3509887e-38
            nan            nan  0.0000000e+00  3.5873241e-43
  2.1019477e-44  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.3312335e-43  2.3693561e-38  1.4012985e-40  0.0000000e+00
  9.1836897e-41  2.3509887e-38  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.4012985e-43  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  2.2420775e-44  0.0000000e+00  4.2038954e-45  3.5873241e-43
  2.3509887e-38  0.0000000e+00  2.2420775e-44  0.0000000e+00
  1.0000000e-01  7.0064923e-45  4.4841551e-44  1.4012985e-45
  1.4012985e-44  4.2038954e-44  4.4841551e-44  1.4012985e-45
            nan            nan  1.4012985e-45  0.0000000e+00
 -1.7014118e+38  0.0000000e+00  9.2194229e-41  2.3509887e-38
  2.3509887e-38  0.0000000e+00  0.0000000e+00  2.3510604e-38
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  8.2284644e-38  0.0000000e+00  2.8025969e-44  5.6051939e-44
  0.0000000e+00            nan            nan            nan]
top_k_tensor [        0         0         0         0         0         0         0
         0     -4096         0         0         0         0         0
         0         0     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1 233707072     31147 344830232         0     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707048     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -8192        -1     -8192        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707064     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1 233707032     31147
 344830208         0 233707032     31147 344830216         0     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1]
top_p_tensor [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan            nan  1.0248949e+11  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316673e-13 -3.0316488e-13
            nan            nan  1.2113646e+13  4.7644148e-44
            nan            nan -3.0360897e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4933738e+15  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0319264e-13 -3.0360897e-13
            nan            nan -3.0360897e-13 -3.1071440e-13
            nan            nan  1.7651560e+17  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -1.1368684e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
  1.4307163e-26  0.0000000e+00  0.0000000e+00  0.0000000e+00
            nan            nan  2.1188873e+19  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -1.3020833e-03
            nan            nan -3.0316532e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  2.6222406e+21  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0494818e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4012985e-45  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0320188e-13 -3.0316488e-13
            nan            nan  8.5479206e-44  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.6261203e+24  4.3643441e-41
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  1.4692446e-39  0.0000000e+00
            nan            nan  1.4012985e-44  0.0000000e+00
            nan            nan  1.4012985e-45 -1.1368685e-13
            nan            nan  0.0000000e+00  4.7644148e-44
            nan            nan -3.0316488e-13 -4.1268447e+26
            nan            nan -3.0316488e-13 -3.0494124e-13
            nan            nan -3.1027031e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327764e-13
            nan            nan  4.3148833e+29  4.7644148e-44
            nan            nan -3.0319264e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -4.4307661e+35
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan  5.0080864e+31  4.7644148e-44
            nan            nan -3.3158659e-13 -7.8066734e-11
            nan            nan -3.0316491e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327590e-13
            nan            nan  5.7511240e+33  4.7644148e-44
            nan            nan -3.0316499e-13 -4.1685183e-13
            nan            nan            nan -3.0316491e-13
            nan            nan -7.1696835e-03 -3.0316532e-13
            nan            nan -3.0319264e-13 -4.4307597e+35]
hidden_states after model_fn (32, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', None), memory_kind=device)
inputs[4] (16,) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(None,), memory_kind=device)
hidden_states after select (16, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(), memory_kind=device)
hidden_states 0 [-1.14062 -7.96875 0.291016 1.57812 -0.341797 -5.5 -1.54688 0.390625
 0.699219 -0.289062]
hidden_states 1 [-1.14062 -7.96875 0.291016 1.57812 -0.341797 -5.5 -1.54688 0.390625
 0.699219 -0.289062]
hidden_states 2 [-1.14062 -7.96875 0.291016 1.57812 -0.341797 -5.5 -1.54688 0.390625
 0.699219 -0.289062]
hidden_states 3 [-1.14062 -7.96875 0.291016 1.57812 -0.341797 -5.5 -1.54688 0.390625
 0.699219 -0.289062]
hidden_states 4 [-1.14062 -7.96875 0.291016 1.57812 -0.341797 -5.5 -1.54688 0.390625
 0.699219 -0.289062]
hidden_states 5 [-1.14062 -7.96875 0.291016 1.57812 -0.341797 -5.5 -1.54688 0.390625
 0.699219 -0.289062]
hidden_states 6 [-1.14062 -7.96875 0.291016 1.57812 -0.341797 -5.5 -1.54688 0.390625
 0.699219 -0.289062]
hidden_states 7 [-1.14062 -7.96875 0.291016 1.57812 -0.341797 -5.5 -1.54688 0.390625
 0.699219 -0.289062]
hidden_states 8 [-0.21875 -2.53125 -0.140625 0.640625 1.17188 0.279297 -1.125 -0.671875
 -2.0625 -0.625]
hidden_states 9 [-1.14062 -7.96875 0.291016 1.57812 -0.341797 -5.5 -1.54688 0.390625
 0.699219 -0.289062]
hidden_states 10 [-1.14062 -7.96875 0.291016 1.57812 -0.341797 -5.5 -1.54688 0.390625
 0.699219 -0.289062]
hidden_states 11 [-1.14062 -7.96875 0.291016 1.57812 -0.341797 -5.5 -1.54688 0.390625
 0.699219 -0.289062]
logits (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits after select (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits 0 [2.875 6.8125 5.46875 2.875 1.5 3.75 4.0625 6.40625 3.9375 6.6875]
logits 1 [2.875 6.8125 5.46875 2.875 1.5 3.75 4.0625 6.40625 3.9375 6.6875]
logits 2 [2.875 6.8125 5.46875 2.875 1.5 3.75 4.0625 6.40625 3.9375 6.6875]
logits 3 [2.875 6.8125 5.46875 2.875 1.5 3.75 4.0625 6.40625 3.9375 6.6875]
logits 4 [2.875 6.8125 5.46875 2.875 1.5 3.75 4.0625 6.40625 3.9375 6.6875]
logits 5 [2.875 6.8125 5.46875 2.875 1.5 3.75 4.0625 6.40625 3.9375 6.6875]
logits 6 [2.875 6.8125 5.46875 2.875 1.5 3.75 4.0625 6.40625 3.9375 6.6875]
logits 7 [2.875 6.8125 5.46875 2.875 1.5 3.75 4.0625 6.40625 3.9375 6.6875]
logits 8 [4.71875 9.6875 8.6875 5.6875 4.3125 4.53125 7.3125 9.3125 4.96875 8.875]
logits 9 [2.875 6.8125 5.46875 2.875 1.5 3.75 4.0625 6.40625 3.9375 6.6875]
logits 10 [2.875 6.8125 5.46875 2.875 1.5 3.75 4.0625 6.40625 3.9375 6.6875]
logits 11 [2.875 6.8125 5.46875 2.875 1.5 3.75 4.0625 6.40625 3.9375 6.6875]
next_tokens after sampling [ 791  791  791  791  791  791  791  791 3923  791  791  791  791  791
  791  791]
wenxin next_tokens [ 791  791  791  791  791  791  791  791 3923  791  791  791  791  791
  791  791]

self.max_num_reqs 256
dp_rank_reqs [['0'], ['1']]
dp_rank_tokens [[791], [791]]
cumsum_num_reqs_per_dp_rank [0 1 2]
padded_num_reqs_per_dp_rank 128
padded_num_reqs_logits 16
---- dp_rank 0 -----
dp_token_offset 0
dp_req_offset 0
dp_req_padded_offset 0
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [0]
arange [0]
positions_np [7]
self.input_batch.num_computed_tokens_cpu[req_indices] [8]
positions_np after add [8]
token_indices [8]
self.input_ids_cpu [791]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [9 0 0 ... 0 0 0]
dp_req_offset 0
dp_token_offset 0
slices_start [8]
self.input_batch.num_computed_tokens_cpu [8 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
global_block_start_idx [0]
slot_mapping_metadata [[72  0  1]]
num_slices [1, []]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [791   0   0 ...   0   0   0]
---- dp_rank 1 -----
dp_token_offset 16
dp_req_offset 1
dp_req_padded_offset 128
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [1]
arange [0]
positions_np [7]
self.input_batch.num_computed_tokens_cpu[req_indices] [8]
positions_np after add [8]
token_indices [1032]
self.input_ids_cpu [791]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [9 0 0 ... 0 0 0]
dp_req_offset 1
dp_token_offset 16
slices_start [8]
self.input_batch.num_computed_tokens_cpu [8 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
global_block_start_idx [16]
slot_mapping_metadata [[16392     0     1]]
num_slices [1, 1]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [791   0   0 ...   0   0   0]
---- --final start -- -----
input_ids (32,) [791   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 791   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
positions [8 1 2 3 4 5 0 0 0 0 0 0 0 0 0 0 8 1 2 3 4 5 0 0 0 0 0 0 0 0 0 0]
final slot_mapping_metadata (3, 32)
final slot_mapping_metadata [[   72     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0 16392     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    1     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     1     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]]
final slot_mapping_metadata []
block_tables [[1 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
query_start_loc [ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1 16  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0]
seq_lens [9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
logits_indices [ 0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0]
dp_rank_reqs [['0'], ['1']]
num_seqs [1 1]
---- --final finish-- -----
temp_tensor [-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  7.0064923e-45  8.4077908e-45  6.6710422e-30  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.3772934e-30  4.3646243e-41
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0995175e+13
  0.0000000e+00  0.0000000e+00  2.2420775e-44  0.0000000e+00
 -2.0683993e+15  2.2250378e-40  2.2420775e-44  0.0000000e+00
  4.6049755e-26  1.4012985e-45  0.0000000e+00  0.0000000e+00
  1.3772934e-30  4.3646243e-41  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  4.3644842e-41  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  1.2952631e-31  4.3646243e-41
  0.0000000e+00  0.0000000e+00  4.6758818e-30  0.0000000e+00
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  4.2038954e-45  0.0000000e+00
  2.3694278e-38  1.4012985e-45  1.4012985e-45  9.1835496e-41
  0.0000000e+00  5.6051939e-45  0.0000000e+00  2.3694278e-38
            nan  0.0000000e+00            nan  1.1210388e-44
  8.2284644e-38  0.0000000e+00            nan            nan
  0.0000000e+00  1.4012985e-45  5.7397185e-42  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1835496e-41  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1836897e-41  1.4012985e-45
  1.4012985e-45  7.0064923e-45  8.2284644e-38  0.0000000e+00
  0.0000000e+00  9.2195630e-41  8.2284644e-38  0.0000000e+00
  9.2195630e-41  9.1836897e-41  3.5873241e-43  0.0000000e+00
  1.1754944e-38  0.0000000e+00  1.1000000e+00  2.3509887e-38
            nan            nan  0.0000000e+00  3.5873241e-43
  2.1019477e-44  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.3312335e-43  2.3693561e-38  1.4012985e-40  0.0000000e+00
  9.1836897e-41  2.3509887e-38  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.4012985e-43  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  2.2420775e-44  0.0000000e+00  4.2038954e-45  3.5873241e-43
  2.3509887e-38  0.0000000e+00  2.2420775e-44  0.0000000e+00
  1.0000000e-01  7.0064923e-45  4.4841551e-44  1.4012985e-45
  1.4012985e-44  4.2038954e-44  4.4841551e-44  1.4012985e-45
            nan            nan  1.4012985e-45  0.0000000e+00
 -1.7014118e+38  0.0000000e+00  9.2194229e-41  2.3509887e-38
  2.3509887e-38  0.0000000e+00  0.0000000e+00  2.3510604e-38
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  8.2284644e-38  0.0000000e+00  2.8025969e-44  5.6051939e-44
  0.0000000e+00            nan            nan            nan]
top_k_tensor [        0         0         0         0         0         0         0
         0     -4096         0         0         0         0         0
         0         0     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1 233707072     31147 344830232         0     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707048     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -8192        -1     -8192        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707064     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1 233707032     31147
 344830208         0 233707032     31147 344830216         0     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1]
top_p_tensor [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan            nan  1.0248949e+11  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316673e-13 -3.0316488e-13
            nan            nan  1.2113646e+13  4.7644148e-44
            nan            nan -3.0360897e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4933738e+15  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0319264e-13 -3.0360897e-13
            nan            nan -3.0360897e-13 -3.1071440e-13
            nan            nan  1.7651560e+17  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -1.1368684e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
  1.4307163e-26  0.0000000e+00  0.0000000e+00  0.0000000e+00
            nan            nan  2.1188873e+19  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -1.3020833e-03
            nan            nan -3.0316532e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  2.6222406e+21  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0494818e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4012985e-45  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0320188e-13 -3.0316488e-13
            nan            nan  8.5479206e-44  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.6261203e+24  4.3643441e-41
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  1.4692446e-39  0.0000000e+00
            nan            nan  1.4012985e-44  0.0000000e+00
            nan            nan  1.4012985e-45 -1.1368685e-13
            nan            nan  0.0000000e+00  4.7644148e-44
            nan            nan -3.0316488e-13 -4.1268447e+26
            nan            nan -3.0316488e-13 -3.0494124e-13
            nan            nan -3.1027031e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327764e-13
            nan            nan  4.3148833e+29  4.7644148e-44
            nan            nan -3.0319264e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -4.4307661e+35
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan  5.0080864e+31  4.7644148e-44
            nan            nan -3.3158659e-13 -7.8066734e-11
            nan            nan -3.0316491e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327590e-13
            nan            nan  5.7511240e+33  4.7644148e-44
            nan            nan -3.0316499e-13 -4.1685183e-13
            nan            nan            nan -3.0316491e-13
            nan            nan -7.1696835e-03 -3.0316532e-13
            nan            nan -3.0319264e-13 -4.4307597e+35]
hidden_states after model_fn (32, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', None), memory_kind=device)
inputs[4] (16,) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(None,), memory_kind=device)
hidden_states after select (16, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(), memory_kind=device)
hidden_states 0 [-2.04688 -1.63281 -0.714844 0.439453 -1.16406 -1.03906 -0.419922 2.01562
 0.769531 -1.72656]
hidden_states 1 [-2.04688 -1.63281 -0.714844 0.439453 -1.16406 -1.03906 -0.419922 2.01562
 0.769531 -1.72656]
hidden_states 2 [-2.04688 -1.63281 -0.714844 0.439453 -1.16406 -1.03906 -0.419922 2.01562
 0.769531 -1.72656]
hidden_states 3 [-2.04688 -1.63281 -0.714844 0.439453 -1.16406 -1.03906 -0.419922 2.01562
 0.769531 -1.72656]
hidden_states 4 [-2.04688 -1.63281 -0.714844 0.439453 -1.16406 -1.03906 -0.419922 2.01562
 0.769531 -1.72656]
hidden_states 5 [-2.04688 -1.63281 -0.714844 0.439453 -1.16406 -1.03906 -0.419922 2.01562
 0.769531 -1.72656]
hidden_states 6 [-2.04688 -1.63281 -0.714844 0.439453 -1.16406 -1.03906 -0.419922 2.01562
 0.769531 -1.72656]
hidden_states 7 [-2.04688 -1.63281 -0.714844 0.439453 -1.16406 -1.03906 -0.419922 2.01562
 0.769531 -1.72656]
hidden_states 8 [-2.03125 -1.32031 -2.35938 1.66406 -1.15625 3.20312 0.628906 3 -0.523438
 -2.28125]
hidden_states 9 [-2.04688 -1.63281 -0.714844 0.439453 -1.16406 -1.03906 -0.419922 2.01562
 0.769531 -1.72656]
hidden_states 10 [-2.04688 -1.63281 -0.714844 0.439453 -1.16406 -1.03906 -0.419922 2.01562
 0.769531 -1.72656]
hidden_states 11 [-2.04688 -1.63281 -0.714844 0.439453 -1.16406 -1.03906 -0.419922 2.01562
 0.769531 -1.72656]
logits (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits after select (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits 0 [3.39062 4.1875 1.9375 0.0771484 1.625 2.51562 3.54688 1.07031 1.23438
 1.10156]
logits 1 [3.39062 4.1875 1.9375 0.0771484 1.625 2.51562 3.54688 1.07031 1.23438
 1.10156]
logits 2 [3.39062 4.1875 1.9375 0.0771484 1.625 2.51562 3.54688 1.07031 1.23438
 1.10156]
logits 3 [3.39062 4.1875 1.9375 0.0771484 1.625 2.51562 3.54688 1.07031 1.23438
 1.10156]
logits 4 [3.39062 4.1875 1.9375 0.0771484 1.625 2.51562 3.54688 1.07031 1.23438
 1.10156]
logits 5 [3.39062 4.1875 1.9375 0.0771484 1.625 2.51562 3.54688 1.07031 1.23438
 1.10156]
logits 6 [3.39062 4.1875 1.9375 0.0771484 1.625 2.51562 3.54688 1.07031 1.23438
 1.10156]
logits 7 [3.39062 4.1875 1.9375 0.0771484 1.625 2.51562 3.54688 1.07031 1.23438
 1.10156]
logits 8 [5.15625 4.9375 2 4.34375 5 1.60156 3.78125 4.09375 3.4375 3.75]
logits 9 [3.39062 4.1875 1.9375 0.0771484 1.625 2.51562 3.54688 1.07031 1.23438
 1.10156]
logits 10 [3.39062 4.1875 1.9375 0.0771484 1.625 2.51562 3.54688 1.07031 1.23438
 1.10156]
logits 11 [3.39062 4.1875 1.9375 0.0771484 1.625 2.51562 3.54688 1.07031 1.23438
 1.10156]
next_tokens after sampling [220 220 220 220 220 220 220 220 220 220 220 220 220 220 220 220]
wenxin next_tokens [220 220 220 220 220 220 220 220 220 220 220 220 220 220 220 220]

self.max_num_reqs 256
dp_rank_reqs [['0'], ['1']]
dp_rank_tokens [[220], [220]]
cumsum_num_reqs_per_dp_rank [0 1 2]
padded_num_reqs_per_dp_rank 128
padded_num_reqs_logits 16
---- dp_rank 0 -----
dp_token_offset 0
dp_req_offset 0
dp_req_padded_offset 0
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [0]
arange [0]
positions_np [8]
self.input_batch.num_computed_tokens_cpu[req_indices] [9]
positions_np after add [9]
token_indices [9]
self.input_ids_cpu [220]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [10  0  0 ...  0  0  0]
dp_req_offset 0
dp_token_offset 0
slices_start [9]
self.input_batch.num_computed_tokens_cpu [9 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
global_block_start_idx [0]
slot_mapping_metadata [[73  0  1]]
num_slices [1, []]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [220   0   0 ...   0   0   0]
---- dp_rank 1 -----
dp_token_offset 16
dp_req_offset 1
dp_req_padded_offset 128
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [1]
arange [0]
positions_np [8]
self.input_batch.num_computed_tokens_cpu[req_indices] [9]
positions_np after add [9]
token_indices [1033]
self.input_ids_cpu [220]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [10  0  0 ...  0  0  0]
dp_req_offset 1
dp_token_offset 16
slices_start [9]
self.input_batch.num_computed_tokens_cpu [9 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
global_block_start_idx [16]
slot_mapping_metadata [[16393     0     1]]
num_slices [1, 1]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [220   0   0 ...   0   0   0]
---- --final start -- -----
input_ids (32,) [220   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 220   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
positions [9 1 2 3 4 5 0 0 0 0 0 0 0 0 0 0 9 1 2 3 4 5 0 0 0 0 0 0 0 0 0 0]
final slot_mapping_metadata (3, 32)
final slot_mapping_metadata [[   73     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0 16393     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    1     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     1     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]]
final slot_mapping_metadata []
block_tables [[1 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
query_start_loc [ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1 16  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0]
seq_lens [10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
logits_indices [ 0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0]
dp_rank_reqs [['0'], ['1']]
num_seqs [1 1]
---- --final finish-- -----
temp_tensor [-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  7.0064923e-45  8.4077908e-45  6.6710422e-30  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.3772934e-30  4.3646243e-41
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0995175e+13
  0.0000000e+00  0.0000000e+00  2.2420775e-44  0.0000000e+00
 -2.0683993e+15  2.2250378e-40  2.2420775e-44  0.0000000e+00
  4.6049755e-26  1.4012985e-45  0.0000000e+00  0.0000000e+00
  1.3772934e-30  4.3646243e-41  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  4.3644842e-41  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  1.2952631e-31  4.3646243e-41
  0.0000000e+00  0.0000000e+00  4.6758818e-30  0.0000000e+00
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  4.2038954e-45  0.0000000e+00
  2.3694278e-38  1.4012985e-45  1.4012985e-45  9.1835496e-41
  0.0000000e+00  5.6051939e-45  0.0000000e+00  2.3694278e-38
            nan  0.0000000e+00            nan  1.1210388e-44
  8.2284644e-38  0.0000000e+00            nan            nan
  0.0000000e+00  1.4012985e-45  5.7397185e-42  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1835496e-41  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1836897e-41  1.4012985e-45
  1.4012985e-45  7.0064923e-45  8.2284644e-38  0.0000000e+00
  0.0000000e+00  9.2195630e-41  8.2284644e-38  0.0000000e+00
  9.2195630e-41  9.1836897e-41  3.5873241e-43  0.0000000e+00
  1.1754944e-38  0.0000000e+00  1.1000000e+00  2.3509887e-38
            nan            nan  0.0000000e+00  3.5873241e-43
  2.1019477e-44  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.3312335e-43  2.3693561e-38  1.4012985e-40  0.0000000e+00
  9.1836897e-41  2.3509887e-38  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.4012985e-43  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  2.2420775e-44  0.0000000e+00  4.2038954e-45  3.5873241e-43
  2.3509887e-38  0.0000000e+00  2.2420775e-44  0.0000000e+00
  1.0000000e-01  7.0064923e-45  4.4841551e-44  1.4012985e-45
  1.4012985e-44  4.2038954e-44  4.4841551e-44  1.4012985e-45
            nan            nan  1.4012985e-45  0.0000000e+00
 -1.7014118e+38  0.0000000e+00  9.2194229e-41  2.3509887e-38
  2.3509887e-38  0.0000000e+00  0.0000000e+00  2.3510604e-38
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  8.2284644e-38  0.0000000e+00  2.8025969e-44  5.6051939e-44
  0.0000000e+00            nan            nan            nan]
top_k_tensor [        0         0         0         0         0         0         0
         0     -4096         0         0         0         0         0
         0         0     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1 233707072     31147 344830232         0     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707048     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -8192        -1     -8192        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707064     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1 233707032     31147
 344830208         0 233707032     31147 344830216         0     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1]
top_p_tensor [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan            nan  1.0248949e+11  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316673e-13 -3.0316488e-13
            nan            nan  1.2113646e+13  4.7644148e-44
            nan            nan -3.0360897e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4933738e+15  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0319264e-13 -3.0360897e-13
            nan            nan -3.0360897e-13 -3.1071440e-13
            nan            nan  1.7651560e+17  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -1.1368684e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
  1.4307163e-26  0.0000000e+00  0.0000000e+00  0.0000000e+00
            nan            nan  2.1188873e+19  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -1.3020833e-03
            nan            nan -3.0316532e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  2.6222406e+21  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0494818e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4012985e-45  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0320188e-13 -3.0316488e-13
            nan            nan  8.5479206e-44  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.6261203e+24  4.3643441e-41
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  1.4692446e-39  0.0000000e+00
            nan            nan  1.4012985e-44  0.0000000e+00
            nan            nan  1.4012985e-45 -1.1368685e-13
            nan            nan  0.0000000e+00  4.7644148e-44
            nan            nan -3.0316488e-13 -4.1268447e+26
            nan            nan -3.0316488e-13 -3.0494124e-13
            nan            nan -3.1027031e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327764e-13
            nan            nan  4.3148833e+29  4.7644148e-44
            nan            nan -3.0319264e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -4.4307661e+35
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan  5.0080864e+31  4.7644148e-44
            nan            nan -3.3158659e-13 -7.8066734e-11
            nan            nan -3.0316491e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327590e-13
            nan            nan  5.7511240e+33  4.7644148e-44
            nan            nan -3.0316499e-13 -4.1685183e-13
            nan            nan            nan -3.0316491e-13
            nan            nan -7.1696835e-03 -3.0316532e-13
            nan            nan -3.0319264e-13 -4.4307597e+35]
hidden_states after model_fn (32, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', None), memory_kind=device)
inputs[4] (16,) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(None,), memory_kind=device)
hidden_states after select (16, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(), memory_kind=device)
hidden_states 0 [-1.36719 -3.82812 0.507812 1.94531 2.60938 -5.1875 -0.675781 3.64062
 1.65625 -3.0625]
hidden_states 1 [-1.36719 -3.82812 0.507812 1.94531 2.60938 -5.1875 -0.675781 3.64062
 1.65625 -3.0625]
hidden_states 2 [-1.36719 -3.82812 0.507812 1.94531 2.60938 -5.1875 -0.675781 3.64062
 1.65625 -3.0625]
hidden_states 3 [-1.36719 -3.82812 0.507812 1.94531 2.60938 -5.1875 -0.675781 3.64062
 1.65625 -3.0625]
hidden_states 4 [-1.36719 -3.82812 0.507812 1.94531 2.60938 -5.1875 -0.675781 3.64062
 1.65625 -3.0625]
hidden_states 5 [-1.36719 -3.82812 0.507812 1.94531 2.60938 -5.1875 -0.675781 3.64062
 1.65625 -3.0625]
hidden_states 6 [-1.36719 -3.82812 0.507812 1.94531 2.60938 -5.1875 -0.675781 3.64062
 1.65625 -3.0625]
hidden_states 7 [-1.36719 -3.82812 0.507812 1.94531 2.60938 -5.1875 -0.675781 3.64062
 1.65625 -3.0625]
hidden_states 8 [-4.65625 -4.28125 0.757812 -0.0456543 2.75 1.38281 -4.5625 -2.51562
 -1.83594 0.886719]
hidden_states 9 [-1.36719 -3.82812 0.507812 1.94531 2.60938 -5.1875 -0.675781 3.64062
 1.65625 -3.0625]
hidden_states 10 [-1.36719 -3.82812 0.507812 1.94531 2.60938 -5.1875 -0.675781 3.64062
 1.65625 -3.0625]
hidden_states 11 [-1.36719 -3.82812 0.507812 1.94531 2.60938 -5.1875 -0.675781 3.64062
 1.65625 -3.0625]
logits (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits after select (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits 0 [-5.59375 -6.125 -3.3125 -5.6875 -5 -1.4375 -3.54688 -4.8125 -3.53125
 -4.03125]
logits 1 [-5.59375 -6.125 -3.3125 -5.6875 -5 -1.4375 -3.54688 -4.8125 -3.53125
 -4.03125]
logits 2 [-5.59375 -6.125 -3.3125 -5.6875 -5 -1.4375 -3.54688 -4.8125 -3.53125
 -4.03125]
logits 3 [-5.59375 -6.125 -3.3125 -5.6875 -5 -1.4375 -3.54688 -4.8125 -3.53125
 -4.03125]
logits 4 [-5.59375 -6.125 -3.3125 -5.6875 -5 -1.4375 -3.54688 -4.8125 -3.53125
 -4.03125]
logits 5 [-5.59375 -6.125 -3.3125 -5.6875 -5 -1.4375 -3.54688 -4.8125 -3.53125
 -4.03125]
logits 6 [-5.59375 -6.125 -3.3125 -5.6875 -5 -1.4375 -3.54688 -4.8125 -3.53125
 -4.03125]
logits 7 [-5.59375 -6.125 -3.3125 -5.6875 -5 -1.4375 -3.54688 -4.8125 -3.53125
 -4.03125]
logits 8 [0.408203 -4.5 0.988281 -5.03125 -0.976562 0.451172 -3.84375 -3.96875
 -2.71875 -3.42188]
logits 9 [-5.59375 -6.125 -3.3125 -5.6875 -5 -1.4375 -3.54688 -4.8125 -3.53125
 -4.03125]
logits 10 [-5.59375 -6.125 -3.3125 -5.6875 -5 -1.4375 -3.54688 -4.8125 -3.53125
 -4.03125]
logits 11 [-5.59375 -6.125 -3.3125 -5.6875 -5 -1.4375 -3.54688 -4.8125 -3.53125
 -4.03125]
next_tokens after sampling [1187 1187 1187 1187 1187 1187 1187 1187  220 1187 1187 1187 1187 1187
 1187 1187]
wenxin next_tokens [1187 1187 1187 1187 1187 1187 1187 1187  220 1187 1187 1187 1187 1187
 1187 1187]

self.max_num_reqs 256
dp_rank_reqs [['0'], ['1']]
dp_rank_tokens [[1187], [1187]]
cumsum_num_reqs_per_dp_rank [0 1 2]
padded_num_reqs_per_dp_rank 128
padded_num_reqs_logits 16
---- dp_rank 0 -----
dp_token_offset 0
dp_req_offset 0
dp_req_padded_offset 0
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [0]
arange [0]
positions_np [9]
self.input_batch.num_computed_tokens_cpu[req_indices] [10]
positions_np after add [10]
token_indices [10]
self.input_ids_cpu [1187]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [11  0  0 ...  0  0  0]
dp_req_offset 0
dp_token_offset 0
slices_start [10]
self.input_batch.num_computed_tokens_cpu [10 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [0]
slot_mapping_metadata [[74  0  1]]
num_slices [1, []]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [1187    0    0 ...    0    0    0]
---- dp_rank 1 -----
dp_token_offset 16
dp_req_offset 1
dp_req_padded_offset 128
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [1]
arange [0]
positions_np [9]
self.input_batch.num_computed_tokens_cpu[req_indices] [10]
positions_np after add [10]
token_indices [1034]
self.input_ids_cpu [1187]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [11  0  0 ...  0  0  0]
dp_req_offset 1
dp_token_offset 16
slices_start [10]
self.input_batch.num_computed_tokens_cpu [10 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [16]
slot_mapping_metadata [[16394     0     1]]
num_slices [1, 1]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [1187    0    0 ...    0    0    0]
---- --final start -- -----
input_ids (32,) [1187    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0 1187    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0]
positions [10  1  2  3  4  5  0  0  0  0  0  0  0  0  0  0 10  1  2  3  4  5  0  0
  0  0  0  0  0  0  0  0]
final slot_mapping_metadata (3, 32)
final slot_mapping_metadata [[   74     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0 16394     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    1     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     1     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]]
final slot_mapping_metadata []
block_tables [[1 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
query_start_loc [ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1 16  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0]
seq_lens [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
logits_indices [ 0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0]
dp_rank_reqs [['0'], ['1']]
num_seqs [1 1]
---- --final finish-- -----
temp_tensor [-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  7.0064923e-45  8.4077908e-45  6.6710422e-30  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.3772934e-30  4.3646243e-41
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0995175e+13
  0.0000000e+00  0.0000000e+00  2.2420775e-44  0.0000000e+00
 -2.0683993e+15  2.2250378e-40  2.2420775e-44  0.0000000e+00
  4.6049755e-26  1.4012985e-45  0.0000000e+00  0.0000000e+00
  1.3772934e-30  4.3646243e-41  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  4.3644842e-41  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  1.2952631e-31  4.3646243e-41
  0.0000000e+00  0.0000000e+00  4.6758818e-30  0.0000000e+00
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  4.2038954e-45  0.0000000e+00
  2.3694278e-38  1.4012985e-45  1.4012985e-45  9.1835496e-41
  0.0000000e+00  5.6051939e-45  0.0000000e+00  2.3694278e-38
            nan  0.0000000e+00            nan  1.1210388e-44
  8.2284644e-38  0.0000000e+00            nan            nan
  0.0000000e+00  1.4012985e-45  5.7397185e-42  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1835496e-41  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1836897e-41  1.4012985e-45
  1.4012985e-45  7.0064923e-45  8.2284644e-38  0.0000000e+00
  0.0000000e+00  9.2195630e-41  8.2284644e-38  0.0000000e+00
  9.2195630e-41  9.1836897e-41  3.5873241e-43  0.0000000e+00
  1.1754944e-38  0.0000000e+00  1.1000000e+00  2.3509887e-38
            nan            nan  0.0000000e+00  3.5873241e-43
  2.1019477e-44  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.3312335e-43  2.3693561e-38  1.4012985e-40  0.0000000e+00
  9.1836897e-41  2.3509887e-38  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.4012985e-43  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  2.2420775e-44  0.0000000e+00  4.2038954e-45  3.5873241e-43
  2.3509887e-38  0.0000000e+00  2.2420775e-44  0.0000000e+00
  1.0000000e-01  7.0064923e-45  4.4841551e-44  1.4012985e-45
  1.4012985e-44  4.2038954e-44  4.4841551e-44  1.4012985e-45
            nan            nan  1.4012985e-45  0.0000000e+00
 -1.7014118e+38  0.0000000e+00  9.2194229e-41  2.3509887e-38
  2.3509887e-38  0.0000000e+00  0.0000000e+00  2.3510604e-38
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  8.2284644e-38  0.0000000e+00  2.8025969e-44  5.6051939e-44
  0.0000000e+00            nan            nan            nan]
top_k_tensor [        0         0         0         0         0         0         0
         0     -4096         0         0         0         0         0
         0         0     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1 233707072     31147 344830232         0     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707048     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -8192        -1     -8192        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707064     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1 233707032     31147
 344830208         0 233707032     31147 344830216         0     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1]
top_p_tensor [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan            nan  1.0248949e+11  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316673e-13 -3.0316488e-13
            nan            nan  1.2113646e+13  4.7644148e-44
            nan            nan -3.0360897e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4933738e+15  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0319264e-13 -3.0360897e-13
            nan            nan -3.0360897e-13 -3.1071440e-13
            nan            nan  1.7651560e+17  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -1.1368684e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
  1.4307163e-26  0.0000000e+00  0.0000000e+00  0.0000000e+00
            nan            nan  2.1188873e+19  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -1.3020833e-03
            nan            nan -3.0316532e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  2.6222406e+21  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0494818e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4012985e-45  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0320188e-13 -3.0316488e-13
            nan            nan  8.5479206e-44  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.6261203e+24  4.3643441e-41
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  1.4692446e-39  0.0000000e+00
            nan            nan  1.4012985e-44  0.0000000e+00
            nan            nan  1.4012985e-45 -1.1368685e-13
            nan            nan  0.0000000e+00  4.7644148e-44
            nan            nan -3.0316488e-13 -4.1268447e+26
            nan            nan -3.0316488e-13 -3.0494124e-13
            nan            nan -3.1027031e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327764e-13
            nan            nan  4.3148833e+29  4.7644148e-44
            nan            nan -3.0319264e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -4.4307661e+35
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan  5.0080864e+31  4.7644148e-44
            nan            nan -3.3158659e-13 -7.8066734e-11
            nan            nan -3.0316491e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327590e-13
            nan            nan  5.7511240e+33  4.7644148e-44
            nan            nan -3.0316499e-13 -4.1685183e-13
            nan            nan            nan -3.0316491e-13
            nan            nan -7.1696835e-03 -3.0316532e-13
            nan            nan -3.0319264e-13 -4.4307597e+35]
hidden_states after model_fn (32, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', None), memory_kind=device)
inputs[4] (16,) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(None,), memory_kind=device)
hidden_states after select (16, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(), memory_kind=device)
hidden_states 0 [-0.738281 -2.23438 -1.6875 0.474609 0.226562 2.625 -3.34375 0.188477
 -1.75781 -2.39062]
hidden_states 1 [-0.738281 -2.23438 -1.6875 0.474609 0.226562 2.625 -3.34375 0.188477
 -1.75781 -2.39062]
hidden_states 2 [-0.738281 -2.23438 -1.6875 0.474609 0.226562 2.625 -3.34375 0.188477
 -1.75781 -2.39062]
hidden_states 3 [-0.738281 -2.23438 -1.6875 0.474609 0.226562 2.625 -3.34375 0.188477
 -1.75781 -2.39062]
hidden_states 4 [-0.738281 -2.23438 -1.6875 0.474609 0.226562 2.625 -3.34375 0.188477
 -1.75781 -2.39062]
hidden_states 5 [-0.738281 -2.23438 -1.6875 0.474609 0.226562 2.625 -3.34375 0.188477
 -1.75781 -2.39062]
hidden_states 6 [-0.738281 -2.23438 -1.6875 0.474609 0.226562 2.625 -3.34375 0.188477
 -1.75781 -2.39062]
hidden_states 7 [-0.738281 -2.23438 -1.6875 0.474609 0.226562 2.625 -3.34375 0.188477
 -1.75781 -2.39062]
hidden_states 8 [0.277344 0.353516 -0.00350952 1.03906 2.8125 5.90625 -6.03125 1.49219
 -1.28125 -0.800781]
hidden_states 9 [-0.738281 -2.23438 -1.6875 0.474609 0.226562 2.625 -3.34375 0.188477
 -1.75781 -2.39062]
hidden_states 10 [-0.738281 -2.23438 -1.6875 0.474609 0.226562 2.625 -3.34375 0.188477
 -1.75781 -2.39062]
hidden_states 11 [-0.738281 -2.23438 -1.6875 0.474609 0.226562 2.625 -3.34375 0.188477
 -1.75781 -2.39062]
logits (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits after select (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits 0 [9 5.4375 5.21875 0.226562 4.8125 4.90625 6.5 4.59375 4.375 3.90625]
logits 1 [9 5.4375 5.21875 0.226562 4.8125 4.90625 6.5 4.59375 4.375 3.90625]
logits 2 [9 5.4375 5.21875 0.226562 4.8125 4.90625 6.5 4.59375 4.375 3.90625]
logits 3 [9 5.4375 5.21875 0.226562 4.8125 4.90625 6.5 4.59375 4.375 3.90625]
logits 4 [9 5.4375 5.21875 0.226562 4.8125 4.90625 6.5 4.59375 4.375 3.90625]
logits 5 [9 5.4375 5.21875 0.226562 4.8125 4.90625 6.5 4.59375 4.375 3.90625]
logits 6 [9 5.4375 5.21875 0.226562 4.8125 4.90625 6.5 4.59375 4.375 3.90625]
logits 7 [9 5.4375 5.21875 0.226562 4.8125 4.90625 6.5 4.59375 4.375 3.90625]
logits 8 [7.5625 7.8125 4.8125 2.29688 5.0625 6.40625 7.46875 7.875 8.1875 6]
logits 9 [9 5.4375 5.21875 0.226562 4.8125 4.90625 6.5 4.59375 4.375 3.90625]
logits 10 [9 5.4375 5.21875 0.226562 4.8125 4.90625 6.5 4.59375 4.375 3.90625]
logits 11 [9 5.4375 5.21875 0.226562 4.8125 4.90625 6.5 4.59375 4.375 3.90625]
next_tokens after sampling [11 11 11 11 11 11 11 11 13 11 11 11 11 11 11 11]
wenxin next_tokens [11 11 11 11 11 11 11 11 13 11 11 11 11 11 11 11]

self.max_num_reqs 256
dp_rank_reqs [['0'], ['1']]
dp_rank_tokens [[11], [11]]
cumsum_num_reqs_per_dp_rank [0 1 2]
padded_num_reqs_per_dp_rank 128
padded_num_reqs_logits 16
---- dp_rank 0 -----
dp_token_offset 0
dp_req_offset 0
dp_req_padded_offset 0
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [0]
arange [0]
positions_np [10]
self.input_batch.num_computed_tokens_cpu[req_indices] [11]
positions_np after add [11]
token_indices [11]
self.input_ids_cpu [11]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [12  0  0 ...  0  0  0]
dp_req_offset 0
dp_token_offset 0
slices_start [11]
self.input_batch.num_computed_tokens_cpu [11 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [0]
slot_mapping_metadata [[75  0  1]]
num_slices [1, []]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [11  0  0 ...  0  0  0]
---- dp_rank 1 -----
dp_token_offset 16
dp_req_offset 1
dp_req_padded_offset 128
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [1]
arange [0]
positions_np [10]
self.input_batch.num_computed_tokens_cpu[req_indices] [11]
positions_np after add [11]
token_indices [1035]
self.input_ids_cpu [11]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [12  0  0 ...  0  0  0]
dp_req_offset 1
dp_token_offset 16
slices_start [11]
self.input_batch.num_computed_tokens_cpu [11 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [16]
slot_mapping_metadata [[16395     0     1]]
num_slices [1, 1]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [11  0  0 ...  0  0  0]
---- --final start -- -----
input_ids (32,) [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0]
positions [11  1  2  3  4  5  0  0  0  0  0  0  0  0  0  0 11  1  2  3  4  5  0  0
  0  0  0  0  0  0  0  0]
final slot_mapping_metadata (3, 32)
final slot_mapping_metadata [[   75     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0 16395     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    1     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     1     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]]
final slot_mapping_metadata []
block_tables [[1 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
query_start_loc [ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1 16  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0]
seq_lens [12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
logits_indices [ 0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0]
dp_rank_reqs [['0'], ['1']]
num_seqs [1 1]
---- --final finish-- -----
temp_tensor [-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  7.0064923e-45  8.4077908e-45  6.6710422e-30  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.3772934e-30  4.3646243e-41
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0995175e+13
  0.0000000e+00  0.0000000e+00  2.2420775e-44  0.0000000e+00
 -2.0683993e+15  2.2250378e-40  2.2420775e-44  0.0000000e+00
  4.6049755e-26  1.4012985e-45  0.0000000e+00  0.0000000e+00
  1.3772934e-30  4.3646243e-41  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  4.3644842e-41  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  1.2952631e-31  4.3646243e-41
  0.0000000e+00  0.0000000e+00  4.6758818e-30  0.0000000e+00
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  4.2038954e-45  0.0000000e+00
  2.3694278e-38  1.4012985e-45  1.4012985e-45  9.1835496e-41
  0.0000000e+00  5.6051939e-45  0.0000000e+00  2.3694278e-38
            nan  0.0000000e+00            nan  1.1210388e-44
  8.2284644e-38  0.0000000e+00            nan            nan
  0.0000000e+00  1.4012985e-45  5.7397185e-42  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1835496e-41  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1836897e-41  1.4012985e-45
  1.4012985e-45  7.0064923e-45  8.2284644e-38  0.0000000e+00
  0.0000000e+00  9.2195630e-41  8.2284644e-38  0.0000000e+00
  9.2195630e-41  9.1836897e-41  3.5873241e-43  0.0000000e+00
  1.1754944e-38  0.0000000e+00  1.1000000e+00  2.3509887e-38
            nan            nan  0.0000000e+00  3.5873241e-43
  2.1019477e-44  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.3312335e-43  2.3693561e-38  1.4012985e-40  0.0000000e+00
  9.1836897e-41  2.3509887e-38  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.4012985e-43  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  2.2420775e-44  0.0000000e+00  4.2038954e-45  3.5873241e-43
  2.3509887e-38  0.0000000e+00  2.2420775e-44  0.0000000e+00
  1.0000000e-01  7.0064923e-45  4.4841551e-44  1.4012985e-45
  1.4012985e-44  4.2038954e-44  4.4841551e-44  1.4012985e-45
            nan            nan  1.4012985e-45  0.0000000e+00
 -1.7014118e+38  0.0000000e+00  9.2194229e-41  2.3509887e-38
  2.3509887e-38  0.0000000e+00  0.0000000e+00  2.3510604e-38
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  8.2284644e-38  0.0000000e+00  2.8025969e-44  5.6051939e-44
  0.0000000e+00            nan            nan            nan]
top_k_tensor [        0         0         0         0         0         0         0
         0     -4096         0         0         0         0         0
         0         0     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1 233707072     31147 344830232         0     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707048     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -8192        -1     -8192        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707064     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1 233707032     31147
 344830208         0 233707032     31147 344830216         0     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1]
top_p_tensor [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan            nan  1.0248949e+11  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316673e-13 -3.0316488e-13
            nan            nan  1.2113646e+13  4.7644148e-44
            nan            nan -3.0360897e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4933738e+15  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0319264e-13 -3.0360897e-13
            nan            nan -3.0360897e-13 -3.1071440e-13
            nan            nan  1.7651560e+17  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -1.1368684e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
  1.4307163e-26  0.0000000e+00  0.0000000e+00  0.0000000e+00
            nan            nan  2.1188873e+19  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -1.3020833e-03
            nan            nan -3.0316532e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  2.6222406e+21  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0494818e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4012985e-45  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0320188e-13 -3.0316488e-13
            nan            nan  8.5479206e-44  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.6261203e+24  4.3643441e-41
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  1.4692446e-39  0.0000000e+00
            nan            nan  1.4012985e-44  0.0000000e+00
            nan            nan  1.4012985e-45 -1.1368685e-13
            nan            nan  0.0000000e+00  4.7644148e-44
            nan            nan -3.0316488e-13 -4.1268447e+26
            nan            nan -3.0316488e-13 -3.0494124e-13
            nan            nan -3.1027031e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327764e-13
            nan            nan  4.3148833e+29  4.7644148e-44
            nan            nan -3.0319264e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -4.4307661e+35
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan  5.0080864e+31  4.7644148e-44
            nan            nan -3.3158659e-13 -7.8066734e-11
            nan            nan -3.0316491e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327590e-13
            nan            nan  5.7511240e+33  4.7644148e-44
            nan            nan -3.0316499e-13 -4.1685183e-13
            nan            nan            nan -3.0316491e-13
            nan            nan -7.1696835e-03 -3.0316532e-13
            nan            nan -3.0319264e-13 -4.4307597e+35]
hidden_states after model_fn (32, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', None), memory_kind=device)
inputs[4] (16,) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(None,), memory_kind=device)
hidden_states after select (16, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(), memory_kind=device)
hidden_states 0 [-2.17188 -1.01562 0.421875 -0.761719 -0.0786133 1.15625 0.644531
 -0.984375 -0.554688 1.50781]
hidden_states 1 [-2.17188 -1.01562 0.421875 -0.761719 -0.0786133 1.15625 0.644531
 -0.984375 -0.554688 1.50781]
hidden_states 2 [-2.17188 -1.01562 0.421875 -0.761719 -0.0786133 1.15625 0.644531
 -0.984375 -0.554688 1.50781]
hidden_states 3 [-2.17188 -1.01562 0.421875 -0.761719 -0.0786133 1.15625 0.644531
 -0.984375 -0.554688 1.50781]
hidden_states 4 [-2.17188 -1.01562 0.421875 -0.761719 -0.0786133 1.15625 0.644531
 -0.984375 -0.554688 1.50781]
hidden_states 5 [-2.17188 -1.01562 0.421875 -0.761719 -0.0786133 1.15625 0.644531
 -0.984375 -0.554688 1.50781]
hidden_states 6 [-2.17188 -1.01562 0.421875 -0.761719 -0.0786133 1.15625 0.644531
 -0.984375 -0.554688 1.50781]
hidden_states 7 [-2.17188 -1.01562 0.421875 -0.761719 -0.0786133 1.15625 0.644531
 -0.984375 -0.554688 1.50781]
hidden_states 8 [-0.21875 -1.46094 -0.235352 1.05469 0.710938 0.441406 -4.6875 1 0.412109
 -2.25]
hidden_states 9 [-2.17188 -1.01562 0.421875 -0.761719 -0.0786133 1.15625 0.644531
 -0.984375 -0.554688 1.50781]
hidden_states 10 [-2.17188 -1.01562 0.421875 -0.761719 -0.0786133 1.15625 0.644531
 -0.984375 -0.554688 1.50781]
hidden_states 11 [-2.17188 -1.01562 0.421875 -0.761719 -0.0786133 1.15625 0.644531
 -0.984375 -0.554688 1.50781]
logits (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits after select (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits 0 [2.60938 3.14062 1.07031 3.14062 3.6875 3.59375 3.10938 4.0625 3.73438
 4.65625]
logits 1 [2.60938 3.14062 1.07031 3.14062 3.6875 3.59375 3.10938 4.0625 3.73438
 4.65625]
logits 2 [2.60938 3.14062 1.07031 3.14062 3.6875 3.59375 3.10938 4.0625 3.73438
 4.65625]
logits 3 [2.60938 3.14062 1.07031 3.14062 3.6875 3.59375 3.10938 4.0625 3.73438
 4.65625]
logits 4 [2.60938 3.14062 1.07031 3.14062 3.6875 3.59375 3.10938 4.0625 3.73438
 4.65625]
logits 5 [2.60938 3.14062 1.07031 3.14062 3.6875 3.59375 3.10938 4.0625 3.73438
 4.65625]
logits 6 [2.60938 3.14062 1.07031 3.14062 3.6875 3.59375 3.10938 4.0625 3.73438
 4.65625]
logits 7 [2.60938 3.14062 1.07031 3.14062 3.6875 3.59375 3.10938 4.0625 3.73438
 4.65625]
logits 8 [0.439453 -3.40625 -0.875 -3.25 -0.71875 -1.77344 -2.95312 -3.40625
 -0.113281 -2.875]
logits 9 [2.60938 3.14062 1.07031 3.14062 3.6875 3.59375 3.10938 4.0625 3.73438
 4.65625]
logits 10 [2.60938 3.14062 1.07031 3.14062 3.6875 3.59375 3.10938 4.0625 3.73438
 4.65625]
logits 11 [2.60938 3.14062 1.07031 3.14062 3.6875 3.59375 3.10938 4.0625 3.73438
 4.65625]
next_tokens after sampling [12592 12592 12592 12592 12592 12592 12592 12592   719 12592 12592 12592
 12592 12592 12592 12592]
wenxin next_tokens [12592 12592 12592 12592 12592 12592 12592 12592   719 12592 12592 12592
 12592 12592 12592 12592]

self.max_num_reqs 256
dp_rank_reqs [['0'], ['1']]
dp_rank_tokens [[12592], [12592]]
cumsum_num_reqs_per_dp_rank [0 1 2]
padded_num_reqs_per_dp_rank 128
padded_num_reqs_logits 16
---- dp_rank 0 -----
dp_token_offset 0
dp_req_offset 0
dp_req_padded_offset 0
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [0]
arange [0]
positions_np [11]
self.input_batch.num_computed_tokens_cpu[req_indices] [12]
positions_np after add [12]
token_indices [12]
self.input_ids_cpu [12592]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [13  0  0 ...  0  0  0]
dp_req_offset 0
dp_token_offset 0
slices_start [12]
self.input_batch.num_computed_tokens_cpu [12 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [0]
slot_mapping_metadata [[76  0  1]]
num_slices [1, []]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [12592     0     0 ...     0     0     0]
---- dp_rank 1 -----
dp_token_offset 16
dp_req_offset 1
dp_req_padded_offset 128
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [1]
arange [0]
positions_np [11]
self.input_batch.num_computed_tokens_cpu[req_indices] [12]
positions_np after add [12]
token_indices [1036]
self.input_ids_cpu [12592]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [13  0  0 ...  0  0  0]
dp_req_offset 1
dp_token_offset 16
slices_start [12]
self.input_batch.num_computed_tokens_cpu [12 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [16]
slot_mapping_metadata [[16396     0     1]]
num_slices [1, 1]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [12592     0     0 ...     0     0     0]
---- --final start -- -----
input_ids (32,) [12592     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0 12592     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0]
positions [12  1  2  3  4  5  0  0  0  0  0  0  0  0  0  0 12  1  2  3  4  5  0  0
  0  0  0  0  0  0  0  0]
final slot_mapping_metadata (3, 32)
final slot_mapping_metadata [[   76     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0 16396     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    1     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     1     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]]
final slot_mapping_metadata []
block_tables [[1 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
query_start_loc [ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1 16  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0]
seq_lens [13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0 13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
logits_indices [ 0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0]
dp_rank_reqs [['0'], ['1']]
num_seqs [1 1]
---- --final finish-- -----
temp_tensor [-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  7.0064923e-45  8.4077908e-45  6.6710422e-30  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.3772934e-30  4.3646243e-41
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0995175e+13
  0.0000000e+00  0.0000000e+00  2.2420775e-44  0.0000000e+00
 -2.0683993e+15  2.2250378e-40  2.2420775e-44  0.0000000e+00
  4.6049755e-26  1.4012985e-45  0.0000000e+00  0.0000000e+00
  1.3772934e-30  4.3646243e-41  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  4.3644842e-41  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  1.2952631e-31  4.3646243e-41
  0.0000000e+00  0.0000000e+00  4.6758818e-30  0.0000000e+00
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  4.2038954e-45  0.0000000e+00
  2.3694278e-38  1.4012985e-45  1.4012985e-45  9.1835496e-41
  0.0000000e+00  5.6051939e-45  0.0000000e+00  2.3694278e-38
            nan  0.0000000e+00            nan  1.1210388e-44
  8.2284644e-38  0.0000000e+00            nan            nan
  0.0000000e+00  1.4012985e-45  5.7397185e-42  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1835496e-41  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1836897e-41  1.4012985e-45
  1.4012985e-45  7.0064923e-45  8.2284644e-38  0.0000000e+00
  0.0000000e+00  9.2195630e-41  8.2284644e-38  0.0000000e+00
  9.2195630e-41  9.1836897e-41  3.5873241e-43  0.0000000e+00
  1.1754944e-38  0.0000000e+00  1.1000000e+00  2.3509887e-38
            nan            nan  0.0000000e+00  3.5873241e-43
  2.1019477e-44  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.3312335e-43  2.3693561e-38  1.4012985e-40  0.0000000e+00
  9.1836897e-41  2.3509887e-38  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.4012985e-43  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  2.2420775e-44  0.0000000e+00  4.2038954e-45  3.5873241e-43
  2.3509887e-38  0.0000000e+00  2.2420775e-44  0.0000000e+00
  1.0000000e-01  7.0064923e-45  4.4841551e-44  1.4012985e-45
  1.4012985e-44  4.2038954e-44  4.4841551e-44  1.4012985e-45
            nan            nan  1.4012985e-45  0.0000000e+00
 -1.7014118e+38  0.0000000e+00  9.2194229e-41  2.3509887e-38
  2.3509887e-38  0.0000000e+00  0.0000000e+00  2.3510604e-38
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  8.2284644e-38  0.0000000e+00  2.8025969e-44  5.6051939e-44
  0.0000000e+00            nan            nan            nan]
top_k_tensor [        0         0         0         0         0         0         0
         0     -4096         0         0         0         0         0
         0         0     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1 233707072     31147 344830232         0     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707048     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -8192        -1     -8192        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707064     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1 233707032     31147
 344830208         0 233707032     31147 344830216         0     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1]
top_p_tensor [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan            nan  1.0248949e+11  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316673e-13 -3.0316488e-13
            nan            nan  1.2113646e+13  4.7644148e-44
            nan            nan -3.0360897e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4933738e+15  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0319264e-13 -3.0360897e-13
            nan            nan -3.0360897e-13 -3.1071440e-13
            nan            nan  1.7651560e+17  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -1.1368684e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
  1.4307163e-26  0.0000000e+00  0.0000000e+00  0.0000000e+00
            nan            nan  2.1188873e+19  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -1.3020833e-03
            nan            nan -3.0316532e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  2.6222406e+21  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0494818e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4012985e-45  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0320188e-13 -3.0316488e-13
            nan            nan  8.5479206e-44  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.6261203e+24  4.3643441e-41
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  1.4692446e-39  0.0000000e+00
            nan            nan  1.4012985e-44  0.0000000e+00
            nan            nan  1.4012985e-45 -1.1368685e-13
            nan            nan  0.0000000e+00  4.7644148e-44
            nan            nan -3.0316488e-13 -4.1268447e+26
            nan            nan -3.0316488e-13 -3.0494124e-13
            nan            nan -3.1027031e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327764e-13
            nan            nan  4.3148833e+29  4.7644148e-44
            nan            nan -3.0319264e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -4.4307661e+35
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan  5.0080864e+31  4.7644148e-44
            nan            nan -3.3158659e-13 -7.8066734e-11
            nan            nan -3.0316491e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327590e-13
            nan            nan  5.7511240e+33  4.7644148e-44
            nan            nan -3.0316499e-13 -4.1685183e-13
            nan            nan            nan -3.0316491e-13
            nan            nan -7.1696835e-03 -3.0316532e-13
            nan            nan -3.0319264e-13 -4.4307597e+35]
hidden_states after model_fn (32, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', None), memory_kind=device)
inputs[4] (16,) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(None,), memory_kind=device)
hidden_states after select (16, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(), memory_kind=device)
hidden_states 0 [-0.412109 0.265625 -1.46094 -0.625 2.51562 1.69531 -0.753906 -2.79688
 1.97656 -1.86719]
hidden_states 1 [-0.412109 0.265625 -1.46094 -0.625 2.51562 1.69531 -0.753906 -2.79688
 1.97656 -1.86719]
hidden_states 2 [-0.412109 0.265625 -1.46094 -0.625 2.51562 1.69531 -0.753906 -2.79688
 1.97656 -1.86719]
hidden_states 3 [-0.412109 0.265625 -1.46094 -0.625 2.51562 1.69531 -0.753906 -2.79688
 1.97656 -1.86719]
hidden_states 4 [-0.412109 0.265625 -1.46094 -0.625 2.51562 1.69531 -0.753906 -2.79688
 1.97656 -1.86719]
hidden_states 5 [-0.412109 0.265625 -1.46094 -0.625 2.51562 1.69531 -0.753906 -2.79688
 1.97656 -1.86719]
hidden_states 6 [-0.412109 0.265625 -1.46094 -0.625 2.51562 1.69531 -0.753906 -2.79688
 1.97656 -1.86719]
hidden_states 7 [-0.412109 0.265625 -1.46094 -0.625 2.51562 1.69531 -0.753906 -2.79688
 1.97656 -1.86719]
hidden_states 8 [1.88281 -2.89062 1.61719 1.8125 5.78125 0.796875 -1.07031 0.189453
 0.12793 -1.52344]
hidden_states 9 [-0.412109 0.265625 -1.46094 -0.625 2.51562 1.69531 -0.753906 -2.79688
 1.97656 -1.86719]
hidden_states 10 [-0.412109 0.265625 -1.46094 -0.625 2.51562 1.69531 -0.753906 -2.79688
 1.97656 -1.86719]
hidden_states 11 [-0.412109 0.265625 -1.46094 -0.625 2.51562 1.69531 -0.753906 -2.79688
 1.97656 -1.86719]
logits (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits after select (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits 0 [4.40625 4.375 5.40625 0.527344 4.65625 3.5625 4.90625 5.84375 4.9375 3.5]
logits 1 [4.40625 4.375 5.40625 0.527344 4.65625 3.5625 4.90625 5.84375 4.9375 3.5]
logits 2 [4.40625 4.375 5.40625 0.527344 4.65625 3.5625 4.90625 5.84375 4.9375 3.5]
logits 3 [4.40625 4.375 5.40625 0.527344 4.65625 3.5625 4.90625 5.84375 4.9375 3.5]
logits 4 [4.40625 4.375 5.40625 0.527344 4.65625 3.5625 4.90625 5.84375 4.9375 3.5]
logits 5 [4.40625 4.375 5.40625 0.527344 4.65625 3.5625 4.90625 5.84375 4.9375 3.5]
logits 6 [4.40625 4.375 5.40625 0.527344 4.65625 3.5625 4.90625 5.84375 4.9375 3.5]
logits 7 [4.40625 4.375 5.40625 0.527344 4.65625 3.5625 4.90625 5.84375 4.9375 3.5]
logits 8 [3.34375 4.6875 1.32031 -0.65625 2.15625 0.316406 1.48438 4.46875 3.29688
 1.5]
logits 9 [4.40625 4.375 5.40625 0.527344 4.65625 3.5625 4.90625 5.84375 4.9375 3.5]
logits 10 [4.40625 4.375 5.40625 0.527344 4.65625 3.5625 4.90625 5.84375 4.9375 3.5]
logits 11 [4.40625 4.375 5.40625 0.527344 4.65625 3.5625 4.90625 5.84375 4.9375 3.5]
next_tokens after sampling [ 11  11  11  11  11  11  11  11 198  11  11  11  11  11  11  11]
wenxin next_tokens [ 11  11  11  11  11  11  11  11 198  11  11  11  11  11  11  11]

self.max_num_reqs 256
dp_rank_reqs [['0'], ['1']]
dp_rank_tokens [[11], [11]]
cumsum_num_reqs_per_dp_rank [0 1 2]
padded_num_reqs_per_dp_rank 128
padded_num_reqs_logits 16
---- dp_rank 0 -----
dp_token_offset 0
dp_req_offset 0
dp_req_padded_offset 0
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [0]
arange [0]
positions_np [12]
self.input_batch.num_computed_tokens_cpu[req_indices] [13]
positions_np after add [13]
token_indices [13]
self.input_ids_cpu [11]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [14  0  0 ...  0  0  0]
dp_req_offset 0
dp_token_offset 0
slices_start [13]
self.input_batch.num_computed_tokens_cpu [13 13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [0]
slot_mapping_metadata [[77  0  1]]
num_slices [1, []]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [11  0  0 ...  0  0  0]
---- dp_rank 1 -----
dp_token_offset 16
dp_req_offset 1
dp_req_padded_offset 128
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [1]
arange [0]
positions_np [12]
self.input_batch.num_computed_tokens_cpu[req_indices] [13]
positions_np after add [13]
token_indices [1037]
self.input_ids_cpu [11]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [14  0  0 ...  0  0  0]
dp_req_offset 1
dp_token_offset 16
slices_start [13]
self.input_batch.num_computed_tokens_cpu [13 13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [16]
slot_mapping_metadata [[16397     0     1]]
num_slices [1, 1]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [11  0  0 ...  0  0  0]
---- --final start -- -----
input_ids (32,) [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0]
positions [13  1  2  3  4  5  0  0  0  0  0  0  0  0  0  0 13  1  2  3  4  5  0  0
  0  0  0  0  0  0  0  0]
final slot_mapping_metadata (3, 32)
final slot_mapping_metadata [[   77     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0 16397     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    1     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     1     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]]
final slot_mapping_metadata []
block_tables [[1 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
query_start_loc [ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1 16  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0]
seq_lens [14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
logits_indices [ 0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0]
dp_rank_reqs [['0'], ['1']]
num_seqs [1 1]
---- --final finish-- -----
temp_tensor [-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  7.0064923e-45  8.4077908e-45  6.6710422e-30  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.3772934e-30  4.3646243e-41
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0995175e+13
  0.0000000e+00  0.0000000e+00  2.2420775e-44  0.0000000e+00
 -2.0683993e+15  2.2250378e-40  2.2420775e-44  0.0000000e+00
  4.6049755e-26  1.4012985e-45  0.0000000e+00  0.0000000e+00
  1.3772934e-30  4.3646243e-41  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  4.3644842e-41  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  1.2952631e-31  4.3646243e-41
  0.0000000e+00  0.0000000e+00  4.6758818e-30  0.0000000e+00
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  4.2038954e-45  0.0000000e+00
  2.3694278e-38  1.4012985e-45  1.4012985e-45  9.1835496e-41
  0.0000000e+00  5.6051939e-45  0.0000000e+00  2.3694278e-38
            nan  0.0000000e+00            nan  1.1210388e-44
  8.2284644e-38  0.0000000e+00            nan            nan
  0.0000000e+00  1.4012985e-45  5.7397185e-42  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1835496e-41  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1836897e-41  1.4012985e-45
  1.4012985e-45  7.0064923e-45  8.2284644e-38  0.0000000e+00
  0.0000000e+00  9.2195630e-41  8.2284644e-38  0.0000000e+00
  9.2195630e-41  9.1836897e-41  3.5873241e-43  0.0000000e+00
  1.1754944e-38  0.0000000e+00  1.1000000e+00  2.3509887e-38
            nan            nan  0.0000000e+00  3.5873241e-43
  2.1019477e-44  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.3312335e-43  2.3693561e-38  1.4012985e-40  0.0000000e+00
  9.1836897e-41  2.3509887e-38  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.4012985e-43  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  2.2420775e-44  0.0000000e+00  4.2038954e-45  3.5873241e-43
  2.3509887e-38  0.0000000e+00  2.2420775e-44  0.0000000e+00
  1.0000000e-01  7.0064923e-45  4.4841551e-44  1.4012985e-45
  1.4012985e-44  4.2038954e-44  4.4841551e-44  1.4012985e-45
            nan            nan  1.4012985e-45  0.0000000e+00
 -1.7014118e+38  0.0000000e+00  9.2194229e-41  2.3509887e-38
  2.3509887e-38  0.0000000e+00  0.0000000e+00  2.3510604e-38
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  8.2284644e-38  0.0000000e+00  2.8025969e-44  5.6051939e-44
  0.0000000e+00            nan            nan            nan]
top_k_tensor [        0         0         0         0         0         0         0
         0     -4096         0         0         0         0         0
         0         0     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1 233707072     31147 344830232         0     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707048     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -8192        -1     -8192        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707064     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1 233707032     31147
 344830208         0 233707032     31147 344830216         0     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1]
top_p_tensor [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan            nan  1.0248949e+11  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316673e-13 -3.0316488e-13
            nan            nan  1.2113646e+13  4.7644148e-44
            nan            nan -3.0360897e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4933738e+15  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0319264e-13 -3.0360897e-13
            nan            nan -3.0360897e-13 -3.1071440e-13
            nan            nan  1.7651560e+17  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -1.1368684e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
  1.4307163e-26  0.0000000e+00  0.0000000e+00  0.0000000e+00
            nan            nan  2.1188873e+19  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -1.3020833e-03
            nan            nan -3.0316532e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  2.6222406e+21  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0494818e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4012985e-45  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0320188e-13 -3.0316488e-13
            nan            nan  8.5479206e-44  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.6261203e+24  4.3643441e-41
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  1.4692446e-39  0.0000000e+00
            nan            nan  1.4012985e-44  0.0000000e+00
            nan            nan  1.4012985e-45 -1.1368685e-13
            nan            nan  0.0000000e+00  4.7644148e-44
            nan            nan -3.0316488e-13 -4.1268447e+26
            nan            nan -3.0316488e-13 -3.0494124e-13
            nan            nan -3.1027031e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327764e-13
            nan            nan  4.3148833e+29  4.7644148e-44
            nan            nan -3.0319264e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -4.4307661e+35
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan  5.0080864e+31  4.7644148e-44
            nan            nan -3.3158659e-13 -7.8066734e-11
            nan            nan -3.0316491e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327590e-13
            nan            nan  5.7511240e+33  4.7644148e-44
            nan            nan -3.0316499e-13 -4.1685183e-13
            nan            nan            nan -3.0316491e-13
            nan            nan -7.1696835e-03 -3.0316532e-13
            nan            nan -3.0319264e-13 -4.4307597e+35]
hidden_states after model_fn (32, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', None), memory_kind=device)
inputs[4] (16,) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(None,), memory_kind=device)
hidden_states after select (16, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(), memory_kind=device)
hidden_states 0 [-0.597656 -1.60156 0.131836 -2.20312 0.71875 2.5625 0.550781 0.347656
 -1.42969 -1.45312]
hidden_states 1 [-0.597656 -1.60156 0.131836 -2.20312 0.71875 2.5625 0.550781 0.347656
 -1.42969 -1.45312]
hidden_states 2 [-0.597656 -1.60156 0.131836 -2.20312 0.71875 2.5625 0.550781 0.347656
 -1.42969 -1.45312]
hidden_states 3 [-0.597656 -1.60156 0.131836 -2.20312 0.71875 2.5625 0.550781 0.347656
 -1.42969 -1.45312]
hidden_states 4 [-0.597656 -1.60156 0.131836 -2.20312 0.71875 2.5625 0.550781 0.347656
 -1.42969 -1.45312]
hidden_states 5 [-0.597656 -1.60156 0.131836 -2.20312 0.71875 2.5625 0.550781 0.347656
 -1.42969 -1.45312]
hidden_states 6 [-0.597656 -1.60156 0.131836 -2.20312 0.71875 2.5625 0.550781 0.347656
 -1.42969 -1.45312]
hidden_states 7 [-0.597656 -1.60156 0.131836 -2.20312 0.71875 2.5625 0.550781 0.347656
 -1.42969 -1.45312]
hidden_states 8 [2.14062 -0.78125 2.40625 0.111328 -0.570312 1.33594 -2.5625 -2.125
 -0.710938 -0.585938]
hidden_states 9 [-0.597656 -1.60156 0.131836 -2.20312 0.71875 2.5625 0.550781 0.347656
 -1.42969 -1.45312]
hidden_states 10 [-0.597656 -1.60156 0.131836 -2.20312 0.71875 2.5625 0.550781 0.347656
 -1.42969 -1.45312]
hidden_states 11 [-0.597656 -1.60156 0.131836 -2.20312 0.71875 2.5625 0.550781 0.347656
 -1.42969 -1.45312]
logits (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits after select (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits 0 [6.3125 4.4375 4.40625 4.4375 5.90625 4.40625 4.3125 5.78125 5.46875
 4.34375]
logits 1 [6.3125 4.4375 4.40625 4.4375 5.90625 4.40625 4.3125 5.78125 5.46875
 4.34375]
logits 2 [6.3125 4.4375 4.40625 4.4375 5.90625 4.40625 4.3125 5.78125 5.46875
 4.34375]
logits 3 [6.3125 4.4375 4.40625 4.4375 5.90625 4.40625 4.3125 5.78125 5.46875
 4.34375]
logits 4 [6.3125 4.4375 4.40625 4.4375 5.90625 4.40625 4.3125 5.78125 5.46875
 4.34375]
logits 5 [6.3125 4.4375 4.40625 4.4375 5.90625 4.40625 4.3125 5.78125 5.46875
 4.34375]
logits 6 [6.3125 4.4375 4.40625 4.4375 5.90625 4.40625 4.3125 5.78125 5.46875
 4.34375]
logits 7 [6.3125 4.4375 4.40625 4.4375 5.90625 4.40625 4.3125 5.78125 5.46875
 4.34375]
logits 8 [2.64062 -1.24219 1.57812 -1.50781 0.0722656 1.23438 -2.57812 -1.25781
 -1.98438 -0.617188]
logits 9 [6.3125 4.4375 4.40625 4.4375 5.90625 4.40625 4.3125 5.78125 5.46875
 4.34375]
logits 10 [6.3125 4.4375 4.40625 4.4375 5.90625 4.40625 4.3125 5.78125 5.46875
 4.34375]
logits 11 [6.3125 4.4375 4.40625 4.4375 5.90625 4.40625 4.3125 5.78125 5.46875
 4.34375]
next_tokens after sampling [198 198 198 198 198 198 198 198 279 198 198 198 198 198 198 198]
wenxin next_tokens [198 198 198 198 198 198 198 198 279 198 198 198 198 198 198 198]

self.max_num_reqs 256
dp_rank_reqs [['0'], ['1']]
dp_rank_tokens [[198], [198]]
cumsum_num_reqs_per_dp_rank [0 1 2]
padded_num_reqs_per_dp_rank 128
padded_num_reqs_logits 16
---- dp_rank 0 -----
dp_token_offset 0
dp_req_offset 0
dp_req_padded_offset 0
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [0]
arange [0]
positions_np [13]
self.input_batch.num_computed_tokens_cpu[req_indices] [14]
positions_np after add [14]
token_indices [14]
self.input_ids_cpu [198]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [15  0  0 ...  0  0  0]
dp_req_offset 0
dp_token_offset 0
slices_start [14]
self.input_batch.num_computed_tokens_cpu [14 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [0]
slot_mapping_metadata [[78  0  1]]
num_slices [1, []]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [198   0   0 ...   0   0   0]
---- dp_rank 1 -----
dp_token_offset 16
dp_req_offset 1
dp_req_padded_offset 128
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [1]
arange [0]
positions_np [13]
self.input_batch.num_computed_tokens_cpu[req_indices] [14]
positions_np after add [14]
token_indices [1038]
self.input_ids_cpu [198]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [15  0  0 ...  0  0  0]
dp_req_offset 1
dp_token_offset 16
slices_start [14]
self.input_batch.num_computed_tokens_cpu [14 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [16]
slot_mapping_metadata [[16398     0     1]]
num_slices [1, 1]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [198   0   0 ...   0   0   0]
---- --final start -- -----
input_ids (32,) [198   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 198   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
positions [14  1  2  3  4  5  0  0  0  0  0  0  0  0  0  0 14  1  2  3  4  5  0  0
  0  0  0  0  0  0  0  0]
final slot_mapping_metadata (3, 32)
final slot_mapping_metadata [[   78     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0 16398     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    1     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     1     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]]
final slot_mapping_metadata []
block_tables [[1 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
query_start_loc [ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1 16  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0]
seq_lens [15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0 15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
logits_indices [ 0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0]
dp_rank_reqs [['0'], ['1']]
num_seqs [1 1]
---- --final finish-- -----
temp_tensor [-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  7.0064923e-45  8.4077908e-45  6.6710422e-30  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.3772934e-30  4.3646243e-41
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0995175e+13
  0.0000000e+00  0.0000000e+00  2.2420775e-44  0.0000000e+00
 -2.0683993e+15  2.2250378e-40  2.2420775e-44  0.0000000e+00
  4.6049755e-26  1.4012985e-45  0.0000000e+00  0.0000000e+00
  1.3772934e-30  4.3646243e-41  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  4.3644842e-41  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  1.2952631e-31  4.3646243e-41
  0.0000000e+00  0.0000000e+00  4.6758818e-30  0.0000000e+00
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  4.2038954e-45  0.0000000e+00
  2.3694278e-38  1.4012985e-45  1.4012985e-45  9.1835496e-41
  0.0000000e+00  5.6051939e-45  0.0000000e+00  2.3694278e-38
            nan  0.0000000e+00            nan  1.1210388e-44
  8.2284644e-38  0.0000000e+00            nan            nan
  0.0000000e+00  1.4012985e-45  5.7397185e-42  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1835496e-41  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1836897e-41  1.4012985e-45
  1.4012985e-45  7.0064923e-45  8.2284644e-38  0.0000000e+00
  0.0000000e+00  9.2195630e-41  8.2284644e-38  0.0000000e+00
  9.2195630e-41  9.1836897e-41  3.5873241e-43  0.0000000e+00
  1.1754944e-38  0.0000000e+00  1.1000000e+00  2.3509887e-38
            nan            nan  0.0000000e+00  3.5873241e-43
  2.1019477e-44  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.3312335e-43  2.3693561e-38  1.4012985e-40  0.0000000e+00
  9.1836897e-41  2.3509887e-38  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.4012985e-43  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  2.2420775e-44  0.0000000e+00  4.2038954e-45  3.5873241e-43
  2.3509887e-38  0.0000000e+00  2.2420775e-44  0.0000000e+00
  1.0000000e-01  7.0064923e-45  4.4841551e-44  1.4012985e-45
  1.4012985e-44  4.2038954e-44  4.4841551e-44  1.4012985e-45
            nan            nan  1.4012985e-45  0.0000000e+00
 -1.7014118e+38  0.0000000e+00  9.2194229e-41  2.3509887e-38
  2.3509887e-38  0.0000000e+00  0.0000000e+00  2.3510604e-38
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  8.2284644e-38  0.0000000e+00  2.8025969e-44  5.6051939e-44
  0.0000000e+00            nan            nan            nan]
top_k_tensor [        0         0         0         0         0         0         0
         0     -4096         0         0         0         0         0
         0         0     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1 233707072     31147 344830232         0     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707048     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -8192        -1     -8192        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707064     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1 233707032     31147
 344830208         0 233707032     31147 344830216         0     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1]
top_p_tensor [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan            nan  1.0248949e+11  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316673e-13 -3.0316488e-13
            nan            nan  1.2113646e+13  4.7644148e-44
            nan            nan -3.0360897e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4933738e+15  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0319264e-13 -3.0360897e-13
            nan            nan -3.0360897e-13 -3.1071440e-13
            nan            nan  1.7651560e+17  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -1.1368684e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
  1.4307163e-26  0.0000000e+00  0.0000000e+00  0.0000000e+00
            nan            nan  2.1188873e+19  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -1.3020833e-03
            nan            nan -3.0316532e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  2.6222406e+21  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0494818e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4012985e-45  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0320188e-13 -3.0316488e-13
            nan            nan  8.5479206e-44  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.6261203e+24  4.3643441e-41
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  1.4692446e-39  0.0000000e+00
            nan            nan  1.4012985e-44  0.0000000e+00
            nan            nan  1.4012985e-45 -1.1368685e-13
            nan            nan  0.0000000e+00  4.7644148e-44
            nan            nan -3.0316488e-13 -4.1268447e+26
            nan            nan -3.0316488e-13 -3.0494124e-13
            nan            nan -3.1027031e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327764e-13
            nan            nan  4.3148833e+29  4.7644148e-44
            nan            nan -3.0319264e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -4.4307661e+35
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan  5.0080864e+31  4.7644148e-44
            nan            nan -3.3158659e-13 -7.8066734e-11
            nan            nan -3.0316491e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327590e-13
            nan            nan  5.7511240e+33  4.7644148e-44
            nan            nan -3.0316499e-13 -4.1685183e-13
            nan            nan            nan -3.0316491e-13
            nan            nan -7.1696835e-03 -3.0316532e-13
            nan            nan -3.0319264e-13 -4.4307597e+35]
hidden_states after model_fn (32, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', None), memory_kind=device)
inputs[4] (16,) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(None,), memory_kind=device)
hidden_states after select (16, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(), memory_kind=device)
hidden_states 0 [-2.26562 -5.5625 -0.808594 -0.382812 3.4375 0.902344 -0.929688 -0.126953
 1.45312 -4.5]
hidden_states 1 [-2.26562 -5.5625 -0.808594 -0.382812 3.4375 0.902344 -0.929688 -0.126953
 1.45312 -4.5]
hidden_states 2 [-2.26562 -5.5625 -0.808594 -0.382812 3.4375 0.902344 -0.929688 -0.126953
 1.45312 -4.5]
hidden_states 3 [-2.26562 -5.5625 -0.808594 -0.382812 3.4375 0.902344 -0.929688 -0.126953
 1.45312 -4.5]
hidden_states 4 [-2.26562 -5.5625 -0.808594 -0.382812 3.4375 0.902344 -0.929688 -0.126953
 1.45312 -4.5]
hidden_states 5 [-2.26562 -5.5625 -0.808594 -0.382812 3.4375 0.902344 -0.929688 -0.126953
 1.45312 -4.5]
hidden_states 6 [-2.26562 -5.5625 -0.808594 -0.382812 3.4375 0.902344 -0.929688 -0.126953
 1.45312 -4.5]
hidden_states 7 [-2.26562 -5.5625 -0.808594 -0.382812 3.4375 0.902344 -0.929688 -0.126953
 1.45312 -4.5]
hidden_states 8 [-2.67188 -0.302734 -0.228516 0.392578 0.386719 -0.196289 -2.60938
 0.0120239 -1.51562 -2.1875]
hidden_states 9 [-2.26562 -5.5625 -0.808594 -0.382812 3.4375 0.902344 -0.929688 -0.126953
 1.45312 -4.5]
hidden_states 10 [-2.26562 -5.5625 -0.808594 -0.382812 3.4375 0.902344 -0.929688 -0.126953
 1.45312 -4.5]
hidden_states 11 [-2.26562 -5.5625 -0.808594 -0.382812 3.4375 0.902344 -0.929688 -0.126953
 1.45312 -4.5]
logits (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits after select (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits 0 [7.75 7.09375 8.0625 7.65625 7.0625 5.6875 5.625 10.875 4.59375 10.75]
logits 1 [7.75 7.09375 8.0625 7.65625 7.0625 5.6875 5.625 10.875 4.59375 10.75]
logits 2 [7.75 7.09375 8.0625 7.65625 7.0625 5.6875 5.625 10.875 4.59375 10.75]
logits 3 [7.75 7.09375 8.0625 7.65625 7.0625 5.6875 5.625 10.875 4.59375 10.75]
logits 4 [7.75 7.09375 8.0625 7.65625 7.0625 5.6875 5.625 10.875 4.59375 10.75]
logits 5 [7.75 7.09375 8.0625 7.65625 7.0625 5.6875 5.625 10.875 4.59375 10.75]
logits 6 [7.75 7.09375 8.0625 7.65625 7.0625 5.6875 5.625 10.875 4.59375 10.75]
logits 7 [7.75 7.09375 8.0625 7.65625 7.0625 5.6875 5.625 10.875 4.59375 10.75]
logits 8 [4.625 6.8125 6.8125 5.21875 4.78125 3.82812 4.90625 7.5 4.15625 7.6875]
logits 9 [7.75 7.09375 8.0625 7.65625 7.0625 5.6875 5.625 10.875 4.59375 10.75]
logits 10 [7.75 7.09375 8.0625 7.65625 7.0625 5.6875 5.625 10.875 4.59375 10.75]
logits 11 [7.75 7.09375 8.0625 7.65625 7.0625 5.6875 5.625 10.875 4.59375 10.75]
next_tokens after sampling [8651 8651 8651 8651 8651 8651 8651 8651  262 8651 8651 8651 8651 8651
 8651 8651]
wenxin next_tokens [8651 8651 8651 8651 8651 8651 8651 8651  262 8651 8651 8651 8651 8651
 8651 8651]

self.max_num_reqs 256
dp_rank_reqs [['0'], ['1']]
dp_rank_tokens [[8651], [8651]]
cumsum_num_reqs_per_dp_rank [0 1 2]
padded_num_reqs_per_dp_rank 128
padded_num_reqs_logits 16
---- dp_rank 0 -----
dp_token_offset 0
dp_req_offset 0
dp_req_padded_offset 0
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [0]
arange [0]
positions_np [14]
self.input_batch.num_computed_tokens_cpu[req_indices] [15]
positions_np after add [15]
token_indices [15]
self.input_ids_cpu [8651]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [16  0  0 ...  0  0  0]
dp_req_offset 0
dp_token_offset 0
slices_start [15]
self.input_batch.num_computed_tokens_cpu [15 15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [0]
slot_mapping_metadata [[79  0  1]]
num_slices [1, []]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [8651    0    0 ...    0    0    0]
---- dp_rank 1 -----
dp_token_offset 16
dp_req_offset 1
dp_req_padded_offset 128
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [1]
arange [0]
positions_np [14]
self.input_batch.num_computed_tokens_cpu[req_indices] [15]
positions_np after add [15]
token_indices [1039]
self.input_ids_cpu [8651]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [16  0  0 ...  0  0  0]
dp_req_offset 1
dp_token_offset 16
slices_start [15]
self.input_batch.num_computed_tokens_cpu [15 15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [16]
slot_mapping_metadata [[16399     0     1]]
num_slices [1, 1]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [8651    0    0 ...    0    0    0]
---- --final start -- -----
input_ids (32,) [8651    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0 8651    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0]
positions [15  1  2  3  4  5  0  0  0  0  0  0  0  0  0  0 15  1  2  3  4  5  0  0
  0  0  0  0  0  0  0  0]
final slot_mapping_metadata (3, 32)
final slot_mapping_metadata [[   79     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0 16399     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    1     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     1     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]]
final slot_mapping_metadata []
block_tables [[1 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
query_start_loc [ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1 16  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0]
seq_lens [16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
logits_indices [ 0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0]
dp_rank_reqs [['0'], ['1']]
num_seqs [1 1]
---- --final finish-- -----
temp_tensor [-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  7.0064923e-45  8.4077908e-45  6.6710422e-30  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.3772934e-30  4.3646243e-41
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0995175e+13
  0.0000000e+00  0.0000000e+00  2.2420775e-44  0.0000000e+00
 -2.0683993e+15  2.2250378e-40  2.2420775e-44  0.0000000e+00
  4.6049755e-26  1.4012985e-45  0.0000000e+00  0.0000000e+00
  1.3772934e-30  4.3646243e-41  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  4.3644842e-41  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  1.2952631e-31  4.3646243e-41
  0.0000000e+00  0.0000000e+00  4.6758818e-30  0.0000000e+00
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  4.2038954e-45  0.0000000e+00
  2.3694278e-38  1.4012985e-45  1.4012985e-45  9.1835496e-41
  0.0000000e+00  5.6051939e-45  0.0000000e+00  2.3694278e-38
            nan  0.0000000e+00            nan  1.1210388e-44
  8.2284644e-38  0.0000000e+00            nan            nan
  0.0000000e+00  1.4012985e-45  5.7397185e-42  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1835496e-41  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1836897e-41  1.4012985e-45
  1.4012985e-45  7.0064923e-45  8.2284644e-38  0.0000000e+00
  0.0000000e+00  9.2195630e-41  8.2284644e-38  0.0000000e+00
  9.2195630e-41  9.1836897e-41  3.5873241e-43  0.0000000e+00
  1.1754944e-38  0.0000000e+00  1.1000000e+00  2.3509887e-38
            nan            nan  0.0000000e+00  3.5873241e-43
  2.1019477e-44  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.3312335e-43  2.3693561e-38  1.4012985e-40  0.0000000e+00
  9.1836897e-41  2.3509887e-38  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.4012985e-43  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  2.2420775e-44  0.0000000e+00  4.2038954e-45  3.5873241e-43
  2.3509887e-38  0.0000000e+00  2.2420775e-44  0.0000000e+00
  1.0000000e-01  7.0064923e-45  4.4841551e-44  1.4012985e-45
  1.4012985e-44  4.2038954e-44  4.4841551e-44  1.4012985e-45
            nan            nan  1.4012985e-45  0.0000000e+00
 -1.7014118e+38  0.0000000e+00  9.2194229e-41  2.3509887e-38
  2.3509887e-38  0.0000000e+00  0.0000000e+00  2.3510604e-38
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  8.2284644e-38  0.0000000e+00  2.8025969e-44  5.6051939e-44
  0.0000000e+00            nan            nan            nan]
top_k_tensor [        0         0         0         0         0         0         0
         0     -4096         0         0         0         0         0
         0         0     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1 233707072     31147 344830232         0     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707048     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -8192        -1     -8192        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707064     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1 233707032     31147
 344830208         0 233707032     31147 344830216         0     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1]
top_p_tensor [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan            nan  1.0248949e+11  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316673e-13 -3.0316488e-13
            nan            nan  1.2113646e+13  4.7644148e-44
            nan            nan -3.0360897e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4933738e+15  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0319264e-13 -3.0360897e-13
            nan            nan -3.0360897e-13 -3.1071440e-13
            nan            nan  1.7651560e+17  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -1.1368684e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
  1.4307163e-26  0.0000000e+00  0.0000000e+00  0.0000000e+00
            nan            nan  2.1188873e+19  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -1.3020833e-03
            nan            nan -3.0316532e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  2.6222406e+21  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0494818e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4012985e-45  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0320188e-13 -3.0316488e-13
            nan            nan  8.5479206e-44  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.6261203e+24  4.3643441e-41
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  1.4692446e-39  0.0000000e+00
            nan            nan  1.4012985e-44  0.0000000e+00
            nan            nan  1.4012985e-45 -1.1368685e-13
            nan            nan  0.0000000e+00  4.7644148e-44
            nan            nan -3.0316488e-13 -4.1268447e+26
            nan            nan -3.0316488e-13 -3.0494124e-13
            nan            nan -3.1027031e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327764e-13
            nan            nan  4.3148833e+29  4.7644148e-44
            nan            nan -3.0319264e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -4.4307661e+35
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan  5.0080864e+31  4.7644148e-44
            nan            nan -3.3158659e-13 -7.8066734e-11
            nan            nan -3.0316491e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327590e-13
            nan            nan  5.7511240e+33  4.7644148e-44
            nan            nan -3.0316499e-13 -4.1685183e-13
            nan            nan            nan -3.0316491e-13
            nan            nan -7.1696835e-03 -3.0316532e-13
            nan            nan -3.0319264e-13 -4.4307597e+35]
hidden_states after model_fn (32, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', None), memory_kind=device)
inputs[4] (16,) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(None,), memory_kind=device)
hidden_states after select (16, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(), memory_kind=device)
hidden_states 0 [0.273438 0.361328 0.671875 3.39062 0.722656 -0.214844 -2.64062 -1.08594
 1.66406 -3.40625]
hidden_states 1 [0.273438 0.361328 0.671875 3.39062 0.722656 -0.214844 -2.64062 -1.08594
 1.66406 -3.40625]
hidden_states 2 [0.273438 0.361328 0.671875 3.39062 0.722656 -0.214844 -2.64062 -1.08594
 1.66406 -3.40625]
hidden_states 3 [0.273438 0.361328 0.671875 3.39062 0.722656 -0.214844 -2.64062 -1.08594
 1.66406 -3.40625]
hidden_states 4 [0.273438 0.361328 0.671875 3.39062 0.722656 -0.214844 -2.64062 -1.08594
 1.66406 -3.40625]
hidden_states 5 [0.273438 0.361328 0.671875 3.39062 0.722656 -0.214844 -2.64062 -1.08594
 1.66406 -3.40625]
hidden_states 6 [0.273438 0.361328 0.671875 3.39062 0.722656 -0.214844 -2.64062 -1.08594
 1.66406 -3.40625]
hidden_states 7 [0.273438 0.361328 0.671875 3.39062 0.722656 -0.214844 -2.64062 -1.08594
 1.66406 -3.40625]
hidden_states 8 [4.3125 -2.09375 2.15625 0.53125 1.69531 3.1875 -1.42188 -0.644531
 -0.957031 -1.57812]
hidden_states 9 [0.273438 0.361328 0.671875 3.39062 0.722656 -0.214844 -2.64062 -1.08594
 1.66406 -3.40625]
hidden_states 10 [0.273438 0.361328 0.671875 3.39062 0.722656 -0.214844 -2.64062 -1.08594
 1.66406 -3.40625]
hidden_states 11 [0.273438 0.361328 0.671875 3.39062 0.722656 -0.214844 -2.64062 -1.08594
 1.66406 -3.40625]
logits (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits after select (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits 0 [2.09375 4.1875 1.90625 0.0751953 2.6875 1.89062 4.0625 1.26562 3.39062
 1.16406]
logits 1 [2.09375 4.1875 1.90625 0.0751953 2.6875 1.89062 4.0625 1.26562 3.39062
 1.16406]
logits 2 [2.09375 4.1875 1.90625 0.0751953 2.6875 1.89062 4.0625 1.26562 3.39062
 1.16406]
logits 3 [2.09375 4.1875 1.90625 0.0751953 2.6875 1.89062 4.0625 1.26562 3.39062
 1.16406]
logits 4 [2.09375 4.1875 1.90625 0.0751953 2.6875 1.89062 4.0625 1.26562 3.39062
 1.16406]
logits 5 [2.09375 4.1875 1.90625 0.0751953 2.6875 1.89062 4.0625 1.26562 3.39062
 1.16406]
logits 6 [2.09375 4.1875 1.90625 0.0751953 2.6875 1.89062 4.0625 1.26562 3.39062
 1.16406]
logits 7 [2.09375 4.1875 1.90625 0.0751953 2.6875 1.89062 4.0625 1.26562 3.39062
 1.16406]
logits 8 [3.10938 6.1875 2.40625 1.69531 3.51562 3.17188 4.4375 5.25 4.5 2.5]
logits 9 [2.09375 4.1875 1.90625 0.0751953 2.6875 1.89062 4.0625 1.26562 3.39062
 1.16406]
logits 10 [2.09375 4.1875 1.90625 0.0751953 2.6875 1.89062 4.0625 1.26562 3.39062
 1.16406]
logits 11 [2.09375 4.1875 1.90625 0.0751953 2.6875 1.89062 4.0625 1.26562 3.39062
 1.16406]
next_tokens after sampling [ 91  91  91  91  91  91  91  91 284  91  91  91  91  91  91  91]
wenxin next_tokens [ 91  91  91  91  91  91  91  91 284  91  91  91  91  91  91  91]

self.max_num_reqs 256
dp_rank_reqs [['0'], ['1']]
dp_rank_tokens [[91], [91]]
cumsum_num_reqs_per_dp_rank [0 1 2]
padded_num_reqs_per_dp_rank 128
padded_num_reqs_logits 16
---- dp_rank 0 -----
dp_token_offset 0
dp_req_offset 0
dp_req_padded_offset 0
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [0]
arange [0]
positions_np [15]
self.input_batch.num_computed_tokens_cpu[req_indices] [16]
positions_np after add [16]
token_indices [16]
self.input_ids_cpu [91]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [17  0  0 ...  0  0  0]
dp_req_offset 0
dp_token_offset 0
slices_start [16]
self.input_batch.num_computed_tokens_cpu [16 16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [0]
slot_mapping_metadata [[80  0  1]]
num_slices [1, []]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [91  0  0 ...  0  0  0]
---- dp_rank 1 -----
dp_token_offset 16
dp_req_offset 1
dp_req_padded_offset 128
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [1]
arange [0]
positions_np [15]
self.input_batch.num_computed_tokens_cpu[req_indices] [16]
positions_np after add [16]
token_indices [1040]
self.input_ids_cpu [91]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [17  0  0 ...  0  0  0]
dp_req_offset 1
dp_token_offset 16
slices_start [16]
self.input_batch.num_computed_tokens_cpu [16 16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [16]
slot_mapping_metadata [[16400     0     1]]
num_slices [1, 1]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [91  0  0 ...  0  0  0]
---- --final start -- -----
input_ids (32,) [91  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 91  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0]
positions [16  1  2  3  4  5  0  0  0  0  0  0  0  0  0  0 16  1  2  3  4  5  0  0
  0  0  0  0  0  0  0  0]
final slot_mapping_metadata (3, 32)
final slot_mapping_metadata [[   80     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0 16400     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    1     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     1     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]]
final slot_mapping_metadata []
block_tables [[1 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
query_start_loc [ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1 16  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0]
seq_lens [17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
logits_indices [ 0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0]
dp_rank_reqs [['0'], ['1']]
num_seqs [1 1]
---- --final finish-- -----
temp_tensor [-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  7.0064923e-45  8.4077908e-45  6.6710422e-30  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.3772934e-30  4.3646243e-41
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0995175e+13
  0.0000000e+00  0.0000000e+00  2.2420775e-44  0.0000000e+00
 -2.0683993e+15  2.2250378e-40  2.2420775e-44  0.0000000e+00
  4.6049755e-26  1.4012985e-45  0.0000000e+00  0.0000000e+00
  1.3772934e-30  4.3646243e-41  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  4.3644842e-41  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  1.2952631e-31  4.3646243e-41
  0.0000000e+00  0.0000000e+00  4.6758818e-30  0.0000000e+00
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  4.2038954e-45  0.0000000e+00
  2.3694278e-38  1.4012985e-45  1.4012985e-45  9.1835496e-41
  0.0000000e+00  5.6051939e-45  0.0000000e+00  2.3694278e-38
            nan  0.0000000e+00            nan  1.1210388e-44
  8.2284644e-38  0.0000000e+00            nan            nan
  0.0000000e+00  1.4012985e-45  5.7397185e-42  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1835496e-41  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1836897e-41  1.4012985e-45
  1.4012985e-45  7.0064923e-45  8.2284644e-38  0.0000000e+00
  0.0000000e+00  9.2195630e-41  8.2284644e-38  0.0000000e+00
  9.2195630e-41  9.1836897e-41  3.5873241e-43  0.0000000e+00
  1.1754944e-38  0.0000000e+00  1.1000000e+00  2.3509887e-38
            nan            nan  0.0000000e+00  3.5873241e-43
  2.1019477e-44  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.3312335e-43  2.3693561e-38  1.4012985e-40  0.0000000e+00
  9.1836897e-41  2.3509887e-38  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.4012985e-43  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  2.2420775e-44  0.0000000e+00  4.2038954e-45  3.5873241e-43
  2.3509887e-38  0.0000000e+00  2.2420775e-44  0.0000000e+00
  1.0000000e-01  7.0064923e-45  4.4841551e-44  1.4012985e-45
  1.4012985e-44  4.2038954e-44  4.4841551e-44  1.4012985e-45
            nan            nan  1.4012985e-45  0.0000000e+00
 -1.7014118e+38  0.0000000e+00  9.2194229e-41  2.3509887e-38
  2.3509887e-38  0.0000000e+00  0.0000000e+00  2.3510604e-38
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  8.2284644e-38  0.0000000e+00  2.8025969e-44  5.6051939e-44
  0.0000000e+00            nan            nan            nan]
top_k_tensor [        0         0         0         0         0         0         0
         0     -4096         0         0         0         0         0
         0         0     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1 233707072     31147 344830232         0     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707048     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -8192        -1     -8192        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707064     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1 233707032     31147
 344830208         0 233707032     31147 344830216         0     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1]
top_p_tensor [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan            nan  1.0248949e+11  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316673e-13 -3.0316488e-13
            nan            nan  1.2113646e+13  4.7644148e-44
            nan            nan -3.0360897e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4933738e+15  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0319264e-13 -3.0360897e-13
            nan            nan -3.0360897e-13 -3.1071440e-13
            nan            nan  1.7651560e+17  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -1.1368684e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
  1.4307163e-26  0.0000000e+00  0.0000000e+00  0.0000000e+00
            nan            nan  2.1188873e+19  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -1.3020833e-03
            nan            nan -3.0316532e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  2.6222406e+21  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0494818e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4012985e-45  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0320188e-13 -3.0316488e-13
            nan            nan  8.5479206e-44  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.6261203e+24  4.3643441e-41
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  1.4692446e-39  0.0000000e+00
            nan            nan  1.4012985e-44  0.0000000e+00
            nan            nan  1.4012985e-45 -1.1368685e-13
            nan            nan  0.0000000e+00  4.7644148e-44
            nan            nan -3.0316488e-13 -4.1268447e+26
            nan            nan -3.0316488e-13 -3.0494124e-13
            nan            nan -3.1027031e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327764e-13
            nan            nan  4.3148833e+29  4.7644148e-44
            nan            nan -3.0319264e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -4.4307661e+35
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan  5.0080864e+31  4.7644148e-44
            nan            nan -3.3158659e-13 -7.8066734e-11
            nan            nan -3.0316491e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327590e-13
            nan            nan  5.7511240e+33  4.7644148e-44
            nan            nan -3.0316499e-13 -4.1685183e-13
            nan            nan            nan -3.0316491e-13
            nan            nan -7.1696835e-03 -3.0316532e-13
            nan            nan -3.0319264e-13 -4.4307597e+35]
hidden_states after model_fn (32, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', None), memory_kind=device)
inputs[4] (16,) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(None,), memory_kind=device)
hidden_states after select (16, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(), memory_kind=device)
hidden_states 0 [-7.0625 3.64062 -1.85156 1.22656 -1.97656 -4.09375 1.32031 0.404297
 4.59375 -1.91406]
hidden_states 1 [-7.0625 3.64062 -1.85156 1.22656 -1.97656 -4.09375 1.32031 0.404297
 4.59375 -1.91406]
hidden_states 2 [-7.0625 3.64062 -1.85156 1.22656 -1.97656 -4.09375 1.32031 0.404297
 4.59375 -1.91406]
hidden_states 3 [-7.0625 3.64062 -1.85156 1.22656 -1.97656 -4.09375 1.32031 0.404297
 4.59375 -1.91406]
hidden_states 4 [-7.0625 3.64062 -1.85156 1.22656 -1.97656 -4.09375 1.32031 0.404297
 4.59375 -1.91406]
hidden_states 5 [-7.0625 3.64062 -1.85156 1.22656 -1.97656 -4.09375 1.32031 0.404297
 4.59375 -1.91406]
hidden_states 6 [-7.0625 3.64062 -1.85156 1.22656 -1.97656 -4.09375 1.32031 0.404297
 4.59375 -1.91406]
hidden_states 7 [-7.0625 3.64062 -1.85156 1.22656 -1.97656 -4.09375 1.32031 0.404297
 4.59375 -1.91406]
hidden_states 8 [-1.8125 -1.53906 -0.100098 2.9375 -0.679688 0.828125 -0.416016 -1.28125
 0.984375 -1.46875]
hidden_states 9 [-7.0625 3.64062 -1.85156 1.22656 -1.97656 -4.09375 1.32031 0.404297
 4.59375 -1.91406]
hidden_states 10 [-7.0625 3.64062 -1.85156 1.22656 -1.97656 -4.09375 1.32031 0.404297
 4.59375 -1.91406]
hidden_states 11 [-7.0625 3.64062 -1.85156 1.22656 -1.97656 -4.09375 1.32031 0.404297
 4.59375 -1.91406]
logits (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits after select (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits 0 [4.125 3.09375 0.808594 -1.33594 -0.757812 3.9375 4.75 0.412109 2.39062
 3.10938]
logits 1 [4.125 3.09375 0.808594 -1.33594 -0.757812 3.9375 4.75 0.412109 2.39062
 3.10938]
logits 2 [4.125 3.09375 0.808594 -1.33594 -0.757812 3.9375 4.75 0.412109 2.39062
 3.10938]
logits 3 [4.125 3.09375 0.808594 -1.33594 -0.757812 3.9375 4.75 0.412109 2.39062
 3.10938]
logits 4 [4.125 3.09375 0.808594 -1.33594 -0.757812 3.9375 4.75 0.412109 2.39062
 3.10938]
logits 5 [4.125 3.09375 0.808594 -1.33594 -0.757812 3.9375 4.75 0.412109 2.39062
 3.10938]
logits 6 [4.125 3.09375 0.808594 -1.33594 -0.757812 3.9375 4.75 0.412109 2.39062
 3.10938]
logits 7 [4.125 3.09375 0.808594 -1.33594 -0.757812 3.9375 4.75 0.412109 2.39062
 3.10938]
logits 8 [6.4375 1.89844 2.26562 -1.97656 1.32031 3.46875 2.04688 -1.09375 2.03125
 2.17188]
logits 9 [4.125 3.09375 0.808594 -1.33594 -0.757812 3.9375 4.75 0.412109 2.39062
 3.10938]
logits 10 [4.125 3.09375 0.808594 -1.33594 -0.757812 3.9375 4.75 0.412109 2.39062
 3.10938]
logits 11 [4.125 3.09375 0.808594 -1.33594 -0.757812 3.9375 4.75 0.412109 2.39062
 3.10938]
next_tokens after sampling [ 11  11  11  11  11  11  11  11 320  11  11  11  11  11  11  11]
wenxin next_tokens [ 11  11  11  11  11  11  11  11 320  11  11  11  11  11  11  11]

self.max_num_reqs 256
dp_rank_reqs [['0'], ['1']]
dp_rank_tokens [[11], [11]]
cumsum_num_reqs_per_dp_rank [0 1 2]
padded_num_reqs_per_dp_rank 128
padded_num_reqs_logits 16
---- dp_rank 0 -----
dp_token_offset 0
dp_req_offset 0
dp_req_padded_offset 0
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [0]
arange [0]
positions_np [16]
self.input_batch.num_computed_tokens_cpu[req_indices] [17]
positions_np after add [17]
token_indices [17]
self.input_ids_cpu [11]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [18  0  0 ...  0  0  0]
dp_req_offset 0
dp_token_offset 0
slices_start [17]
self.input_batch.num_computed_tokens_cpu [17 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [0]
slot_mapping_metadata [[81  0  1]]
num_slices [1, []]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [11  0  0 ...  0  0  0]
---- dp_rank 1 -----
dp_token_offset 16
dp_req_offset 1
dp_req_padded_offset 128
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [1]
arange [0]
positions_np [16]
self.input_batch.num_computed_tokens_cpu[req_indices] [17]
positions_np after add [17]
token_indices [1041]
self.input_ids_cpu [11]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [18  0  0 ...  0  0  0]
dp_req_offset 1
dp_token_offset 16
slices_start [17]
self.input_batch.num_computed_tokens_cpu [17 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [16]
slot_mapping_metadata [[16401     0     1]]
num_slices [1, 1]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [11  0  0 ...  0  0  0]
---- --final start -- -----
input_ids (32,) [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0]
positions [17  1  2  3  4  5  0  0  0  0  0  0  0  0  0  0 17  1  2  3  4  5  0  0
  0  0  0  0  0  0  0  0]
final slot_mapping_metadata (3, 32)
final slot_mapping_metadata [[   81     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0 16401     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    1     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     1     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]]
final slot_mapping_metadata []
block_tables [[1 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
query_start_loc [ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1 16  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0]
seq_lens [18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0 18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
logits_indices [ 0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0]
dp_rank_reqs [['0'], ['1']]
num_seqs [1 1]
---- --final finish-- -----
temp_tensor [-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  7.0064923e-45  8.4077908e-45  6.6710422e-30  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.3772934e-30  4.3646243e-41
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0995175e+13
  0.0000000e+00  0.0000000e+00  2.2420775e-44  0.0000000e+00
 -2.0683993e+15  2.2250378e-40  2.2420775e-44  0.0000000e+00
  4.6049755e-26  1.4012985e-45  0.0000000e+00  0.0000000e+00
  1.3772934e-30  4.3646243e-41  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  4.3644842e-41  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  1.2952631e-31  4.3646243e-41
  0.0000000e+00  0.0000000e+00  4.6758818e-30  0.0000000e+00
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  4.2038954e-45  0.0000000e+00
  2.3694278e-38  1.4012985e-45  1.4012985e-45  9.1835496e-41
  0.0000000e+00  5.6051939e-45  0.0000000e+00  2.3694278e-38
            nan  0.0000000e+00            nan  1.1210388e-44
  8.2284644e-38  0.0000000e+00            nan            nan
  0.0000000e+00  1.4012985e-45  5.7397185e-42  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1835496e-41  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1836897e-41  1.4012985e-45
  1.4012985e-45  7.0064923e-45  8.2284644e-38  0.0000000e+00
  0.0000000e+00  9.2195630e-41  8.2284644e-38  0.0000000e+00
  9.2195630e-41  9.1836897e-41  3.5873241e-43  0.0000000e+00
  1.1754944e-38  0.0000000e+00  1.1000000e+00  2.3509887e-38
            nan            nan  0.0000000e+00  3.5873241e-43
  2.1019477e-44  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.3312335e-43  2.3693561e-38  1.4012985e-40  0.0000000e+00
  9.1836897e-41  2.3509887e-38  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.4012985e-43  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  2.2420775e-44  0.0000000e+00  4.2038954e-45  3.5873241e-43
  2.3509887e-38  0.0000000e+00  2.2420775e-44  0.0000000e+00
  1.0000000e-01  7.0064923e-45  4.4841551e-44  1.4012985e-45
  1.4012985e-44  4.2038954e-44  4.4841551e-44  1.4012985e-45
            nan            nan  1.4012985e-45  0.0000000e+00
 -1.7014118e+38  0.0000000e+00  9.2194229e-41  2.3509887e-38
  2.3509887e-38  0.0000000e+00  0.0000000e+00  2.3510604e-38
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  8.2284644e-38  0.0000000e+00  2.8025969e-44  5.6051939e-44
  0.0000000e+00            nan            nan            nan]
top_k_tensor [        0         0         0         0         0         0         0
         0     -4096         0         0         0         0         0
         0         0     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1 233707072     31147 344830232         0     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707048     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -8192        -1     -8192        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707064     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1 233707032     31147
 344830208         0 233707032     31147 344830216         0     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1]
top_p_tensor [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan            nan  1.0248949e+11  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316673e-13 -3.0316488e-13
            nan            nan  1.2113646e+13  4.7644148e-44
            nan            nan -3.0360897e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4933738e+15  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0319264e-13 -3.0360897e-13
            nan            nan -3.0360897e-13 -3.1071440e-13
            nan            nan  1.7651560e+17  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -1.1368684e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
  1.4307163e-26  0.0000000e+00  0.0000000e+00  0.0000000e+00
            nan            nan  2.1188873e+19  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -1.3020833e-03
            nan            nan -3.0316532e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  2.6222406e+21  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0494818e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4012985e-45  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0320188e-13 -3.0316488e-13
            nan            nan  8.5479206e-44  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.6261203e+24  4.3643441e-41
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  1.4692446e-39  0.0000000e+00
            nan            nan  1.4012985e-44  0.0000000e+00
            nan            nan  1.4012985e-45 -1.1368685e-13
            nan            nan  0.0000000e+00  4.7644148e-44
            nan            nan -3.0316488e-13 -4.1268447e+26
            nan            nan -3.0316488e-13 -3.0494124e-13
            nan            nan -3.1027031e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327764e-13
            nan            nan  4.3148833e+29  4.7644148e-44
            nan            nan -3.0319264e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -4.4307661e+35
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan  5.0080864e+31  4.7644148e-44
            nan            nan -3.3158659e-13 -7.8066734e-11
            nan            nan -3.0316491e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327590e-13
            nan            nan  5.7511240e+33  4.7644148e-44
            nan            nan -3.0316499e-13 -4.1685183e-13
            nan            nan            nan -3.0316491e-13
            nan            nan -7.1696835e-03 -3.0316532e-13
            nan            nan -3.0319264e-13 -4.4307597e+35]
hidden_states after model_fn (32, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', None), memory_kind=device)
inputs[4] (16,) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(None,), memory_kind=device)
hidden_states after select (16, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(), memory_kind=device)
hidden_states 0 [2.1875 -2.35938 0.353516 0.691406 1.52344 -0.632812 -2.57812 -0.337891
 0.253906 1.10156]
hidden_states 1 [2.1875 -2.35938 0.353516 0.691406 1.52344 -0.632812 -2.57812 -0.337891
 0.253906 1.10156]
hidden_states 2 [2.1875 -2.35938 0.353516 0.691406 1.52344 -0.632812 -2.57812 -0.337891
 0.253906 1.10156]
hidden_states 3 [2.1875 -2.35938 0.353516 0.691406 1.52344 -0.632812 -2.57812 -0.337891
 0.253906 1.10156]
hidden_states 4 [2.1875 -2.35938 0.353516 0.691406 1.52344 -0.632812 -2.57812 -0.337891
 0.253906 1.10156]
hidden_states 5 [2.1875 -2.35938 0.353516 0.691406 1.52344 -0.632812 -2.57812 -0.337891
 0.253906 1.10156]
hidden_states 6 [2.1875 -2.35938 0.353516 0.691406 1.52344 -0.632812 -2.57812 -0.337891
 0.253906 1.10156]
hidden_states 7 [2.1875 -2.35938 0.353516 0.691406 1.52344 -0.632812 -2.57812 -0.337891
 0.253906 1.10156]
hidden_states 8 [-1.19531 -1.28906 1.99219 1.99219 0.353516 1.29688 -4.9375 1.13281
 0.369141 -1.75]
hidden_states 9 [2.1875 -2.35938 0.353516 0.691406 1.52344 -0.632812 -2.57812 -0.337891
 0.253906 1.10156]
hidden_states 10 [2.1875 -2.35938 0.353516 0.691406 1.52344 -0.632812 -2.57812 -0.337891
 0.253906 1.10156]
hidden_states 11 [2.1875 -2.35938 0.353516 0.691406 1.52344 -0.632812 -2.57812 -0.337891
 0.253906 1.10156]
logits (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits after select (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits 0 [-1.67188 -8 -4.5 -7.375 -3.76562 -7.53125 -6.96875 -8.125 -3.73438
 -8.3125]
logits 1 [-1.67188 -8 -4.5 -7.375 -3.76562 -7.53125 -6.96875 -8.125 -3.73438
 -8.3125]
logits 2 [-1.67188 -8 -4.5 -7.375 -3.76562 -7.53125 -6.96875 -8.125 -3.73438
 -8.3125]
logits 3 [-1.67188 -8 -4.5 -7.375 -3.76562 -7.53125 -6.96875 -8.125 -3.73438
 -8.3125]
logits 4 [-1.67188 -8 -4.5 -7.375 -3.76562 -7.53125 -6.96875 -8.125 -3.73438
 -8.3125]
logits 5 [-1.67188 -8 -4.5 -7.375 -3.76562 -7.53125 -6.96875 -8.125 -3.73438
 -8.3125]
logits 6 [-1.67188 -8 -4.5 -7.375 -3.76562 -7.53125 -6.96875 -8.125 -3.73438
 -8.3125]
logits 7 [-1.67188 -8 -4.5 -7.375 -3.76562 -7.53125 -6.96875 -8.125 -3.73438
 -8.3125]
logits 8 [2.26562 -1.00781 1.50781 -1.75781 0.498047 1.86719 -0.211914 -1.46094
 1.14844 -0.5625]
logits 9 [-1.67188 -8 -4.5 -7.375 -3.76562 -7.53125 -6.96875 -8.125 -3.73438
 -8.3125]
logits 10 [-1.67188 -8 -4.5 -7.375 -3.76562 -7.53125 -6.96875 -8.125 -3.73438
 -8.3125]
logits 11 [-1.67188 -8 -4.5 -7.375 -3.76562 -7.53125 -6.96875 -8.125 -3.73438
 -8.3125]
next_tokens after sampling [279 279 279 279 279 279 279 279 719 279 279 279 279 279 279 279]
wenxin next_tokens [279 279 279 279 279 279 279 279 719 279 279 279 279 279 279 279]

self.max_num_reqs 256
dp_rank_reqs [['0'], ['1']]
dp_rank_tokens [[279], [279]]
cumsum_num_reqs_per_dp_rank [0 1 2]
padded_num_reqs_per_dp_rank 128
padded_num_reqs_logits 16
---- dp_rank 0 -----
dp_token_offset 0
dp_req_offset 0
dp_req_padded_offset 0
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [0]
arange [0]
positions_np [17]
self.input_batch.num_computed_tokens_cpu[req_indices] [18]
positions_np after add [18]
token_indices [18]
self.input_ids_cpu [279]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [19  0  0 ...  0  0  0]
dp_req_offset 0
dp_token_offset 0
slices_start [18]
self.input_batch.num_computed_tokens_cpu [18 18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [0]
slot_mapping_metadata [[82  0  1]]
num_slices [1, []]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [279   0   0 ...   0   0   0]
---- dp_rank 1 -----
dp_token_offset 16
dp_req_offset 1
dp_req_padded_offset 128
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [1]
arange [0]
positions_np [17]
self.input_batch.num_computed_tokens_cpu[req_indices] [18]
positions_np after add [18]
token_indices [1042]
self.input_ids_cpu [279]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [19  0  0 ...  0  0  0]
dp_req_offset 1
dp_token_offset 16
slices_start [18]
self.input_batch.num_computed_tokens_cpu [18 18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [16]
slot_mapping_metadata [[16402     0     1]]
num_slices [1, 1]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [279   0   0 ...   0   0   0]
---- --final start -- -----
input_ids (32,) [279   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 279   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
positions [18  1  2  3  4  5  0  0  0  0  0  0  0  0  0  0 18  1  2  3  4  5  0  0
  0  0  0  0  0  0  0  0]
final slot_mapping_metadata (3, 32)
final slot_mapping_metadata [[   82     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0 16402     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    1     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     1     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]]
final slot_mapping_metadata []
block_tables [[1 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
query_start_loc [ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1 16  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0]
seq_lens [19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0 19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
logits_indices [ 0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0]
dp_rank_reqs [['0'], ['1']]
num_seqs [1 1]
---- --final finish-- -----
temp_tensor [-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  7.0064923e-45  8.4077908e-45  6.6710422e-30  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.3772934e-30  4.3646243e-41
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0995175e+13
  0.0000000e+00  0.0000000e+00  2.2420775e-44  0.0000000e+00
 -2.0683993e+15  2.2250378e-40  2.2420775e-44  0.0000000e+00
  4.6049755e-26  1.4012985e-45  0.0000000e+00  0.0000000e+00
  1.3772934e-30  4.3646243e-41  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  4.3644842e-41  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  1.2952631e-31  4.3646243e-41
  0.0000000e+00  0.0000000e+00  4.6758818e-30  0.0000000e+00
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  4.2038954e-45  0.0000000e+00
  2.3694278e-38  1.4012985e-45  1.4012985e-45  9.1835496e-41
  0.0000000e+00  5.6051939e-45  0.0000000e+00  2.3694278e-38
            nan  0.0000000e+00            nan  1.1210388e-44
  8.2284644e-38  0.0000000e+00            nan            nan
  0.0000000e+00  1.4012985e-45  5.7397185e-42  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1835496e-41  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1836897e-41  1.4012985e-45
  1.4012985e-45  7.0064923e-45  8.2284644e-38  0.0000000e+00
  0.0000000e+00  9.2195630e-41  8.2284644e-38  0.0000000e+00
  9.2195630e-41  9.1836897e-41  3.5873241e-43  0.0000000e+00
  1.1754944e-38  0.0000000e+00  1.1000000e+00  2.3509887e-38
            nan            nan  0.0000000e+00  3.5873241e-43
  2.1019477e-44  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.3312335e-43  2.3693561e-38  1.4012985e-40  0.0000000e+00
  9.1836897e-41  2.3509887e-38  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.4012985e-43  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  2.2420775e-44  0.0000000e+00  4.2038954e-45  3.5873241e-43
  2.3509887e-38  0.0000000e+00  2.2420775e-44  0.0000000e+00
  1.0000000e-01  7.0064923e-45  4.4841551e-44  1.4012985e-45
  1.4012985e-44  4.2038954e-44  4.4841551e-44  1.4012985e-45
            nan            nan  1.4012985e-45  0.0000000e+00
 -1.7014118e+38  0.0000000e+00  9.2194229e-41  2.3509887e-38
  2.3509887e-38  0.0000000e+00  0.0000000e+00  2.3510604e-38
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  8.2284644e-38  0.0000000e+00  2.8025969e-44  5.6051939e-44
  0.0000000e+00            nan            nan            nan]
top_k_tensor [        0         0         0         0         0         0         0
         0     -4096         0         0         0         0         0
         0         0     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1 233707072     31147 344830232         0     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707048     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -8192        -1     -8192        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707064     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1 233707032     31147
 344830208         0 233707032     31147 344830216         0     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1]
top_p_tensor [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan            nan  1.0248949e+11  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316673e-13 -3.0316488e-13
            nan            nan  1.2113646e+13  4.7644148e-44
            nan            nan -3.0360897e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4933738e+15  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0319264e-13 -3.0360897e-13
            nan            nan -3.0360897e-13 -3.1071440e-13
            nan            nan  1.7651560e+17  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -1.1368684e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
  1.4307163e-26  0.0000000e+00  0.0000000e+00  0.0000000e+00
            nan            nan  2.1188873e+19  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -1.3020833e-03
            nan            nan -3.0316532e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  2.6222406e+21  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0494818e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4012985e-45  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0320188e-13 -3.0316488e-13
            nan            nan  8.5479206e-44  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.6261203e+24  4.3643441e-41
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  1.4692446e-39  0.0000000e+00
            nan            nan  1.4012985e-44  0.0000000e+00
            nan            nan  1.4012985e-45 -1.1368685e-13
            nan            nan  0.0000000e+00  4.7644148e-44
            nan            nan -3.0316488e-13 -4.1268447e+26
            nan            nan -3.0316488e-13 -3.0494124e-13
            nan            nan -3.1027031e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327764e-13
            nan            nan  4.3148833e+29  4.7644148e-44
            nan            nan -3.0319264e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -4.4307661e+35
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan  5.0080864e+31  4.7644148e-44
            nan            nan -3.3158659e-13 -7.8066734e-11
            nan            nan -3.0316491e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327590e-13
            nan            nan  5.7511240e+33  4.7644148e-44
            nan            nan -3.0316499e-13 -4.1685183e-13
            nan            nan            nan -3.0316491e-13
            nan            nan -7.1696835e-03 -3.0316532e-13
            nan            nan -3.0319264e-13 -4.4307597e+35]
hidden_states after model_fn (32, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', None), memory_kind=device)
inputs[4] (16,) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(None,), memory_kind=device)
hidden_states after select (16, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(), memory_kind=device)
hidden_states 0 [0.081543 -0.953125 -0.0400391 0.882812 0.12793 0.59375 0.146484 1.5625
 -0.886719 -1.53125]
hidden_states 1 [0.081543 -0.953125 -0.0400391 0.882812 0.12793 0.59375 0.146484 1.5625
 -0.886719 -1.53125]
hidden_states 2 [0.081543 -0.953125 -0.0400391 0.882812 0.12793 0.59375 0.146484 1.5625
 -0.886719 -1.53125]
hidden_states 3 [0.081543 -0.953125 -0.0400391 0.882812 0.12793 0.59375 0.146484 1.5625
 -0.886719 -1.53125]
hidden_states 4 [0.081543 -0.953125 -0.0400391 0.882812 0.12793 0.59375 0.146484 1.5625
 -0.886719 -1.53125]
hidden_states 5 [0.081543 -0.953125 -0.0400391 0.882812 0.12793 0.59375 0.146484 1.5625
 -0.886719 -1.53125]
hidden_states 6 [0.081543 -0.953125 -0.0400391 0.882812 0.12793 0.59375 0.146484 1.5625
 -0.886719 -1.53125]
hidden_states 7 [0.081543 -0.953125 -0.0400391 0.882812 0.12793 0.59375 0.146484 1.5625
 -0.886719 -1.53125]
hidden_states 8 [-0.59375 0.980469 1.22656 3.45312 0.130859 0.996094 1.65625 0.976562
 -2.54688 -2.8125]
hidden_states 9 [0.081543 -0.953125 -0.0400391 0.882812 0.12793 0.59375 0.146484 1.5625
 -0.886719 -1.53125]
hidden_states 10 [0.081543 -0.953125 -0.0400391 0.882812 0.12793 0.59375 0.146484 1.5625
 -0.886719 -1.53125]
hidden_states 11 [0.081543 -0.953125 -0.0400391 0.882812 0.12793 0.59375 0.146484 1.5625
 -0.886719 -1.53125]
logits (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits after select (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits 0 [5.46875 2.96875 2.10938 -0.186523 2.25 3.73438 3.53125 1.40625 1.91406
 0.964844]
logits 1 [5.46875 2.96875 2.10938 -0.186523 2.25 3.73438 3.53125 1.40625 1.91406
 0.964844]
logits 2 [5.46875 2.96875 2.10938 -0.186523 2.25 3.73438 3.53125 1.40625 1.91406
 0.964844]
logits 3 [5.46875 2.96875 2.10938 -0.186523 2.25 3.73438 3.53125 1.40625 1.91406
 0.964844]
logits 4 [5.46875 2.96875 2.10938 -0.186523 2.25 3.73438 3.53125 1.40625 1.91406
 0.964844]
logits 5 [5.46875 2.96875 2.10938 -0.186523 2.25 3.73438 3.53125 1.40625 1.91406
 0.964844]
logits 6 [5.46875 2.96875 2.10938 -0.186523 2.25 3.73438 3.53125 1.40625 1.91406
 0.964844]
logits 7 [5.46875 2.96875 2.10938 -0.186523 2.25 3.73438 3.53125 1.40625 1.91406
 0.964844]
logits 8 [3.14062 6.5625 3.54688 4.03125 7.78125 4.46875 3.09375 5.625 3.39062
 4.09375]
logits 9 [5.46875 2.96875 2.10938 -0.186523 2.25 3.73438 3.53125 1.40625 1.91406
 0.964844]
logits 10 [5.46875 2.96875 2.10938 -0.186523 2.25 3.73438 3.53125 1.40625 1.91406
 0.964844]
logits 11 [5.46875 2.96875 2.10938 -0.186523 2.25 3.73438 3.53125 1.40625 1.91406
 0.964844]
next_tokens after sampling [ 82  82  82  82  82  82  82  82 765  82  82  82  82  82  82  82]
wenxin next_tokens [ 82  82  82  82  82  82  82  82 765  82  82  82  82  82  82  82]

self.max_num_reqs 256
dp_rank_reqs [['0'], ['1']]
dp_rank_tokens [[82], [82]]
cumsum_num_reqs_per_dp_rank [0 1 2]
padded_num_reqs_per_dp_rank 128
padded_num_reqs_logits 16
---- dp_rank 0 -----
dp_token_offset 0
dp_req_offset 0
dp_req_padded_offset 0
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [0]
arange [0]
positions_np [18]
self.input_batch.num_computed_tokens_cpu[req_indices] [19]
positions_np after add [19]
token_indices [19]
self.input_ids_cpu [82]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [20  0  0 ...  0  0  0]
dp_req_offset 0
dp_token_offset 0
slices_start [19]
self.input_batch.num_computed_tokens_cpu [19 19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [0]
slot_mapping_metadata [[83  0  1]]
num_slices [1, []]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [82  0  0 ...  0  0  0]
---- dp_rank 1 -----
dp_token_offset 16
dp_req_offset 1
dp_req_padded_offset 128
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [1]
arange [0]
positions_np [18]
self.input_batch.num_computed_tokens_cpu[req_indices] [19]
positions_np after add [19]
token_indices [1043]
self.input_ids_cpu [82]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [20  0  0 ...  0  0  0]
dp_req_offset 1
dp_token_offset 16
slices_start [19]
self.input_batch.num_computed_tokens_cpu [19 19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [16]
slot_mapping_metadata [[16403     0     1]]
num_slices [1, 1]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [82  0  0 ...  0  0  0]
---- --final start -- -----
input_ids (32,) [82  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 82  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0]
positions [19  1  2  3  4  5  0  0  0  0  0  0  0  0  0  0 19  1  2  3  4  5  0  0
  0  0  0  0  0  0  0  0]
final slot_mapping_metadata (3, 32)
final slot_mapping_metadata [[   83     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0 16403     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    1     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     1     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]]
final slot_mapping_metadata []
block_tables [[1 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
query_start_loc [ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1 16  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0]
seq_lens [20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
logits_indices [ 0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0]
dp_rank_reqs [['0'], ['1']]
num_seqs [1 1]
---- --final finish-- -----
temp_tensor [-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  7.0064923e-45  8.4077908e-45  6.6710422e-30  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.3772934e-30  4.3646243e-41
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0995175e+13
  0.0000000e+00  0.0000000e+00  2.2420775e-44  0.0000000e+00
 -2.0683993e+15  2.2250378e-40  2.2420775e-44  0.0000000e+00
  4.6049755e-26  1.4012985e-45  0.0000000e+00  0.0000000e+00
  1.3772934e-30  4.3646243e-41  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  4.3644842e-41  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  1.2952631e-31  4.3646243e-41
  0.0000000e+00  0.0000000e+00  4.6758818e-30  0.0000000e+00
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  4.2038954e-45  0.0000000e+00
  2.3694278e-38  1.4012985e-45  1.4012985e-45  9.1835496e-41
  0.0000000e+00  5.6051939e-45  0.0000000e+00  2.3694278e-38
            nan  0.0000000e+00            nan  1.1210388e-44
  8.2284644e-38  0.0000000e+00            nan            nan
  0.0000000e+00  1.4012985e-45  5.7397185e-42  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1835496e-41  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1836897e-41  1.4012985e-45
  1.4012985e-45  7.0064923e-45  8.2284644e-38  0.0000000e+00
  0.0000000e+00  9.2195630e-41  8.2284644e-38  0.0000000e+00
  9.2195630e-41  9.1836897e-41  3.5873241e-43  0.0000000e+00
  1.1754944e-38  0.0000000e+00  1.1000000e+00  2.3509887e-38
            nan            nan  0.0000000e+00  3.5873241e-43
  2.1019477e-44  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.3312335e-43  2.3693561e-38  1.4012985e-40  0.0000000e+00
  9.1836897e-41  2.3509887e-38  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.4012985e-43  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  2.2420775e-44  0.0000000e+00  4.2038954e-45  3.5873241e-43
  2.3509887e-38  0.0000000e+00  2.2420775e-44  0.0000000e+00
  1.0000000e-01  7.0064923e-45  4.4841551e-44  1.4012985e-45
  1.4012985e-44  4.2038954e-44  4.4841551e-44  1.4012985e-45
            nan            nan  1.4012985e-45  0.0000000e+00
 -1.7014118e+38  0.0000000e+00  9.2194229e-41  2.3509887e-38
  2.3509887e-38  0.0000000e+00  0.0000000e+00  2.3510604e-38
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  8.2284644e-38  0.0000000e+00  2.8025969e-44  5.6051939e-44
  0.0000000e+00            nan            nan            nan]
top_k_tensor [        0         0         0         0         0         0         0
         0     -4096         0         0         0         0         0
         0         0     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1 233707072     31147 344830232         0     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707048     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -8192        -1     -8192        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707064     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1 233707032     31147
 344830208         0 233707032     31147 344830216         0     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1]
top_p_tensor [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan            nan  1.0248949e+11  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316673e-13 -3.0316488e-13
            nan            nan  1.2113646e+13  4.7644148e-44
            nan            nan -3.0360897e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4933738e+15  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0319264e-13 -3.0360897e-13
            nan            nan -3.0360897e-13 -3.1071440e-13
            nan            nan  1.7651560e+17  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -1.1368684e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
  1.4307163e-26  0.0000000e+00  0.0000000e+00  0.0000000e+00
            nan            nan  2.1188873e+19  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -1.3020833e-03
            nan            nan -3.0316532e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  2.6222406e+21  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0494818e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4012985e-45  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0320188e-13 -3.0316488e-13
            nan            nan  8.5479206e-44  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.6261203e+24  4.3643441e-41
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  1.4692446e-39  0.0000000e+00
            nan            nan  1.4012985e-44  0.0000000e+00
            nan            nan  1.4012985e-45 -1.1368685e-13
            nan            nan  0.0000000e+00  4.7644148e-44
            nan            nan -3.0316488e-13 -4.1268447e+26
            nan            nan -3.0316488e-13 -3.0494124e-13
            nan            nan -3.1027031e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327764e-13
            nan            nan  4.3148833e+29  4.7644148e-44
            nan            nan -3.0319264e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -4.4307661e+35
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan  5.0080864e+31  4.7644148e-44
            nan            nan -3.3158659e-13 -7.8066734e-11
            nan            nan -3.0316491e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327590e-13
            nan            nan  5.7511240e+33  4.7644148e-44
            nan            nan -3.0316499e-13 -4.1685183e-13
            nan            nan            nan -3.0316491e-13
            nan            nan -7.1696835e-03 -3.0316532e-13
            nan            nan -3.0319264e-13 -4.4307597e+35]
hidden_states after model_fn (32, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', None), memory_kind=device)
inputs[4] (16,) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(None,), memory_kind=device)
hidden_states after select (16, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(), memory_kind=device)
hidden_states 0 [2.89062 -1.9375 -1.51562 -3.20312 -3.95312 1.96094 -3.78125 0.208008
 -0.121582 1.25]
hidden_states 1 [2.89062 -1.9375 -1.51562 -3.20312 -3.95312 1.96094 -3.78125 0.208008
 -0.121582 1.25]
hidden_states 2 [2.89062 -1.9375 -1.51562 -3.20312 -3.95312 1.96094 -3.78125 0.208008
 -0.121582 1.25]
hidden_states 3 [2.89062 -1.9375 -1.51562 -3.20312 -3.95312 1.96094 -3.78125 0.208008
 -0.121582 1.25]
hidden_states 4 [2.89062 -1.9375 -1.51562 -3.20312 -3.95312 1.96094 -3.78125 0.208008
 -0.121582 1.25]
hidden_states 5 [2.89062 -1.9375 -1.51562 -3.20312 -3.95312 1.96094 -3.78125 0.208008
 -0.121582 1.25]
hidden_states 6 [2.89062 -1.9375 -1.51562 -3.20312 -3.95312 1.96094 -3.78125 0.208008
 -0.121582 1.25]
hidden_states 7 [2.89062 -1.9375 -1.51562 -3.20312 -3.95312 1.96094 -3.78125 0.208008
 -0.121582 1.25]
hidden_states 8 [1.09375 -2.46875 0.328125 0.205078 0.0800781 3.20312 -2.39062 0.777344
 -0.722656 1.20312]
hidden_states 9 [2.89062 -1.9375 -1.51562 -3.20312 -3.95312 1.96094 -3.78125 0.208008
 -0.121582 1.25]
hidden_states 10 [2.89062 -1.9375 -1.51562 -3.20312 -3.95312 1.96094 -3.78125 0.208008
 -0.121582 1.25]
hidden_states 11 [2.89062 -1.9375 -1.51562 -3.20312 -3.95312 1.96094 -3.78125 0.208008
 -0.121582 1.25]
logits (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits after select (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits 0 [6.375 6.40625 3.25 2.53125 3 2.40625 5.375 3.07812 6 2.6875]
logits 1 [6.375 6.40625 3.25 2.53125 3 2.40625 5.375 3.07812 6 2.6875]
logits 2 [6.375 6.40625 3.25 2.53125 3 2.40625 5.375 3.07812 6 2.6875]
logits 3 [6.375 6.40625 3.25 2.53125 3 2.40625 5.375 3.07812 6 2.6875]
logits 4 [6.375 6.40625 3.25 2.53125 3 2.40625 5.375 3.07812 6 2.6875]
logits 5 [6.375 6.40625 3.25 2.53125 3 2.40625 5.375 3.07812 6 2.6875]
logits 6 [6.375 6.40625 3.25 2.53125 3 2.40625 5.375 3.07812 6 2.6875]
logits 7 [6.375 6.40625 3.25 2.53125 3 2.40625 5.375 3.07812 6 2.6875]
logits 8 [4.875 7.40625 3.14062 2.10938 3.07812 3.0625 5.9375 4.96875 6.375 2.20312]
logits 9 [6.375 6.40625 3.25 2.53125 3 2.40625 5.375 3.07812 6 2.6875]
logits 10 [6.375 6.40625 3.25 2.53125 3 2.40625 5.375 3.07812 6 2.6875]
logits 11 [6.375 6.40625 3.25 2.53125 3 2.40625 5.375 3.07812 6 2.6875]
next_tokens after sampling [7569 7569 7569 7569 7569 7569 7569 7569  198 7569 7569 7569 7569 7569
 7569 7569]
wenxin next_tokens [7569 7569 7569 7569 7569 7569 7569 7569  198 7569 7569 7569 7569 7569
 7569 7569]

self.max_num_reqs 256
dp_rank_reqs [['0'], ['1']]
dp_rank_tokens [[7569], [7569]]
cumsum_num_reqs_per_dp_rank [0 1 2]
padded_num_reqs_per_dp_rank 128
padded_num_reqs_logits 16
---- dp_rank 0 -----
dp_token_offset 0
dp_req_offset 0
dp_req_padded_offset 0
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [0]
arange [0]
positions_np [19]
self.input_batch.num_computed_tokens_cpu[req_indices] [20]
positions_np after add [20]
token_indices [20]
self.input_ids_cpu [7569]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [21  0  0 ...  0  0  0]
dp_req_offset 0
dp_token_offset 0
slices_start [20]
self.input_batch.num_computed_tokens_cpu [20 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [0]
slot_mapping_metadata [[84  0  1]]
num_slices [1, []]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [7569    0    0 ...    0    0    0]
---- dp_rank 1 -----
dp_token_offset 16
dp_req_offset 1
dp_req_padded_offset 128
num_reqs_dp 1
num_scheduled_tokens_per_req [1]
total_num_scheduled_tokens 1
req_indices [1]
arange [0]
positions_np [19]
self.input_batch.num_computed_tokens_cpu[req_indices] [20]
positions_np after add [20]
token_indices [1044]
self.input_ids_cpu [7569]
self.query_start_loc_cpu [0 1 1 ... 0 0 0]
self.seq_lens_cpu [21  0  0 ...  0  0  0]
dp_req_offset 1
dp_token_offset 16
slices_start [20]
self.input_batch.num_computed_tokens_cpu [20 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
global_block_start_idx [16]
slot_mapping_metadata [[16404     0     1]]
num_slices [1, 1]
padded_num_slices_per_dp_rank 16
slot_mapping_metadata after pad (16, 3)
self.input_ids_cpu [7569    0    0 ...    0    0    0]
---- --final start -- -----
input_ids (32,) [7569    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0 7569    0    0    0    0    0    0    0    0    0    0    0
    0    0    0    0]
positions [20  1  2  3  4  5  0  0  0  0  0  0  0  0  0  0 20  1  2  3  4  5  0  0
  0  0  0  0  0  0  0  0]
final slot_mapping_metadata (3, 32)
final slot_mapping_metadata [[   84     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0 16404     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [    1     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     1     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]]
final slot_mapping_metadata []
block_tables [[1 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
query_start_loc [ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1 16  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0]
seq_lens [21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0 21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
logits_indices [ 0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0]
dp_rank_reqs [['0'], ['1']]
num_seqs [1 1]
---- --final finish-- -----
temp_tensor [-1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00 -1.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  7.0064923e-45  8.4077908e-45  6.6710422e-30  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.3772934e-30  4.3646243e-41
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0995175e+13
  0.0000000e+00  0.0000000e+00  2.2420775e-44  0.0000000e+00
 -2.0683993e+15  2.2250378e-40  2.2420775e-44  0.0000000e+00
  4.6049755e-26  1.4012985e-45  0.0000000e+00  0.0000000e+00
  1.3772934e-30  4.3646243e-41  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  4.3644842e-41  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.4012985e-45  0.0000000e+00  1.2952631e-31  4.3646243e-41
  0.0000000e+00  0.0000000e+00  4.6758818e-30  0.0000000e+00
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  1.4701549e-30  4.3646243e-41
  1.4701549e-30  4.3646243e-41  4.2038954e-45  0.0000000e+00
  2.3694278e-38  1.4012985e-45  1.4012985e-45  9.1835496e-41
  0.0000000e+00  5.6051939e-45  0.0000000e+00  2.3694278e-38
            nan  0.0000000e+00            nan  1.1210388e-44
  8.2284644e-38  0.0000000e+00            nan            nan
  0.0000000e+00  1.4012985e-45  5.7397185e-42  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1835496e-41  0.0000000e+00
  2.3509887e-38  0.0000000e+00  9.1836897e-41  1.4012985e-45
  1.4012985e-45  7.0064923e-45  8.2284644e-38  0.0000000e+00
  0.0000000e+00  9.2195630e-41  8.2284644e-38  0.0000000e+00
  9.2195630e-41  9.1836897e-41  3.5873241e-43  0.0000000e+00
  1.1754944e-38  0.0000000e+00  1.1000000e+00  2.3509887e-38
            nan            nan  0.0000000e+00  3.5873241e-43
  2.1019477e-44  0.0000000e+00  0.0000000e+00  0.0000000e+00
  1.3312335e-43  2.3693561e-38  1.4012985e-40  0.0000000e+00
  9.1836897e-41  2.3509887e-38  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  0.0000000e+00  0.0000000e+00  1.4012985e-43  0.0000000e+00
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  2.2420775e-44  0.0000000e+00  4.2038954e-45  3.5873241e-43
  2.3509887e-38  0.0000000e+00  2.2420775e-44  0.0000000e+00
  1.0000000e-01  7.0064923e-45  4.4841551e-44  1.4012985e-45
  1.4012985e-44  4.2038954e-44  4.4841551e-44  1.4012985e-45
            nan            nan  1.4012985e-45  0.0000000e+00
 -1.7014118e+38  0.0000000e+00  9.2194229e-41  2.3509887e-38
  2.3509887e-38  0.0000000e+00  0.0000000e+00  2.3510604e-38
  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00
  8.2284644e-38  0.0000000e+00  2.8025969e-44  5.6051939e-44
  0.0000000e+00            nan            nan            nan]
top_k_tensor [        0         0         0         0         0         0         0
         0     -4096         0         0         0         0         0
         0         0     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1 233707072     31147 344830232         0     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707048     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -8192        -1     -8192        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1 233707064     31147 344830232
         0     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1     -4096        -1     -4096
        -1     -4096        -1     -4096        -1 233707032     31147
 344830208         0 233707032     31147 344830216         0     -4096
        -1     -4096        -1     -4096        -1     -4096        -1
     -4096        -1     -4096        -1]
top_p_tensor [ 1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan  1.0000000e+00  1.0000000e+00  1.0000000e+00
  1.0000000e+00  1.0000000e+00  1.0000000e+00  1.0000000e+00
            nan            nan  1.0248949e+11  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316673e-13 -3.0316488e-13
            nan            nan  1.2113646e+13  4.7644148e-44
            nan            nan -3.0360897e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4933738e+15  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0319264e-13 -3.0360897e-13
            nan            nan -3.0360897e-13 -3.1071440e-13
            nan            nan  1.7651560e+17  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -1.1368684e-13 -3.0316488e-13
            nan            nan  0.0000000e+00  0.0000000e+00
  1.4307163e-26  0.0000000e+00  0.0000000e+00  0.0000000e+00
            nan            nan  2.1188873e+19  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -1.3020833e-03
            nan            nan -3.0316532e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  2.6222406e+21  4.7644148e-44
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0494818e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan  1.4012985e-45  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.0320188e-13 -3.0316488e-13
            nan            nan  8.5479206e-44  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan -3.6261203e+24  4.3643441e-41
            nan            nan  0.0000000e+00  0.0000000e+00
            nan            nan  1.4692446e-39  0.0000000e+00
            nan            nan  1.4012985e-44  0.0000000e+00
            nan            nan  1.4012985e-45 -1.1368685e-13
            nan            nan  0.0000000e+00  4.7644148e-44
            nan            nan -3.0316488e-13 -4.1268447e+26
            nan            nan -3.0316488e-13 -3.0494124e-13
            nan            nan -3.1027031e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327764e-13
            nan            nan  4.3148833e+29  4.7644148e-44
            nan            nan -3.0319264e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -4.4307661e+35
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -7.7610210e-11
            nan            nan  5.0080864e+31  4.7644148e-44
            nan            nan -3.3158659e-13 -7.8066734e-11
            nan            nan -3.0316491e-13 -7.7610210e-11
            nan            nan -3.0316488e-13 -3.0316488e-13
            nan            nan -3.0316488e-13 -3.0327590e-13
            nan            nan  5.7511240e+33  4.7644148e-44
            nan            nan -3.0316499e-13 -4.1685183e-13
            nan            nan            nan -3.0316491e-13
            nan            nan -7.1696835e-03 -3.0316532e-13
            nan            nan -3.0319264e-13 -4.4307597e+35]
hidden_states after model_fn (32, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', None), memory_kind=device)
inputs[4] (16,) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(None,), memory_kind=device)
hidden_states after select (16, 4096) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec(), memory_kind=device)
hidden_states 0 [2.59375 -1.10938 -0.296875 -2.23438 1.42969 3.1875 0.191406 1.32812
 -1.30469 -1.57812]
hidden_states 1 [2.59375 -1.10938 -0.296875 -2.23438 1.42969 3.1875 0.191406 1.32812
 -1.30469 -1.57812]
hidden_states 2 [2.59375 -1.10938 -0.296875 -2.23438 1.42969 3.1875 0.191406 1.32812
 -1.30469 -1.57812]
hidden_states 3 [2.59375 -1.10938 -0.296875 -2.23438 1.42969 3.1875 0.191406 1.32812
 -1.30469 -1.57812]
hidden_states 4 [2.59375 -1.10938 -0.296875 -2.23438 1.42969 3.1875 0.191406 1.32812
 -1.30469 -1.57812]
hidden_states 5 [2.59375 -1.10938 -0.296875 -2.23438 1.42969 3.1875 0.191406 1.32812
 -1.30469 -1.57812]
hidden_states 6 [2.59375 -1.10938 -0.296875 -2.23438 1.42969 3.1875 0.191406 1.32812
 -1.30469 -1.57812]
hidden_states 7 [2.59375 -1.10938 -0.296875 -2.23438 1.42969 3.1875 0.191406 1.32812
 -1.30469 -1.57812]
hidden_states 8 [0.753906 -0.839844 1.64062 -0.294922 0.125 0.769531 -4.15625 0.455078
 0.632812 -0.279297]
hidden_states 9 [2.59375 -1.10938 -0.296875 -2.23438 1.42969 3.1875 0.191406 1.32812
 -1.30469 -1.57812]
hidden_states 10 [2.59375 -1.10938 -0.296875 -2.23438 1.42969 3.1875 0.191406 1.32812
 -1.30469 -1.57812]
hidden_states 11 [2.59375 -1.10938 -0.296875 -2.23438 1.42969 3.1875 0.191406 1.32812
 -1.30469 -1.57812]
logits (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits after select (16, 128256) NamedSharding(mesh=Mesh('data': 2, 'expert': 1, 'seq': 1, 'model': 4, axis_types=(Auto, Auto, Auto, Auto)), spec=PartitionSpec('data', 'model'), memory_kind=device)
logits 0 [4.375 6.46875 3.51562 3.95312 1.22656 1.74219 5.8125 4.1875 5.875 3.92188]
logits 1 [4.375 6.46875 3.51562 3.95312 1.22656 1.74219 5.8125 4.1875 5.875 3.92188]
logits 2 [4.375 6.46875 3.51562 3.95312 1.22656 1.74219 5.8125 4.1875 5.875 3.92188]
logits 3 [4.375 6.46875 3.51562 3.95312 1.22656 1.74219 5.8125 4.1875 5.875 3.92188]
logits 4 [4.375 6.46875 3.51562 3.95312 1.22656 1.74219 5.8125 4.1875 5.875 3.92188]
logits 5 [4.375 6.46875 3.51562 3.95312 1.22656 1.74219 5.8125 4.1875 5.875 3.92188]
logits 6 [4.375 6.46875 3.51562 3.95312 1.22656 1.74219 5.8125 4.1875 5.875 3.92188]
logits 7 [4.375 6.46875 3.51562 3.95312 1.22656 1.74219 5.8125 4.1875 5.875 3.92188]
logits 8 [3.59375 4.0625 1.89844 0.601562 2.625 3.23438 3.8125 3.8125 4.53125
 2.21875]
logits 9 [4.375 6.46875 3.51562 3.95312 1.22656 1.74219 5.8125 4.1875 5.875 3.92188]
logits 10 [4.375 6.46875 3.51562 3.95312 1.22656 1.74219 5.8125 4.1875 5.875 3.92188]
logits 11 [4.375 6.46875 3.51562 3.95312 1.22656 1.74219 5.8125 4.1875 5.875 3.92188]
next_tokens after sampling [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]
wenxin next_tokens [11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]
--------------------------------------------------
Prompt: 'The capital of France is'
Generated text: ' a.\nThe 24,_REF,\n|||, thesolar,'
--------------------------------------------------
Prompt: 'Colors of the rainbow are'
Generated text: ' of.\nThe 24,_REF,\n|||, thesolar,'
--------------------------------------------------
